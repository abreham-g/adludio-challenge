

============================== 2022-03-06 04:58:58.109377 | 93f1c1f7-2418-462c-9e3e-c5c39883e6de ==============================
04:58:58.109377 [info ] [MainThread]: Running with dbt=1.0.3
04:58:58.109992 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, project_name='Analytics_dbt', skip_profile_setup=False, defer=None, state=None, cls=<class 'dbt.task.init.InitTask'>, which='init', rpc_method=None)
04:58:58.110261 [debug] [MainThread]: Tracking: tracking
04:58:58.115060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb73453ff70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb73453ffd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb73453fdf0>]}
04:58:58.116038 [info ] [MainThread]: Creating dbt configuration folder at /home/abreham/.dbt
04:58:58.117248 [debug] [MainThread]: Starter project path: /opt/miniconda/lib/python3.9/site-packages/dbt/include/starter_project
04:59:18.247465 [info ] [MainThread]: Profile Analytics_dbt written to /home/abreham/.dbt/profiles.yml using target's sample configuration. Once updated, you'll be able to start developing with dbt.
04:59:18.251164 [info ] [MainThread]: 
Your new dbt project "Analytics_dbt" was created!

For more information on how to configure the profiles.yml file,
please consult the dbt documentation here:

  https://docs.getdbt.com/docs/configure-your-profile

One more thing:

Need help? Don't hesitate to reach out to us via GitHub issues or on Slack:

  https://community.getdbt.com/

Happy modeling!

04:59:18.251705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb73451cc70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb73451cd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb73451c610>]}


============================== 2022-03-06 09:26:04.566980 | 9fb2f309-8c7f-4dcf-ba9f-77af8c17b614 ==============================
09:26:04.566980 [info ] [MainThread]: Running with dbt=1.0.3
09:26:04.567650 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, config_dir=False, defer=None, state=None, cls=<class 'dbt.task.debug.DebugTask'>, which='debug', rpc_method=None)
09:26:04.568347 [debug] [MainThread]: Tracking: tracking
09:26:04.573204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f829d3c8640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f829d3c8940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f829d3c8820>]}
09:26:04.695869 [debug] [MainThread]: Executing "git --help"
09:26:04.701380 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
09:26:04.702048 [debug] [MainThread]: STDERR: "b''"
09:26:04.702719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f829d3b1760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f829b965a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f829b965be0>]}


============================== 2022-03-06 09:28:31.158222 | 69717325-01d9-40e3-9319-21e290c400b8 ==============================
09:28:31.158222 [info ] [MainThread]: Running with dbt=1.0.3
09:28:31.158827 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, config_dir=False, defer=None, state=None, cls=<class 'dbt.task.debug.DebugTask'>, which='debug', rpc_method=None)
09:28:31.159120 [debug] [MainThread]: Tracking: tracking
09:28:31.166983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c91ef850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c91ef8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c91efa00>]}
09:28:31.300945 [debug] [MainThread]: Executing "git --help"
09:28:31.309178 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
09:28:31.309833 [debug] [MainThread]: STDERR: "b''"
09:28:31.314083 [debug] [MainThread]: Acquiring new postgres connection "debug"
09:28:31.314750 [debug] [MainThread]: Using postgres connection "debug"
09:28:31.315026 [debug] [MainThread]: On debug: select 1 as id
09:28:31.315288 [debug] [MainThread]: Opening a new connection, currently in state init
09:28:31.341247 [debug] [MainThread]: SQL status: SELECT 1 in 0.03 seconds
09:28:31.343871 [debug] [MainThread]: On debug: Close
09:28:31.348093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c77873d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c7787d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c7787940>]}
09:28:31.663325 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-03-06 09:28:50.603165 | f0e3d4f8-54ab-481a-9ad4-13ad32de3350 ==============================
09:28:50.603165 [info ] [MainThread]: Running with dbt=1.0.3
09:28:50.603771 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, config_dir=False, defer=None, state=None, cls=<class 'dbt.task.debug.DebugTask'>, which='debug', rpc_method=None)
09:28:50.604056 [debug] [MainThread]: Tracking: tracking
09:28:50.608403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f32eec8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f32eec940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f32eec910>]}
09:28:50.714060 [debug] [MainThread]: Executing "git --help"
09:28:50.719423 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
09:28:50.720075 [debug] [MainThread]: STDERR: "b''"
09:28:50.724459 [debug] [MainThread]: Acquiring new postgres connection "debug"
09:28:50.725531 [debug] [MainThread]: Using postgres connection "debug"
09:28:50.725858 [debug] [MainThread]: On debug: select 1 as id
09:28:50.726166 [debug] [MainThread]: Opening a new connection, currently in state init
09:28:50.740217 [debug] [MainThread]: SQL status: SELECT 1 in 0.01 seconds
09:28:50.742506 [debug] [MainThread]: On debug: Close
09:28:50.746386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f31480310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f31480f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f314808e0>]}
09:28:51.042614 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-03-06 10:26:49.479278 | e611a26c-4a51-4992-b62c-a5b71effa97f ==============================
10:26:49.479278 [info ] [MainThread]: Running with dbt=1.0.3
10:26:49.484195 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:26:49.484854 [debug] [MainThread]: Tracking: tracking
10:26:49.490668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53fcb1b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53fcb1460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53fcb1640>]}
10:26:49.508863 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
10:26:49.509977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e611a26c-4a51-4992-b62c-a5b71effa97f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53e242fa0>]}
10:26:49.534738 [debug] [MainThread]: Parsing macros/adapters.sql
10:26:49.590857 [debug] [MainThread]: Parsing macros/catalog.sql
10:26:49.597847 [debug] [MainThread]: Parsing macros/relations.sql
10:26:49.600290 [debug] [MainThread]: Parsing macros/materializations/snapshot_merge.sql
10:26:49.603084 [debug] [MainThread]: Parsing macros/adapters/columns.sql
10:26:49.629835 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
10:26:49.636284 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
10:26:49.640973 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
10:26:49.653254 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
10:26:49.660937 [debug] [MainThread]: Parsing macros/adapters/relation.sql
10:26:49.677513 [debug] [MainThread]: Parsing macros/adapters/schema.sql
10:26:49.681260 [debug] [MainThread]: Parsing macros/etc/datetime.sql
10:26:49.695747 [debug] [MainThread]: Parsing macros/etc/statement.sql
10:26:49.703157 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
10:26:49.705566 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
10:26:49.706583 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
10:26:49.708059 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
10:26:49.709210 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
10:26:49.711622 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
10:26:49.714557 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
10:26:49.718491 [debug] [MainThread]: Parsing macros/materializations/configs.sql
10:26:49.723986 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
10:26:49.730799 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
10:26:49.738331 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
10:26:49.759474 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
10:26:49.762221 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
10:26:49.783877 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
10:26:49.822602 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
10:26:49.827525 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
10:26:49.840509 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
10:26:49.845145 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
10:26:49.849324 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
10:26:49.851619 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
10:26:49.863918 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
10:26:49.895587 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
10:26:49.906141 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
10:26:49.928676 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
10:26:49.949212 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
10:26:49.952307 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
10:26:49.986821 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
10:26:49.990036 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
10:26:49.997613 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
10:26:50.000784 [debug] [MainThread]: Parsing tests/generic/builtin.sql
10:26:50.413479 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:26:50.433561 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
10:26:50.531043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e611a26c-4a51-4992-b62c-a5b71effa97f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53d940a90>]}
10:26:50.544090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e611a26c-4a51-4992-b62c-a5b71effa97f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53fc9f9a0>]}
10:26:50.544601 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:26:50.547645 [info ] [MainThread]: 
10:26:50.548869 [debug] [MainThread]: Acquiring new postgres connection "master"
10:26:50.550726 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:26:50.569423 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:26:50.569841 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:26:50.570091 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:26:50.583360 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:26:50.585822 [debug] [ThreadPool]: On list_adludio: Close
10:26:50.587628 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:26:50.603690 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:26:50.604041 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:26:50.604278 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:26:50.617015 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:26:50.617422 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:26:50.617706 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:26:50.628805 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.01 seconds
10:26:50.631156 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:26:50.634859 [debug] [ThreadPool]: On list_adludio_public: Close
10:26:50.641962 [debug] [MainThread]: Using postgres connection "master"
10:26:50.642369 [debug] [MainThread]: On master: BEGIN
10:26:50.642637 [debug] [MainThread]: Opening a new connection, currently in state init
10:26:50.668831 [debug] [MainThread]: SQL status: BEGIN in 0.03 seconds
10:26:50.669340 [debug] [MainThread]: Using postgres connection "master"
10:26:50.669912 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:26:50.693152 [debug] [MainThread]: SQL status: SELECT 5 in 0.02 seconds
10:26:50.695716 [debug] [MainThread]: On master: ROLLBACK
10:26:50.696189 [debug] [MainThread]: Using postgres connection "master"
10:26:50.696444 [debug] [MainThread]: On master: BEGIN
10:26:50.696894 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:26:50.697154 [debug] [MainThread]: On master: COMMIT
10:26:50.697381 [debug] [MainThread]: Using postgres connection "master"
10:26:50.697621 [debug] [MainThread]: On master: COMMIT
10:26:50.697927 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:26:50.698189 [debug] [MainThread]: On master: Close
10:26:50.698769 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:26:50.699806 [info ] [MainThread]: 
10:26:50.704611 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:26:50.705069 [info ] [Thread-1  ]: 1 of 2 START table model public.my_first_dbt_model.............................. [RUN]
10:26:50.705909 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.706184 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:26:50.706452 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:26:50.710550 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:26:50.711122 [debug] [Thread-1  ]: finished collecting timing info
10:26:50.711388 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:26:50.776691 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:26:50.777506 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.777803 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:26:50.778056 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:26:50.798826 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
10:26:50.799249 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.799553 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:26:50.804768 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.0 seconds
10:26:50.825016 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.825353 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:26:50.826142 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:26:50.830015 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.830292 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:26:50.830878 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:26:50.849942 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:26:50.850330 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.850634 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:26:50.851971 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:26:50.859128 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.859425 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:26:50.864980 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
10:26:50.866887 [debug] [Thread-1  ]: finished collecting timing info
10:26:50.867215 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:26:50.867871 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e611a26c-4a51-4992-b62c-a5b71effa97f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53c9105b0>]}
10:26:50.868412 [info ] [Thread-1  ]: 1 of 2 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2[0m in 0.16s]
10:26:50.869245 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:26:50.870212 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_second_dbt_model
10:26:50.870876 [info ] [Thread-1  ]: 2 of 2 START view model public.my_second_dbt_model.............................. [RUN]
10:26:50.871593 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:26:50.871846 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_second_dbt_model
10:26:50.872089 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_second_dbt_model
10:26:50.877663 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:26:50.878124 [debug] [Thread-1  ]: finished collecting timing info
10:26:50.878401 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_second_dbt_model
10:26:50.908379 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:26:50.908905 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:26:50.909118 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: BEGIN
10:26:50.909293 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:26:50.919919 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:26:50.920298 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:26:50.920574 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */

  create view "adludio"."public"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "adludio"."public"."my_first_dbt_model"
where id = 1
  );
10:26:50.925149 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.0 seconds
10:26:50.929158 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:26:50.929433 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */
alter table "adludio"."public"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
10:26:50.930147 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:26:50.932176 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: COMMIT
10:26:50.932475 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:26:50.932717 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: COMMIT
10:26:50.933891 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:26:50.936542 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:26:50.936822 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */
drop view if exists "adludio"."public"."my_second_dbt_model__dbt_backup" cascade
10:26:50.937254 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
10:26:50.938943 [debug] [Thread-1  ]: finished collecting timing info
10:26:50.939258 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: Close
10:26:50.939953 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e611a26c-4a51-4992-b62c-a5b71effa97f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53e243940>]}
10:26:50.940504 [info ] [Thread-1  ]: 2 of 2 OK created view model public.my_second_dbt_model......................... [[32mCREATE VIEW[0m in 0.07s]
10:26:50.941474 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_second_dbt_model
10:26:50.947167 [debug] [MainThread]: Acquiring new postgres connection "master"
10:26:50.947513 [debug] [MainThread]: Using postgres connection "master"
10:26:50.947757 [debug] [MainThread]: On master: BEGIN
10:26:50.947984 [debug] [MainThread]: Opening a new connection, currently in state closed
10:26:50.958747 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:26:50.959064 [debug] [MainThread]: On master: COMMIT
10:26:50.959305 [debug] [MainThread]: Using postgres connection "master"
10:26:50.959541 [debug] [MainThread]: On master: COMMIT
10:26:50.959872 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:26:50.960140 [debug] [MainThread]: On master: Close
10:26:50.960712 [info ] [MainThread]: 
10:26:50.961633 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0.41s.
10:26:50.962392 [debug] [MainThread]: Connection 'master' was properly closed.
10:26:50.962667 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:26:50.962883 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_second_dbt_model' was properly closed.
10:26:50.971195 [info ] [MainThread]: 
10:26:50.971689 [info ] [MainThread]: [32mCompleted successfully[0m
10:26:50.977804 [info ] [MainThread]: 
10:26:50.978410 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
10:26:50.979138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53fc9f400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53fc93760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53e1c9490>]}


============================== 2022-03-06 10:31:42.377658 | 7385daaa-d28d-4624-85c6-c5d5b13a0802 ==============================
10:31:42.377658 [info ] [MainThread]: Running with dbt=1.0.3
10:31:42.378504 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:31:42.378859 [debug] [MainThread]: Tracking: tracking
10:31:42.383967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abcf46190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abcf46370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abcf467c0>]}
10:31:42.423021 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:31:42.423849 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:31:42.442839 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:31:42.507628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7385daaa-d28d-4624-85c6-c5d5b13a0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abc5b40d0>]}
10:31:42.515827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7385daaa-d28d-4624-85c6-c5d5b13a0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abc5b46d0>]}
10:31:42.516322 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:31:42.518393 [info ] [MainThread]: 
10:31:42.519172 [debug] [MainThread]: Acquiring new postgres connection "master"
10:31:42.520506 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:31:42.537378 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:31:42.537775 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:31:42.538054 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:31:42.550967 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:31:42.553338 [debug] [ThreadPool]: On list_adludio: Close
10:31:42.555150 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:31:42.618664 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:31:42.619065 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:31:42.619325 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:31:42.630018 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:31:42.630357 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:31:42.630600 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:31:42.633779 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.0 seconds
10:31:42.635989 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:31:42.636400 [debug] [ThreadPool]: On list_adludio_public: Close
10:31:42.643107 [debug] [MainThread]: Using postgres connection "master"
10:31:42.643431 [debug] [MainThread]: On master: BEGIN
10:31:42.643676 [debug] [MainThread]: Opening a new connection, currently in state init
10:31:42.654265 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:31:42.654582 [debug] [MainThread]: Using postgres connection "master"
10:31:42.654828 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:31:42.682657 [debug] [MainThread]: SQL status: SELECT 5 in 0.03 seconds
10:31:42.685196 [debug] [MainThread]: On master: ROLLBACK
10:31:42.685701 [debug] [MainThread]: Using postgres connection "master"
10:31:42.685970 [debug] [MainThread]: On master: BEGIN
10:31:42.686420 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:31:42.686684 [debug] [MainThread]: On master: COMMIT
10:31:42.686928 [debug] [MainThread]: Using postgres connection "master"
10:31:42.687158 [debug] [MainThread]: On master: COMMIT
10:31:42.687474 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:31:42.687734 [debug] [MainThread]: On master: Close
10:31:42.688353 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:31:42.689391 [info ] [MainThread]: 
10:31:42.693987 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:31:42.694446 [info ] [Thread-1  ]: 1 of 2 START table model public.my_first_dbt_model.............................. [RUN]
10:31:42.695447 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.695715 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:31:42.695988 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:31:42.700273 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:31:42.700774 [debug] [Thread-1  ]: finished collecting timing info
10:31:42.701046 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:31:42.744033 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:31:42.744721 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.745018 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:31:42.745262 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:31:42.755875 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:31:42.756215 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.756458 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select * from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:31:42.762638 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
10:31:42.772281 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.772595 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:31:42.773299 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:31:42.776917 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.777183 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:31:42.777787 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:31:42.794082 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:31:42.794419 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.794675 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:31:42.796956 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:31:42.804162 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.804472 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:31:42.806843 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
10:31:42.808624 [debug] [Thread-1  ]: finished collecting timing info
10:31:42.808936 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:31:42.809629 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7385daaa-d28d-4624-85c6-c5d5b13a0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abce5de50>]}
10:31:42.810245 [info ] [Thread-1  ]: 1 of 2 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2037[0m in 0.11s]
10:31:42.811399 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:31:42.812646 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_second_dbt_model
10:31:42.813034 [info ] [Thread-1  ]: 2 of 2 START view model public.my_second_dbt_model.............................. [RUN]
10:31:42.813781 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:31:42.814047 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_second_dbt_model
10:31:42.814291 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_second_dbt_model
10:31:42.817940 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:31:42.818438 [debug] [Thread-1  ]: finished collecting timing info
10:31:42.818705 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_second_dbt_model
10:31:42.845217 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:31:42.845903 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:31:42.846199 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: BEGIN
10:31:42.846434 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:31:42.857047 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:31:42.857385 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:31:42.857675 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */

  create view "adludio"."public"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "adludio"."public"."my_first_dbt_model"
where id = 1
  );
10:31:42.858360 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "id" does not exist
LINE 8: where id = 1
              ^

10:31:42.858639 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: ROLLBACK
10:31:42.859188 [debug] [Thread-1  ]: finished collecting timing info
10:31:42.859467 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: Close
10:31:42.860049 [debug] [Thread-1  ]: Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  column "id" does not exist
  LINE 8: where id = 1
                ^
  compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:31:42.860532 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7385daaa-d28d-4624-85c6-c5d5b13a0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abc05d9d0>]}
10:31:42.861124 [error] [Thread-1  ]: 2 of 2 ERROR creating view model public.my_second_dbt_model..................... [[31mERROR[0m in 0.05s]
10:31:42.861767 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_second_dbt_model
10:31:42.863415 [debug] [MainThread]: Acquiring new postgres connection "master"
10:31:42.863730 [debug] [MainThread]: Using postgres connection "master"
10:31:42.863973 [debug] [MainThread]: On master: BEGIN
10:31:42.864194 [debug] [MainThread]: Opening a new connection, currently in state closed
10:31:42.874627 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:31:42.874989 [debug] [MainThread]: On master: COMMIT
10:31:42.875255 [debug] [MainThread]: Using postgres connection "master"
10:31:42.875490 [debug] [MainThread]: On master: COMMIT
10:31:42.875820 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:31:42.876108 [debug] [MainThread]: On master: Close
10:31:42.876725 [info ] [MainThread]: 
10:31:42.877698 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0.36s.
10:31:42.878193 [debug] [MainThread]: Connection 'master' was properly closed.
10:31:42.878443 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:31:42.878659 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_second_dbt_model' was properly closed.
10:31:42.887932 [info ] [MainThread]: 
10:31:42.888444 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:31:42.888913 [info ] [MainThread]: 
10:31:42.889334 [error] [MainThread]: [33mDatabase Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)[0m
10:31:42.889776 [error] [MainThread]:   column "id" does not exist
10:31:42.890160 [error] [MainThread]:   LINE 8: where id = 1
10:31:42.890528 [error] [MainThread]:                 ^
10:31:42.890889 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:31:42.891273 [info ] [MainThread]: 
10:31:42.891653 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
10:31:42.892164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abe9ba490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abcf31cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abc0725b0>]}


============================== 2022-03-06 10:32:06.140648 | 51fbc5ce-73e1-4764-b690-d84a060365d0 ==============================
10:32:06.140648 [info ] [MainThread]: Running with dbt=1.0.3
10:32:06.141542 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:32:06.141896 [debug] [MainThread]: Tracking: tracking
10:32:06.146649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1e9fbb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1e9fb460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1e9fb640>]}
10:32:06.185830 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
10:32:06.186288 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
10:32:06.196745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '51fbc5ce-73e1-4764-b690-d84a060365d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1ea03040>]}
10:32:06.205542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '51fbc5ce-73e1-4764-b690-d84a060365d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1ced8b50>]}
10:32:06.206066 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:32:06.208160 [info ] [MainThread]: 
10:32:06.208968 [debug] [MainThread]: Acquiring new postgres connection "master"
10:32:06.210374 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:32:06.225855 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:32:06.226237 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:32:06.226503 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:32:06.239393 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:32:06.241815 [debug] [ThreadPool]: On list_adludio: Close
10:32:06.243534 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:32:06.252597 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:32:06.252913 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:32:06.253154 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:32:06.263697 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:32:06.264024 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:32:06.264275 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:32:06.267457 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:32:06.269670 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:32:06.270073 [debug] [ThreadPool]: On list_adludio_public: Close
10:32:06.276798 [debug] [MainThread]: Using postgres connection "master"
10:32:06.277130 [debug] [MainThread]: On master: BEGIN
10:32:06.277371 [debug] [MainThread]: Opening a new connection, currently in state init
10:32:06.287868 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:32:06.288184 [debug] [MainThread]: Using postgres connection "master"
10:32:06.288429 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:32:06.312021 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:32:06.314482 [debug] [MainThread]: On master: ROLLBACK
10:32:06.315007 [debug] [MainThread]: Using postgres connection "master"
10:32:06.315278 [debug] [MainThread]: On master: BEGIN
10:32:06.315738 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:32:06.316028 [debug] [MainThread]: On master: COMMIT
10:32:06.316277 [debug] [MainThread]: Using postgres connection "master"
10:32:06.316498 [debug] [MainThread]: On master: COMMIT
10:32:06.316822 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:32:06.317082 [debug] [MainThread]: On master: Close
10:32:06.317742 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:32:06.318742 [info ] [MainThread]: 
10:32:06.323309 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:32:06.323812 [info ] [Thread-1  ]: 1 of 2 START table model public.my_first_dbt_model.............................. [RUN]
10:32:06.324599 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.324863 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:32:06.325132 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:32:06.329019 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:32:06.329555 [debug] [Thread-1  ]: finished collecting timing info
10:32:06.329840 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:32:06.372662 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:32:06.373395 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.373726 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:32:06.373958 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:32:06.384479 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:32:06.384840 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.385094 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select * from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:32:06.391518 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
10:32:06.401480 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.401888 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:32:06.402585 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:32:06.406397 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.406691 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:32:06.407319 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:32:06.423972 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:32:06.424363 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.424623 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:32:06.427532 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:32:06.436737 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.437074 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:32:06.439617 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
10:32:06.441504 [debug] [Thread-1  ]: finished collecting timing info
10:32:06.441816 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:32:06.442505 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '51fbc5ce-73e1-4764-b690-d84a060365d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1c02bb20>]}
10:32:06.443053 [info ] [Thread-1  ]: 1 of 2 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2037[0m in 0.12s]
10:32:06.443855 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:32:06.444585 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_second_dbt_model
10:32:06.445292 [info ] [Thread-1  ]: 2 of 2 START view model public.my_second_dbt_model.............................. [RUN]
10:32:06.446011 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:06.446272 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_second_dbt_model
10:32:06.446523 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_second_dbt_model
10:32:06.449895 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:32:06.450384 [debug] [Thread-1  ]: finished collecting timing info
10:32:06.450642 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_second_dbt_model
10:32:06.531460 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:32:06.532134 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:06.532416 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: BEGIN
10:32:06.532649 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:32:06.543104 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:32:06.543461 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:06.543722 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */

  create view "adludio"."public"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "adludio"."public"."my_first_dbt_model"
where id = 1
  );
10:32:06.544381 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "id" does not exist
LINE 8: where id = 1
              ^

10:32:06.544655 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: ROLLBACK
10:32:06.545190 [debug] [Thread-1  ]: finished collecting timing info
10:32:06.545501 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: Close
10:32:06.546100 [debug] [Thread-1  ]: Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  column "id" does not exist
  LINE 8: where id = 1
                ^
  compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:32:06.546574 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '51fbc5ce-73e1-4764-b690-d84a060365d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1c04ebb0>]}
10:32:06.547142 [error] [Thread-1  ]: 2 of 2 ERROR creating view model public.my_second_dbt_model..................... [[31mERROR[0m in 0.10s]
10:32:06.547750 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_second_dbt_model
10:32:06.549125 [debug] [MainThread]: Acquiring new postgres connection "master"
10:32:06.549483 [debug] [MainThread]: Using postgres connection "master"
10:32:06.549737 [debug] [MainThread]: On master: BEGIN
10:32:06.549962 [debug] [MainThread]: Opening a new connection, currently in state closed
10:32:06.560304 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:32:06.560665 [debug] [MainThread]: On master: COMMIT
10:32:06.560919 [debug] [MainThread]: Using postgres connection "master"
10:32:06.561144 [debug] [MainThread]: On master: COMMIT
10:32:06.561516 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:32:06.561790 [debug] [MainThread]: On master: Close
10:32:06.562405 [info ] [MainThread]: 
10:32:06.563372 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0.35s.
10:32:06.563851 [debug] [MainThread]: Connection 'master' was properly closed.
10:32:06.564165 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:32:06.564437 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_second_dbt_model' was properly closed.
10:32:06.571868 [info ] [MainThread]: 
10:32:06.572388 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:32:06.572863 [info ] [MainThread]: 
10:32:06.573286 [error] [MainThread]: [33mDatabase Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)[0m
10:32:06.573717 [error] [MainThread]:   column "id" does not exist
10:32:06.574088 [error] [MainThread]:   LINE 8: where id = 1
10:32:06.574469 [error] [MainThread]:                 ^
10:32:06.574832 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:32:06.575222 [info ] [MainThread]: 
10:32:06.575606 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
10:32:06.576124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1ce621c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1ce62130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1cf8cc40>]}


============================== 2022-03-06 10:32:14.421374 | 27c28a41-c642-4840-af1f-9c50a8516148 ==============================
10:32:14.421374 [info ] [MainThread]: Running with dbt=1.0.3
10:32:14.422800 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:32:14.423241 [debug] [MainThread]: Tracking: tracking
10:32:14.428055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8380fb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8380f460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8380f640>]}
10:32:14.467558 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:32:14.468352 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_second_dbt_model.sql
10:32:14.487413 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
10:32:14.552141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '27c28a41-c642-4840-af1f-9c50a8516148', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8140e0d0>]}
10:32:14.560541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '27c28a41-c642-4840-af1f-9c50a8516148', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8140e6d0>]}
10:32:14.561100 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:32:14.564220 [info ] [MainThread]: 
10:32:14.565469 [debug] [MainThread]: Acquiring new postgres connection "master"
10:32:14.566813 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:32:14.583725 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:32:14.584107 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:32:14.584380 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:32:14.597350 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:32:14.599728 [debug] [ThreadPool]: On list_adludio: Close
10:32:14.601419 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:32:14.662311 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:32:14.662678 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:32:14.662980 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:32:14.673721 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:32:14.674025 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:32:14.674267 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:32:14.677420 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:32:14.679533 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:32:14.679910 [debug] [ThreadPool]: On list_adludio_public: Close
10:32:14.686473 [debug] [MainThread]: Using postgres connection "master"
10:32:14.686758 [debug] [MainThread]: On master: BEGIN
10:32:14.687021 [debug] [MainThread]: Opening a new connection, currently in state init
10:32:14.697536 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:32:14.697841 [debug] [MainThread]: Using postgres connection "master"
10:32:14.698085 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:32:14.722137 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:32:14.724624 [debug] [MainThread]: On master: ROLLBACK
10:32:14.725070 [debug] [MainThread]: Using postgres connection "master"
10:32:14.725337 [debug] [MainThread]: On master: BEGIN
10:32:14.725836 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:32:14.726113 [debug] [MainThread]: On master: COMMIT
10:32:14.726369 [debug] [MainThread]: Using postgres connection "master"
10:32:14.726598 [debug] [MainThread]: On master: COMMIT
10:32:14.726923 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:32:14.727185 [debug] [MainThread]: On master: Close
10:32:14.727783 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:32:14.728841 [info ] [MainThread]: 
10:32:14.733024 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:32:14.733494 [info ] [Thread-1  ]: 1 of 2 START table model public.my_first_dbt_model.............................. [RUN]
10:32:14.734459 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.734728 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:32:14.735017 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:32:14.739133 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:32:14.739660 [debug] [Thread-1  ]: finished collecting timing info
10:32:14.739961 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:32:14.785571 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:32:14.786302 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.787043 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:32:14.787319 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:32:14.797953 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:32:14.798288 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.798527 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select * from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:32:14.804924 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
10:32:14.814635 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.814942 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:32:14.815583 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:32:14.819150 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.819442 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:32:14.820018 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:32:14.836285 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:32:14.836619 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.836869 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:32:14.839552 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:32:14.846700 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.846983 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:32:14.850741 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
10:32:14.852493 [debug] [Thread-1  ]: finished collecting timing info
10:32:14.852791 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:32:14.853427 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27c28a41-c642-4840-af1f-9c50a8516148', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc81df69d0>]}
10:32:14.854050 [info ] [Thread-1  ]: 1 of 2 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2037[0m in 0.12s]
10:32:14.855196 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:32:14.856129 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_second_dbt_model
10:32:14.856521 [info ] [Thread-1  ]: 2 of 2 START view model public.my_second_dbt_model.............................. [RUN]
10:32:14.857210 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:14.857492 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_second_dbt_model
10:32:14.857772 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_second_dbt_model
10:32:14.861236 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:32:14.861792 [debug] [Thread-1  ]: finished collecting timing info
10:32:14.862049 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_second_dbt_model
10:32:14.888541 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:32:14.889217 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:14.889542 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: BEGIN
10:32:14.889797 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:32:14.900476 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:32:14.900801 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:14.901053 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */

  create view "adludio"."public"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

-- select *
-- from "adludio"."public"."my_first_dbt_model"
-- where id = 1
  );
10:32:14.901588 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near ")"
LINE 9:   );
          ^

10:32:14.901863 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: ROLLBACK
10:32:14.902392 [debug] [Thread-1  ]: finished collecting timing info
10:32:14.902669 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: Close
10:32:14.903222 [debug] [Thread-1  ]: Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  syntax error at or near ")"
  LINE 9:   );
            ^
  compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:32:14.903689 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27c28a41-c642-4840-af1f-9c50a8516148', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc803e0a00>]}
10:32:14.904259 [error] [Thread-1  ]: 2 of 2 ERROR creating view model public.my_second_dbt_model..................... [[31mERROR[0m in 0.05s]
10:32:14.904946 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_second_dbt_model
10:32:14.906687 [debug] [MainThread]: Acquiring new postgres connection "master"
10:32:14.907025 [debug] [MainThread]: Using postgres connection "master"
10:32:14.907257 [debug] [MainThread]: On master: BEGIN
10:32:14.907469 [debug] [MainThread]: Opening a new connection, currently in state closed
10:32:14.917952 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:32:14.918265 [debug] [MainThread]: On master: COMMIT
10:32:14.918508 [debug] [MainThread]: Using postgres connection "master"
10:32:14.918735 [debug] [MainThread]: On master: COMMIT
10:32:14.919064 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:32:14.919321 [debug] [MainThread]: On master: Close
10:32:14.919911 [info ] [MainThread]: 
10:32:14.920848 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0.35s.
10:32:14.921408 [debug] [MainThread]: Connection 'master' was properly closed.
10:32:14.921915 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:32:14.922143 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_second_dbt_model' was properly closed.
10:32:14.931170 [info ] [MainThread]: 
10:32:14.931664 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:32:14.932139 [info ] [MainThread]: 
10:32:14.932578 [error] [MainThread]: [33mDatabase Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)[0m
10:32:14.932994 [error] [MainThread]:   syntax error at or near ")"
10:32:14.933384 [error] [MainThread]:   LINE 9:   );
10:32:14.933779 [error] [MainThread]:             ^
10:32:14.934140 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:32:14.934526 [info ] [MainThread]: 
10:32:14.934904 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
10:32:14.935416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc81427df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc83814700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc803f8e80>]}


============================== 2022-03-06 10:32:29.225568 | f5163ffc-884e-469b-8557-4572c29dca4d ==============================
10:32:29.225568 [info ] [MainThread]: Running with dbt=1.0.3
10:32:29.226970 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:32:29.227383 [debug] [MainThread]: Tracking: tracking
10:32:29.233004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0962f18730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0962f18ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0962f18670>]}
10:32:29.272370 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
10:32:29.272876 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
10:32:29.283417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f5163ffc-884e-469b-8557-4572c29dca4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0962f201f0>]}
10:32:29.292337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f5163ffc-884e-469b-8557-4572c29dca4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09613f5b50>]}
10:32:29.292855 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:32:29.294953 [info ] [MainThread]: 
10:32:29.295765 [debug] [MainThread]: Acquiring new postgres connection "master"
10:32:29.297135 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:32:29.312602 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:32:29.312979 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:32:29.313242 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:32:29.326181 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:32:29.328535 [debug] [ThreadPool]: On list_adludio: Close
10:32:29.330361 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:32:29.339371 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:32:29.339708 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:32:29.339952 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:32:29.350846 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:32:29.351215 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:32:29.351469 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:32:29.354689 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:32:29.356813 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:32:29.357362 [debug] [ThreadPool]: On list_adludio_public: Close
10:32:29.364032 [debug] [MainThread]: Using postgres connection "master"
10:32:29.364338 [debug] [MainThread]: On master: BEGIN
10:32:29.364570 [debug] [MainThread]: Opening a new connection, currently in state init
10:32:29.375020 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:32:29.375372 [debug] [MainThread]: Using postgres connection "master"
10:32:29.375611 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:32:29.398719 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:32:29.401171 [debug] [MainThread]: On master: ROLLBACK
10:32:29.401760 [debug] [MainThread]: Using postgres connection "master"
10:32:29.402030 [debug] [MainThread]: On master: BEGIN
10:32:29.402477 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:32:29.402739 [debug] [MainThread]: On master: COMMIT
10:32:29.402975 [debug] [MainThread]: Using postgres connection "master"
10:32:29.403195 [debug] [MainThread]: On master: COMMIT
10:32:29.403502 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:32:29.403754 [debug] [MainThread]: On master: Close
10:32:29.404354 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:32:29.405334 [info ] [MainThread]: 
10:32:29.410371 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:32:29.410872 [info ] [Thread-1  ]: 1 of 2 START table model public.my_first_dbt_model.............................. [RUN]
10:32:29.411687 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.411958 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:32:29.412258 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:32:29.416117 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:32:29.416638 [debug] [Thread-1  ]: finished collecting timing info
10:32:29.416917 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:32:29.459811 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:32:29.460518 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.460818 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:32:29.461053 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:32:29.471697 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:32:29.472075 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.472336 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select * from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:32:29.478664 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
10:32:29.488434 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.488782 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:32:29.489484 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:32:29.493096 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.493369 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:32:29.493959 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:32:29.510235 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:32:29.510603 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.510847 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:32:29.513351 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:32:29.522383 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.522722 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:32:29.525927 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
10:32:29.527740 [debug] [Thread-1  ]: finished collecting timing info
10:32:29.528083 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:32:29.528778 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5163ffc-884e-469b-8557-4572c29dca4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0960395790>]}
10:32:29.529332 [info ] [Thread-1  ]: 1 of 2 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2037[0m in 0.12s]
10:32:29.529941 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:32:29.530838 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_second_dbt_model
10:32:29.531272 [info ] [Thread-1  ]: 2 of 2 START view model public.my_second_dbt_model.............................. [RUN]
10:32:29.531998 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:29.532258 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_second_dbt_model
10:32:29.532522 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_second_dbt_model
10:32:29.535950 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:32:29.536448 [debug] [Thread-1  ]: finished collecting timing info
10:32:29.536703 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_second_dbt_model
10:32:29.615290 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:32:29.615985 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:29.616276 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: BEGIN
10:32:29.616511 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:32:29.627188 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:32:29.627536 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:29.627791 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */

  create view "adludio"."public"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

-- select *
-- from "adludio"."public"."my_first_dbt_model"
-- where id = 1
  );
10:32:29.628244 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near ")"
LINE 9:   );
          ^

10:32:29.628511 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: ROLLBACK
10:32:29.629035 [debug] [Thread-1  ]: finished collecting timing info
10:32:29.629316 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: Close
10:32:29.629927 [debug] [Thread-1  ]: Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  syntax error at or near ")"
  LINE 9:   );
            ^
  compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:32:29.630363 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5163ffc-884e-469b-8557-4572c29dca4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f096140e280>]}
10:32:29.630904 [error] [Thread-1  ]: 2 of 2 ERROR creating view model public.my_second_dbt_model..................... [[31mERROR[0m in 0.10s]
10:32:29.631459 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_second_dbt_model
10:32:29.632944 [debug] [MainThread]: Acquiring new postgres connection "master"
10:32:29.633245 [debug] [MainThread]: Using postgres connection "master"
10:32:29.633503 [debug] [MainThread]: On master: BEGIN
10:32:29.633738 [debug] [MainThread]: Opening a new connection, currently in state closed
10:32:29.644209 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:32:29.644561 [debug] [MainThread]: On master: COMMIT
10:32:29.644811 [debug] [MainThread]: Using postgres connection "master"
10:32:29.645039 [debug] [MainThread]: On master: COMMIT
10:32:29.645367 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:32:29.645692 [debug] [MainThread]: On master: Close
10:32:29.646301 [info ] [MainThread]: 
10:32:29.647223 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0.35s.
10:32:29.647669 [debug] [MainThread]: Connection 'master' was properly closed.
10:32:29.647889 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:32:29.648092 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_second_dbt_model' was properly closed.
10:32:29.655677 [info ] [MainThread]: 
10:32:29.656183 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:32:29.656737 [info ] [MainThread]: 
10:32:29.657142 [error] [MainThread]: [33mDatabase Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)[0m
10:32:29.657548 [error] [MainThread]:   syntax error at or near ")"
10:32:29.657926 [error] [MainThread]:   LINE 9:   );
10:32:29.658291 [error] [MainThread]:             ^
10:32:29.658651 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:32:29.659035 [info ] [MainThread]: 
10:32:29.659460 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
10:32:29.659983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f096137bc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f096036ebb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0962f187c0>]}


============================== 2022-03-06 10:33:30.324247 | c7ba29b2-33a6-44d9-87ab-c9805c6c595e ==============================
10:33:30.324247 [info ] [MainThread]: Running with dbt=1.0.3
10:33:30.325354 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:33:30.325796 [debug] [MainThread]: Tracking: tracking
10:33:30.341325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac934f9730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac934f9ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac934f9670>]}
10:33:30.400888 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 0 files changed.
10:33:30.401468 [debug] [MainThread]: Partial parsing: deleted file: Analytics_dbt://models/example/my_second_dbt_model.sql
10:33:30.414866 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'my_second_dbt_model' in the 'models' section of file 'models/example/schema.yml'
10:33:30.452594 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.Analytics_dbt.unique_my_second_dbt_model_id.57a0f8c493' (models/example/schema.yml) depends on a node named 'my_second_dbt_model' which was not found
10:33:30.453191 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.Analytics_dbt.not_null_my_second_dbt_model_id.151b76d778' (models/example/schema.yml) depends on a node named 'my_second_dbt_model' which was not found
10:33:30.466243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c7ba29b2-33a6-44d9-87ab-c9805c6c595e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac919570d0>]}
10:33:30.474630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c7ba29b2-33a6-44d9-87ab-c9805c6c595e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac91a928b0>]}
10:33:30.475115 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:33:30.477142 [info ] [MainThread]: 
10:33:30.477920 [debug] [MainThread]: Acquiring new postgres connection "master"
10:33:30.479123 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:33:30.494493 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:33:30.494906 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:33:30.495182 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:33:30.511536 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
10:33:30.514050 [debug] [ThreadPool]: On list_adludio: Close
10:33:30.521123 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:33:30.530502 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:33:30.530810 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:33:30.531058 [debug] [ThreadPool]: Opening a new connection, currently in state closed
10:33:30.541837 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:33:30.542154 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:33:30.542395 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:33:30.546519 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:33:30.548831 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:33:30.552387 [debug] [ThreadPool]: On list_adludio_public: Close
10:33:30.558795 [debug] [MainThread]: Using postgres connection "master"
10:33:30.559089 [debug] [MainThread]: On master: BEGIN
10:33:30.559335 [debug] [MainThread]: Opening a new connection, currently in state init
10:33:30.570113 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:33:30.570461 [debug] [MainThread]: Using postgres connection "master"
10:33:30.570743 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:33:30.594850 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:33:30.597771 [debug] [MainThread]: On master: ROLLBACK
10:33:30.598236 [debug] [MainThread]: Using postgres connection "master"
10:33:30.598512 [debug] [MainThread]: On master: BEGIN
10:33:30.598981 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:33:30.599252 [debug] [MainThread]: On master: COMMIT
10:33:30.599510 [debug] [MainThread]: Using postgres connection "master"
10:33:30.599729 [debug] [MainThread]: On master: COMMIT
10:33:30.600051 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:33:30.600331 [debug] [MainThread]: On master: Close
10:33:30.600916 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:33:30.601721 [info ] [MainThread]: 
10:33:30.606014 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:33:30.606470 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:33:30.607471 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.607737 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:33:30.608032 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:33:30.611787 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:33:30.612303 [debug] [Thread-1  ]: finished collecting timing info
10:33:30.612578 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:33:30.707402 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:33:30.708061 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.708338 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:33:30.708562 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:33:30.720330 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:33:30.720681 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.720975 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select * from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:33:30.727520 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
10:33:30.736951 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.737257 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:33:30.737863 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:33:30.741435 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.741732 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:33:30.745283 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:33:30.761862 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:33:30.762169 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.762409 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:33:30.765309 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:33:30.772561 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.772841 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:33:30.776143 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
10:33:30.777935 [debug] [Thread-1  ]: finished collecting timing info
10:33:30.778241 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:33:30.778886 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c7ba29b2-33a6-44d9-87ab-c9805c6c595e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac918d0c70>]}
10:33:30.779472 [info ] [Thread-1  ]: 1 of 1 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2037[0m in 0.17s]
10:33:30.783927 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:33:30.785280 [debug] [MainThread]: Acquiring new postgres connection "master"
10:33:30.785654 [debug] [MainThread]: Using postgres connection "master"
10:33:30.785897 [debug] [MainThread]: On master: BEGIN
10:33:30.786125 [debug] [MainThread]: Opening a new connection, currently in state closed
10:33:30.793779 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:33:30.793989 [debug] [MainThread]: On master: COMMIT
10:33:30.794138 [debug] [MainThread]: Using postgres connection "master"
10:33:30.794274 [debug] [MainThread]: On master: COMMIT
10:33:30.794765 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:33:30.794949 [debug] [MainThread]: On master: Close
10:33:30.795386 [info ] [MainThread]: 
10:33:30.795763 [info ] [MainThread]: Finished running 1 table model in 0.32s.
10:33:30.796136 [debug] [MainThread]: Connection 'master' was properly closed.
10:33:30.796338 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:33:30.802237 [info ] [MainThread]: 
10:33:30.802712 [info ] [MainThread]: [32mCompleted successfully[0m
10:33:30.803297 [info ] [MainThread]: 
10:33:30.803708 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
10:33:30.804227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac918ef190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac918ef1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac93b27df0>]}


============================== 2022-03-06 10:35:43.873777 | 9297734d-af77-4b03-bc36-cbed21e901dc ==============================
10:35:43.873777 [info ] [MainThread]: Running with dbt=1.0.3
10:35:43.875191 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:35:43.875587 [debug] [MainThread]: Tracking: tracking
10:35:43.880960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b7425730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b7425ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b7425670>]}
10:35:43.925073 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:35:43.925909 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:35:43.945154 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:35:44.016178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9297734d-af77-4b03-bc36-cbed21e901dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b50220d0>]}
10:35:44.024774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9297734d-af77-4b03-bc36-cbed21e901dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b5022eb0>]}
10:35:44.025319 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:35:44.027369 [info ] [MainThread]: 
10:35:44.028323 [debug] [MainThread]: Acquiring new postgres connection "master"
10:35:44.029527 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:35:44.046722 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:35:44.047193 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:35:44.047517 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:35:44.060834 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:35:44.063313 [debug] [ThreadPool]: On list_adludio: Close
10:35:44.067690 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:35:44.146697 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:35:44.147134 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:35:44.147438 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:35:44.158835 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:35:44.159162 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:35:44.159408 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:35:44.162790 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:35:44.164902 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:35:44.165369 [debug] [ThreadPool]: On list_adludio_public: Close
10:35:44.171911 [debug] [MainThread]: Using postgres connection "master"
10:35:44.172191 [debug] [MainThread]: On master: BEGIN
10:35:44.172417 [debug] [MainThread]: Opening a new connection, currently in state init
10:35:44.186492 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:35:44.186830 [debug] [MainThread]: Using postgres connection "master"
10:35:44.187081 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:35:44.203488 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:35:44.205952 [debug] [MainThread]: On master: ROLLBACK
10:35:44.206425 [debug] [MainThread]: Using postgres connection "master"
10:35:44.206689 [debug] [MainThread]: On master: BEGIN
10:35:44.207128 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:35:44.207416 [debug] [MainThread]: On master: COMMIT
10:35:44.207675 [debug] [MainThread]: Using postgres connection "master"
10:35:44.207928 [debug] [MainThread]: On master: COMMIT
10:35:44.208247 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:35:44.208502 [debug] [MainThread]: On master: Close
10:35:44.209067 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:35:44.209872 [info ] [MainThread]: 
10:35:44.214243 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:35:44.214683 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:35:44.215442 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:35:44.215720 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:35:44.215978 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:35:44.219964 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:35:44.220506 [debug] [Thread-1  ]: finished collecting timing info
10:35:44.220824 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:35:44.268057 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:35:44.268804 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:35:44.269631 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:35:44.269916 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:35:44.280615 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:35:44.280985 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:35:44.281353 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select Deal_id from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:35:44.282060 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal_id" does not exist
LINE 18:     select Deal_id from sales_table
                    ^
HINT:  Perhaps you meant to reference the column "sales_table.Deal_id".

10:35:44.282354 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: ROLLBACK
10:35:44.282890 [debug] [Thread-1  ]: finished collecting timing info
10:35:44.283184 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:35:44.283766 [debug] [Thread-1  ]: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  column "deal_id" does not exist
  LINE 18:     select Deal_id from sales_table
                      ^
  HINT:  Perhaps you meant to reference the column "sales_table.Deal_id".
  compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:35:44.284318 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9297734d-af77-4b03-bc36-cbed21e901dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b5a110a0>]}
10:35:44.284893 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.my_first_dbt_model..................... [[31mERROR[0m in 0.07s]
10:35:44.285524 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:35:44.287076 [debug] [MainThread]: Acquiring new postgres connection "master"
10:35:44.287379 [debug] [MainThread]: Using postgres connection "master"
10:35:44.287610 [debug] [MainThread]: On master: BEGIN
10:35:44.287828 [debug] [MainThread]: Opening a new connection, currently in state closed
10:35:44.304489 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
10:35:44.304888 [debug] [MainThread]: On master: COMMIT
10:35:44.305142 [debug] [MainThread]: Using postgres connection "master"
10:35:44.305400 [debug] [MainThread]: On master: COMMIT
10:35:44.305801 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:35:44.306086 [debug] [MainThread]: On master: Close
10:35:44.306743 [info ] [MainThread]: 
10:35:44.307486 [info ] [MainThread]: Finished running 1 table model in 0.28s.
10:35:44.308129 [debug] [MainThread]: Connection 'master' was properly closed.
10:35:44.308434 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:35:44.308659 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:35:44.326011 [info ] [MainThread]: 
10:35:44.326521 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:35:44.327284 [info ] [MainThread]: 
10:35:44.328050 [error] [MainThread]: [33mDatabase Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
10:35:44.328476 [error] [MainThread]:   column "deal_id" does not exist
10:35:44.328867 [error] [MainThread]:   LINE 18:     select Deal_id from sales_table
10:35:44.329292 [error] [MainThread]:                       ^
10:35:44.329786 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "sales_table.Deal_id".
10:35:44.330190 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:35:44.330571 [info ] [MainThread]: 
10:35:44.330959 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
10:35:44.331490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b742a8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b742a310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b5a11040>]}


============================== 2022-03-06 10:36:35.521547 | 7799dc36-f9a5-43a7-a811-56e0b3879fb2 ==============================
10:36:35.521547 [info ] [MainThread]: Running with dbt=1.0.3
10:36:35.522471 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:36:35.522828 [debug] [MainThread]: Tracking: tracking
10:36:35.530515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d60f4b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d60f4460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d60f4640>]}
10:36:35.569422 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:36:35.570228 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:36:35.589224 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:36:35.653617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7799dc36-f9a5-43a7-a811-56e0b3879fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d40f0ee0>]}
10:36:35.662022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7799dc36-f9a5-43a7-a811-56e0b3879fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d60f4730>]}
10:36:35.662521 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:36:35.664482 [info ] [MainThread]: 
10:36:35.665254 [debug] [MainThread]: Acquiring new postgres connection "master"
10:36:35.666464 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:36:35.681374 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:36:35.681706 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:36:35.681973 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:36:35.695181 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:36:35.697489 [debug] [ThreadPool]: On list_adludio: Close
10:36:35.699791 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:36:35.709968 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:36:35.710269 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:36:35.710510 [debug] [ThreadPool]: Opening a new connection, currently in state closed
10:36:35.721285 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:36:35.721676 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:36:35.721931 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:36:35.725109 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:36:35.727185 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:36:35.727572 [debug] [ThreadPool]: On list_adludio_public: Close
10:36:35.733710 [debug] [MainThread]: Using postgres connection "master"
10:36:35.733990 [debug] [MainThread]: On master: BEGIN
10:36:35.734234 [debug] [MainThread]: Opening a new connection, currently in state init
10:36:35.744869 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:36:35.745168 [debug] [MainThread]: Using postgres connection "master"
10:36:35.745438 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:36:35.769773 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:36:35.820292 [debug] [MainThread]: On master: ROLLBACK
10:36:35.820880 [debug] [MainThread]: Using postgres connection "master"
10:36:35.821183 [debug] [MainThread]: On master: BEGIN
10:36:35.821704 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:36:35.821982 [debug] [MainThread]: On master: COMMIT
10:36:35.822230 [debug] [MainThread]: Using postgres connection "master"
10:36:35.822447 [debug] [MainThread]: On master: COMMIT
10:36:35.822776 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:36:35.823038 [debug] [MainThread]: On master: Close
10:36:35.823597 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:36:35.824544 [info ] [MainThread]: 
10:36:35.828857 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:36:35.829312 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:36:35.830124 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:36:35.830392 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:36:35.830654 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:36:35.834294 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:36:35.834792 [debug] [Thread-1  ]: finished collecting timing info
10:36:35.835072 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:36:35.877853 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:36:35.878521 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:36:35.878804 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:36:35.879047 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:36:35.890026 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:36:35.890321 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:36:35.890569 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select Deal _Status from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:36:35.891181 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal" does not exist
LINE 18:     select Deal _Status from sales_table
                    ^

10:36:35.891456 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: ROLLBACK
10:36:35.891988 [debug] [Thread-1  ]: finished collecting timing info
10:36:35.892277 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:36:35.892825 [debug] [Thread-1  ]: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  column "deal" does not exist
  LINE 18:     select Deal _Status from sales_table
                      ^
  compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:36:35.893316 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7799dc36-f9a5-43a7-a811-56e0b3879fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d46df8e0>]}
10:36:35.893885 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.my_first_dbt_model..................... [[31mERROR[0m in 0.06s]
10:36:35.894539 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:36:35.896017 [debug] [MainThread]: Acquiring new postgres connection "master"
10:36:35.896332 [debug] [MainThread]: Using postgres connection "master"
10:36:35.896572 [debug] [MainThread]: On master: BEGIN
10:36:35.896787 [debug] [MainThread]: Opening a new connection, currently in state closed
10:36:35.907370 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:36:35.907701 [debug] [MainThread]: On master: COMMIT
10:36:35.907954 [debug] [MainThread]: Using postgres connection "master"
10:36:35.908189 [debug] [MainThread]: On master: COMMIT
10:36:35.908529 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:36:35.908796 [debug] [MainThread]: On master: Close
10:36:35.909429 [info ] [MainThread]: 
10:36:35.910416 [info ] [MainThread]: Finished running 1 table model in 0.24s.
10:36:35.911655 [debug] [MainThread]: Connection 'master' was properly closed.
10:36:35.912156 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:36:35.919273 [info ] [MainThread]: 
10:36:35.919764 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:36:35.920243 [info ] [MainThread]: 
10:36:35.920644 [error] [MainThread]: [33mDatabase Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
10:36:35.921061 [error] [MainThread]:   column "deal" does not exist
10:36:35.921470 [error] [MainThread]:   LINE 18:     select Deal _Status from sales_table
10:36:35.921852 [error] [MainThread]:                       ^
10:36:35.922256 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:36:35.922655 [info ] [MainThread]: 
10:36:35.923048 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
10:36:35.923571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d69f60a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d69f6d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d46dfd90>]}


============================== 2022-03-06 10:36:55.735378 | 53b1d7eb-9912-4cb0-bad6-f4e8005377dc ==============================
10:36:55.735378 [info ] [MainThread]: Running with dbt=1.0.3
10:36:55.736829 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:36:55.737615 [debug] [MainThread]: Tracking: tracking
10:36:55.743972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cc1115b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cc1115460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cc1115640>]}
10:36:55.825704 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:36:55.826525 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:36:55.848849 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:36:55.929808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '53b1d7eb-9912-4cb0-bad6-f4e8005377dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cbed0fee0>]}
10:36:55.935661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '53b1d7eb-9912-4cb0-bad6-f4e8005377dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cc1115730>]}
10:36:55.936176 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:36:55.939030 [info ] [MainThread]: 
10:36:55.940143 [debug] [MainThread]: Acquiring new postgres connection "master"
10:36:55.941643 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:36:55.957556 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:36:55.957920 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:36:55.958209 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:36:55.973952 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
10:36:55.976373 [debug] [ThreadPool]: On list_adludio: Close
10:36:55.979866 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:36:55.990952 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:36:55.991259 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:36:55.991489 [debug] [ThreadPool]: Opening a new connection, currently in state closed
10:36:56.003123 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:36:56.003442 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:36:56.003695 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:36:56.009634 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:36:56.011778 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:36:56.012175 [debug] [ThreadPool]: On list_adludio_public: Close
10:36:56.020192 [debug] [MainThread]: Using postgres connection "master"
10:36:56.020509 [debug] [MainThread]: On master: BEGIN
10:36:56.020778 [debug] [MainThread]: Opening a new connection, currently in state init
10:36:56.036167 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
10:36:56.036476 [debug] [MainThread]: Using postgres connection "master"
10:36:56.036727 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:36:56.079783 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
10:36:56.142510 [debug] [MainThread]: On master: ROLLBACK
10:36:56.143045 [debug] [MainThread]: Using postgres connection "master"
10:36:56.143374 [debug] [MainThread]: On master: BEGIN
10:36:56.143932 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:36:56.144244 [debug] [MainThread]: On master: COMMIT
10:36:56.144517 [debug] [MainThread]: Using postgres connection "master"
10:36:56.144812 [debug] [MainThread]: On master: COMMIT
10:36:56.148443 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:36:56.148761 [debug] [MainThread]: On master: Close
10:36:56.149388 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:36:56.150381 [info ] [MainThread]: 
10:36:56.154578 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:36:56.155058 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:36:56.155822 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:36:56.156133 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:36:56.156400 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:36:56.160781 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:36:56.161499 [debug] [Thread-1  ]: finished collecting timing info
10:36:56.161803 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:36:56.220505 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:36:56.221225 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:36:56.221545 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:36:56.221779 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:36:56.235037 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:36:56.235419 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:36:56.235690 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select Deal_Status from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:36:56.236340 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal_status" does not exist
LINE 18:     select Deal_Status from sales_table
                    ^
HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".

10:36:56.236622 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: ROLLBACK
10:36:56.237185 [debug] [Thread-1  ]: finished collecting timing info
10:36:56.240949 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:36:56.241613 [debug] [Thread-1  ]: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  column "deal_status" does not exist
  LINE 18:     select Deal_Status from sales_table
                      ^
  HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
  compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:36:56.242105 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '53b1d7eb-9912-4cb0-bad6-f4e8005377dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cbf700940>]}
10:36:56.242681 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.my_first_dbt_model..................... [[31mERROR[0m in 0.09s]
10:36:56.243581 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:36:56.245782 [debug] [MainThread]: Acquiring new postgres connection "master"
10:36:56.246107 [debug] [MainThread]: Using postgres connection "master"
10:36:56.246345 [debug] [MainThread]: On master: BEGIN
10:36:56.246569 [debug] [MainThread]: Opening a new connection, currently in state closed
10:36:56.270595 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
10:36:56.270947 [debug] [MainThread]: On master: COMMIT
10:36:56.271205 [debug] [MainThread]: Using postgres connection "master"
10:36:56.271437 [debug] [MainThread]: On master: COMMIT
10:36:56.271769 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:36:56.272038 [debug] [MainThread]: On master: Close
10:36:56.272622 [info ] [MainThread]: 
10:36:56.273348 [info ] [MainThread]: Finished running 1 table model in 0.33s.
10:36:56.274006 [debug] [MainThread]: Connection 'master' was properly closed.
10:36:56.274276 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:36:56.282237 [info ] [MainThread]: 
10:36:56.282710 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:36:56.283203 [info ] [MainThread]: 
10:36:56.283625 [error] [MainThread]: [33mDatabase Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
10:36:56.284047 [error] [MainThread]:   column "deal_status" does not exist
10:36:56.284452 [error] [MainThread]:   LINE 18:     select Deal_Status from sales_table
10:36:56.284862 [error] [MainThread]:                       ^
10:36:56.285325 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
10:36:56.285886 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:36:56.286324 [info ] [MainThread]: 
10:36:56.286732 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
10:36:56.287281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cc1a170a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cc1a17d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cbf700460>]}


============================== 2022-03-06 10:37:14.619915 | c7fe64a4-6df0-48cd-8852-ab8d46360eb9 ==============================
10:37:14.619915 [info ] [MainThread]: Running with dbt=1.0.3
10:37:14.628289 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:37:14.628756 [debug] [MainThread]: Tracking: tracking
10:37:14.634185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19f0c03b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19f0c03460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19f0c03640>]}
10:37:14.709865 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:37:14.710685 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:37:14.739319 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:37:14.820486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c7fe64a4-6df0-48cd-8852-ab8d46360eb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19ee7feee0>]}
10:37:14.829358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c7fe64a4-6df0-48cd-8852-ab8d46360eb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19f0c03730>]}
10:37:14.829914 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:37:14.831973 [info ] [MainThread]: 
10:37:14.832797 [debug] [MainThread]: Acquiring new postgres connection "master"
10:37:14.834040 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:37:14.861204 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:37:14.861600 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:37:14.861896 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:37:14.875434 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:37:14.877928 [debug] [ThreadPool]: On list_adludio: Close
10:37:14.882971 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:37:14.908900 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:37:14.909251 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:37:14.909508 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:37:14.929910 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
10:37:14.930286 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:37:14.930542 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:37:14.937884 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:37:14.940142 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:37:14.940584 [debug] [ThreadPool]: On list_adludio_public: Close
10:37:15.003789 [debug] [MainThread]: Using postgres connection "master"
10:37:15.004190 [debug] [MainThread]: On master: BEGIN
10:37:15.004473 [debug] [MainThread]: Opening a new connection, currently in state init
10:37:15.016043 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:37:15.016455 [debug] [MainThread]: Using postgres connection "master"
10:37:15.016730 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:37:15.044708 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
10:37:15.047348 [debug] [MainThread]: On master: ROLLBACK
10:37:15.047807 [debug] [MainThread]: Using postgres connection "master"
10:37:15.048075 [debug] [MainThread]: On master: BEGIN
10:37:15.048568 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:37:15.048826 [debug] [MainThread]: On master: COMMIT
10:37:15.049055 [debug] [MainThread]: Using postgres connection "master"
10:37:15.049290 [debug] [MainThread]: On master: COMMIT
10:37:15.049737 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:37:15.050003 [debug] [MainThread]: On master: Close
10:37:15.050577 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:37:15.052992 [info ] [MainThread]: 
10:37:15.070104 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:37:15.070879 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:37:15.075582 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:37:15.075882 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:37:15.076297 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:37:15.080253 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:37:15.080764 [debug] [Thread-1  ]: finished collecting timing info
10:37:15.081127 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:37:15.144562 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:37:15.145251 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:37:15.145578 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:37:15.145821 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:37:15.156678 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:37:15.156990 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:37:15.157223 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Status" from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:37:15.157886 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "Deal_Status" does not exist
LINE 18:     select "Deal_Status" from sales_table
                    ^
HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".

10:37:15.158165 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: ROLLBACK
10:37:15.158729 [debug] [Thread-1  ]: finished collecting timing info
10:37:15.159014 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:37:15.159616 [debug] [Thread-1  ]: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  column "Deal_Status" does not exist
  LINE 18:     select "Deal_Status" from sales_table
                      ^
  HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
  compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:37:15.160084 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c7fe64a4-6df0-48cd-8852-ab8d46360eb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19ef0936d0>]}
10:37:15.160647 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.my_first_dbt_model..................... [[31mERROR[0m in 0.09s]
10:37:15.161231 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:37:15.165918 [debug] [MainThread]: Acquiring new postgres connection "master"
10:37:15.166218 [debug] [MainThread]: Using postgres connection "master"
10:37:15.166449 [debug] [MainThread]: On master: BEGIN
10:37:15.166666 [debug] [MainThread]: Opening a new connection, currently in state closed
10:37:15.184729 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
10:37:15.185154 [debug] [MainThread]: On master: COMMIT
10:37:15.185448 [debug] [MainThread]: Using postgres connection "master"
10:37:15.185758 [debug] [MainThread]: On master: COMMIT
10:37:15.186132 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:37:15.186426 [debug] [MainThread]: On master: Close
10:37:15.187071 [info ] [MainThread]: 
10:37:15.188049 [info ] [MainThread]: Finished running 1 table model in 0.35s.
10:37:15.189203 [debug] [MainThread]: Connection 'master' was properly closed.
10:37:15.189510 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:37:15.189775 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:37:15.204323 [info ] [MainThread]: 
10:37:15.205309 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:37:15.206460 [info ] [MainThread]: 
10:37:15.207171 [error] [MainThread]: [33mDatabase Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
10:37:15.207644 [error] [MainThread]:   column "Deal_Status" does not exist
10:37:15.208393 [error] [MainThread]:   LINE 18:     select "Deal_Status" from sales_table
10:37:15.208983 [error] [MainThread]:                       ^
10:37:15.210078 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
10:37:15.210918 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:37:15.211796 [info ] [MainThread]: 
10:37:15.212484 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
10:37:15.213385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19f150cb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19f150c940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19ef0859d0>]}


============================== 2022-03-06 10:37:44.097140 | 33d81efc-9345-4a00-9042-3589ba2eeb17 ==============================
10:37:44.097140 [info ] [MainThread]: Running with dbt=1.0.3
10:37:44.097995 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:37:44.098346 [debug] [MainThread]: Tracking: tracking
10:37:44.104038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff366663b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff366663460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff366663640>]}
10:37:44.143004 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:37:44.143836 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:37:44.162869 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:37:44.227934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '33d81efc-9345-4a00-9042-3589ba2eeb17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff36425dee0>]}
10:37:44.236292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '33d81efc-9345-4a00-9042-3589ba2eeb17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff366663730>]}
10:37:44.236793 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:37:44.238761 [info ] [MainThread]: 
10:37:44.239652 [debug] [MainThread]: Acquiring new postgres connection "master"
10:37:44.240856 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:37:44.255955 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:37:44.256305 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:37:44.256570 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:37:44.269776 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:37:44.272080 [debug] [ThreadPool]: On list_adludio: Close
10:37:44.273738 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:37:44.284135 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:37:44.284456 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:37:44.284693 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:37:44.295290 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:37:44.295617 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:37:44.295860 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:37:44.299054 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:37:44.301181 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:37:44.301600 [debug] [ThreadPool]: On list_adludio_public: Close
10:37:44.361979 [debug] [MainThread]: Using postgres connection "master"
10:37:44.362398 [debug] [MainThread]: On master: BEGIN
10:37:44.362659 [debug] [MainThread]: Opening a new connection, currently in state init
10:37:44.373239 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:37:44.373624 [debug] [MainThread]: Using postgres connection "master"
10:37:44.373882 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:37:44.397320 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:37:44.399847 [debug] [MainThread]: On master: ROLLBACK
10:37:44.400341 [debug] [MainThread]: Using postgres connection "master"
10:37:44.400599 [debug] [MainThread]: On master: BEGIN
10:37:44.401055 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:37:44.401322 [debug] [MainThread]: On master: COMMIT
10:37:44.401625 [debug] [MainThread]: Using postgres connection "master"
10:37:44.401869 [debug] [MainThread]: On master: COMMIT
10:37:44.402208 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:37:44.402469 [debug] [MainThread]: On master: Close
10:37:44.403068 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:37:44.404085 [info ] [MainThread]: 
10:37:44.408780 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:37:44.409275 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:37:44.410074 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:37:44.410335 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:37:44.410600 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:37:44.414421 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:37:44.415061 [debug] [Thread-1  ]: finished collecting timing info
10:37:44.415429 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:37:44.458727 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:37:44.459465 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:37:44.459763 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:37:44.459996 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:37:44.470640 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:37:44.471010 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:37:44.471261 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select sales_table."Deal_Status" from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:37:44.472003 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column sales_table.Deal_Status does not exist
LINE 18:     select sales_table."Deal_Status" from sales_table
                    ^
HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".

10:37:44.472275 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: ROLLBACK
10:37:44.472809 [debug] [Thread-1  ]: finished collecting timing info
10:37:44.473100 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:37:44.473697 [debug] [Thread-1  ]: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  column sales_table.Deal_Status does not exist
  LINE 18:     select sales_table."Deal_Status" from sales_table
                      ^
  HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
  compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:37:44.474164 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '33d81efc-9345-4a00-9042-3589ba2eeb17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff364af1760>]}
10:37:44.474727 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.my_first_dbt_model..................... [[31mERROR[0m in 0.06s]
10:37:44.475288 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:37:44.476981 [debug] [MainThread]: Acquiring new postgres connection "master"
10:37:44.477283 [debug] [MainThread]: Using postgres connection "master"
10:37:44.477564 [debug] [MainThread]: On master: BEGIN
10:37:44.477801 [debug] [MainThread]: Opening a new connection, currently in state closed
10:37:44.488280 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:37:44.488749 [debug] [MainThread]: On master: COMMIT
10:37:44.489054 [debug] [MainThread]: Using postgres connection "master"
10:37:44.489312 [debug] [MainThread]: On master: COMMIT
10:37:44.489711 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:37:44.489981 [debug] [MainThread]: On master: Close
10:37:44.490614 [info ] [MainThread]: 
10:37:44.491508 [info ] [MainThread]: Finished running 1 table model in 0.25s.
10:37:44.492311 [debug] [MainThread]: Connection 'master' was properly closed.
10:37:44.492559 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:37:44.492770 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:37:44.500431 [info ] [MainThread]: 
10:37:44.500936 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:37:44.501664 [info ] [MainThread]: 
10:37:44.502331 [error] [MainThread]: [33mDatabase Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
10:37:44.502753 [error] [MainThread]:   column sales_table.Deal_Status does not exist
10:37:44.503144 [error] [MainThread]:   LINE 18:     select sales_table."Deal_Status" from sales_table
10:37:44.503527 [error] [MainThread]:                       ^
10:37:44.503895 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
10:37:44.504294 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:37:44.504702 [info ] [MainThread]: 
10:37:44.505091 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
10:37:44.505747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff36428ae20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff36428a910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff364af1e80>]}


============================== 2022-03-06 10:38:08.471909 | 42e71d29-0d4e-4a40-885f-6117bf53b8a1 ==============================
10:38:08.471909 [info ] [MainThread]: Running with dbt=1.0.3
10:38:08.473004 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:38:08.473437 [debug] [MainThread]: Tracking: tracking
10:38:08.485286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c05897b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c05897460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c05897640>]}
10:38:08.525240 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
10:38:08.525720 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
10:38:08.543131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '42e71d29-0d4e-4a40-885f-6117bf53b8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c03d230d0>]}
10:38:08.558696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '42e71d29-0d4e-4a40-885f-6117bf53b8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c03d71850>]}
10:38:08.559245 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:38:08.562163 [info ] [MainThread]: 
10:38:08.563350 [debug] [MainThread]: Acquiring new postgres connection "master"
10:38:08.565036 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:38:08.580424 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:38:08.580809 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:38:08.581101 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:38:08.593899 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:38:08.596195 [debug] [ThreadPool]: On list_adludio: Close
10:38:08.598364 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:38:08.607510 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:38:08.607861 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:38:08.608138 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:38:08.622468 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:38:08.622889 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:38:08.623167 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:38:08.626714 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:38:08.639115 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:38:08.639602 [debug] [ThreadPool]: On list_adludio_public: Close
10:38:08.646704 [debug] [MainThread]: Using postgres connection "master"
10:38:08.647047 [debug] [MainThread]: On master: BEGIN
10:38:08.647286 [debug] [MainThread]: Opening a new connection, currently in state init
10:38:08.658233 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:38:08.658547 [debug] [MainThread]: Using postgres connection "master"
10:38:08.658795 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:38:08.683505 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:38:08.686046 [debug] [MainThread]: On master: ROLLBACK
10:38:08.686490 [debug] [MainThread]: Using postgres connection "master"
10:38:08.686771 [debug] [MainThread]: On master: BEGIN
10:38:08.687229 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:38:08.687508 [debug] [MainThread]: On master: COMMIT
10:38:08.687756 [debug] [MainThread]: Using postgres connection "master"
10:38:08.688004 [debug] [MainThread]: On master: COMMIT
10:38:08.688338 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:38:08.688601 [debug] [MainThread]: On master: Close
10:38:08.689202 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:38:08.690251 [info ] [MainThread]: 
10:38:08.702065 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:38:08.702571 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:38:08.703369 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:08.703681 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:38:08.703970 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:38:08.711722 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:38:08.712282 [debug] [Thread-1  ]: finished collecting timing info
10:38:08.712567 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:38:08.767257 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:38:08.767973 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:08.768293 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:38:08.768577 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:38:08.780245 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:38:08.780548 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:08.780781 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select sales_table."Deal_Status" from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:38:08.781503 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column sales_table.Deal_Status does not exist
LINE 18:     select sales_table."Deal_Status" from sales_table
                    ^
HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".

10:38:08.781774 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: ROLLBACK
10:38:08.782298 [debug] [Thread-1  ]: finished collecting timing info
10:38:08.782593 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:38:08.783230 [debug] [Thread-1  ]: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  column sales_table.Deal_Status does not exist
  LINE 18:     select sales_table."Deal_Status" from sales_table
                      ^
  HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
  compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:38:08.783735 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '42e71d29-0d4e-4a40-885f-6117bf53b8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c00ec4a90>]}
10:38:08.784505 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.my_first_dbt_model..................... [[31mERROR[0m in 0.08s]
10:38:08.785129 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:38:08.786771 [debug] [MainThread]: Acquiring new postgres connection "master"
10:38:08.787097 [debug] [MainThread]: Using postgres connection "master"
10:38:08.787358 [debug] [MainThread]: On master: BEGIN
10:38:08.787580 [debug] [MainThread]: Opening a new connection, currently in state closed
10:38:08.801380 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:38:08.801867 [debug] [MainThread]: On master: COMMIT
10:38:08.802184 [debug] [MainThread]: Using postgres connection "master"
10:38:08.802453 [debug] [MainThread]: On master: COMMIT
10:38:08.802816 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:38:08.803161 [debug] [MainThread]: On master: Close
10:38:08.803785 [info ] [MainThread]: 
10:38:08.804739 [info ] [MainThread]: Finished running 1 table model in 0.24s.
10:38:08.805245 [debug] [MainThread]: Connection 'master' was properly closed.
10:38:08.805692 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:38:08.805936 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:38:08.813061 [info ] [MainThread]: 
10:38:08.815344 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:38:08.815893 [info ] [MainThread]: 
10:38:08.816689 [error] [MainThread]: [33mDatabase Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
10:38:08.817299 [error] [MainThread]:   column sales_table.Deal_Status does not exist
10:38:08.818035 [error] [MainThread]:   LINE 18:     select sales_table."Deal_Status" from sales_table
10:38:08.818576 [error] [MainThread]:                       ^
10:38:08.818992 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
10:38:08.819414 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:38:08.819980 [info ] [MainThread]: 
10:38:08.820456 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
10:38:08.821141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c03d05fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c03cf4c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c00ec4970>]}


============================== 2022-03-06 10:38:20.107805 | 515daeb8-dd24-495b-8d3e-c23ee5561df8 ==============================
10:38:20.107805 [info ] [MainThread]: Running with dbt=1.0.3
10:38:20.109483 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:38:20.109913 [debug] [MainThread]: Tracking: tracking
10:38:20.119570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afa2b8370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afa2b8160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afa2b8340>]}
10:38:20.178441 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:38:20.179717 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:38:20.203225 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:38:20.287619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '515daeb8-dd24-495b-8d3e-c23ee5561df8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3af80ca0d0>]}
10:38:20.296196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '515daeb8-dd24-495b-8d3e-c23ee5561df8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afa2b8af0>]}
10:38:20.296733 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:38:20.298738 [info ] [MainThread]: 
10:38:20.299535 [debug] [MainThread]: Acquiring new postgres connection "master"
10:38:20.300792 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:38:20.318597 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:38:20.319021 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:38:20.319332 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:38:20.332923 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:38:20.335337 [debug] [ThreadPool]: On list_adludio: Close
10:38:20.337804 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:38:20.348384 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:38:20.348707 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:38:20.348959 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:38:20.360442 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:38:20.364002 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:38:20.364253 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:38:20.367480 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:38:20.369652 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:38:20.370075 [debug] [ThreadPool]: On list_adludio_public: Close
10:38:20.438881 [debug] [MainThread]: Using postgres connection "master"
10:38:20.439261 [debug] [MainThread]: On master: BEGIN
10:38:20.439526 [debug] [MainThread]: Opening a new connection, currently in state init
10:38:20.449662 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:38:20.450000 [debug] [MainThread]: Using postgres connection "master"
10:38:20.450253 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:38:20.476587 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
10:38:20.479102 [debug] [MainThread]: On master: ROLLBACK
10:38:20.479604 [debug] [MainThread]: Using postgres connection "master"
10:38:20.479871 [debug] [MainThread]: On master: BEGIN
10:38:20.480316 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:38:20.480587 [debug] [MainThread]: On master: COMMIT
10:38:20.480852 [debug] [MainThread]: Using postgres connection "master"
10:38:20.481084 [debug] [MainThread]: On master: COMMIT
10:38:20.481403 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:38:20.481688 [debug] [MainThread]: On master: Close
10:38:20.482270 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:38:20.482786 [info ] [MainThread]: 
10:38:20.489614 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:38:20.490104 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:38:20.490884 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.491152 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:38:20.491423 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:38:20.495250 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:38:20.495748 [debug] [Thread-1  ]: finished collecting timing info
10:38:20.496031 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:38:20.561926 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:38:20.562836 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.563127 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:38:20.563357 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:38:20.574589 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:38:20.574905 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.575145 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal _Status" from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:38:20.580853 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
10:38:20.590796 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.591104 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:38:20.591676 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:38:20.595303 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.595577 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:38:20.596107 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:38:20.614489 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:38:20.614844 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.615104 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:38:20.617240 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:38:20.626104 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.626404 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:38:20.629543 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
10:38:20.631419 [debug] [Thread-1  ]: finished collecting timing info
10:38:20.631713 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:38:20.634193 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '515daeb8-dd24-495b-8d3e-c23ee5561df8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3af80fd460>]}
10:38:20.634725 [info ] [Thread-1  ]: 1 of 1 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2037[0m in 0.14s]
10:38:20.635292 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:38:20.636867 [debug] [MainThread]: Acquiring new postgres connection "master"
10:38:20.637172 [debug] [MainThread]: Using postgres connection "master"
10:38:20.637408 [debug] [MainThread]: On master: BEGIN
10:38:20.637667 [debug] [MainThread]: Opening a new connection, currently in state closed
10:38:20.648503 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:38:20.648812 [debug] [MainThread]: On master: COMMIT
10:38:20.649062 [debug] [MainThread]: Using postgres connection "master"
10:38:20.649325 [debug] [MainThread]: On master: COMMIT
10:38:20.649712 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:38:20.649979 [debug] [MainThread]: On master: Close
10:38:20.650567 [info ] [MainThread]: 
10:38:20.651446 [info ] [MainThread]: Finished running 1 table model in 0.35s.
10:38:20.651966 [debug] [MainThread]: Connection 'master' was properly closed.
10:38:20.652249 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:38:20.652490 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:38:20.662947 [info ] [MainThread]: 
10:38:20.663520 [info ] [MainThread]: [32mCompleted successfully[0m
10:38:20.664096 [info ] [MainThread]: 
10:38:20.664534 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
10:38:20.665128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3af87a3700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afa8f77c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afa8fcca0>]}


============================== 2022-03-06 11:32:11.237269 | 20c16366-0478-4d45-b105-06ba5c66ac27 ==============================
11:32:11.237269 [info ] [MainThread]: Running with dbt=1.0.3
11:32:11.238149 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:32:11.238497 [debug] [MainThread]: Tracking: tracking
11:32:11.243504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a49fa190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a49fa370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a49fa7c0>]}
11:32:11.283191 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 2 files added, 0 files changed.
11:32:11.283974 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/schema.yml
11:32:11.284427 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
11:32:11.284819 [debug] [MainThread]: Partial parsing: deleted file: Analytics_dbt://models/example/my_first_dbt_model.sql
11:32:11.303847 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
11:32:11.335403 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'my_first_dbt_model' in the 'models' section of file 'models/Sales Numbers/schema.yml'
11:32:11.338196 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'my_second_dbt_model' in the 'models' section of file 'models/Sales Numbers/schema.yml'
11:32:11.367033 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.Analytics_dbt.unique_my_first_dbt_model_id.16e066b321' (models/Sales Numbers/schema.yml) depends on a node named 'my_first_dbt_model' which was not found
11:32:11.367601 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.Analytics_dbt.not_null_my_first_dbt_model_id.5fb22c2710' (models/Sales Numbers/schema.yml) depends on a node named 'my_first_dbt_model' which was not found
11:32:11.368097 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.Analytics_dbt.unique_my_second_dbt_model_id.57a0f8c493' (models/Sales Numbers/schema.yml) depends on a node named 'my_second_dbt_model' which was not found
11:32:11.368530 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.Analytics_dbt.not_null_my_second_dbt_model_id.151b76d778' (models/Sales Numbers/schema.yml) depends on a node named 'my_second_dbt_model' which was not found
11:32:11.374805 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:32:11.382714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '20c16366-0478-4d45-b105-06ba5c66ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a40690d0>]}
11:32:11.391373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '20c16366-0478-4d45-b105-06ba5c66ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a49fac70>]}
11:32:11.391875 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:32:11.393856 [info ] [MainThread]: 
11:32:11.394647 [debug] [MainThread]: Acquiring new postgres connection "master"
11:32:11.395971 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:32:11.412464 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:32:11.412857 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:32:11.413137 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:32:11.426100 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
11:32:11.428488 [debug] [ThreadPool]: On list_adludio: Close
11:32:11.430199 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:32:11.491339 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:32:11.491703 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:32:11.491963 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:32:11.502674 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:32:11.502981 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:32:11.503219 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:32:11.506418 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
11:32:11.508498 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:32:11.508883 [debug] [ThreadPool]: On list_adludio_public: Close
11:32:11.515543 [debug] [MainThread]: Using postgres connection "master"
11:32:11.515851 [debug] [MainThread]: On master: BEGIN
11:32:11.516087 [debug] [MainThread]: Opening a new connection, currently in state init
11:32:11.526677 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:32:11.527014 [debug] [MainThread]: Using postgres connection "master"
11:32:11.527257 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:32:11.550992 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
11:32:11.553525 [debug] [MainThread]: On master: ROLLBACK
11:32:11.554010 [debug] [MainThread]: Using postgres connection "master"
11:32:11.554262 [debug] [MainThread]: On master: BEGIN
11:32:11.554730 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:32:11.555014 [debug] [MainThread]: On master: COMMIT
11:32:11.555255 [debug] [MainThread]: Using postgres connection "master"
11:32:11.555508 [debug] [MainThread]: On master: COMMIT
11:32:11.555849 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:32:11.556112 [debug] [MainThread]: On master: Close
11:32:11.556691 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:32:11.557715 [info ] [MainThread]: 
11:32:11.562300 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:32:11.562777 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:32:11.563768 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:32:11.564034 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:32:11.564296 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:32:11.568161 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:32:11.568685 [debug] [Thread-1  ]: finished collecting timing info
11:32:11.568981 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:32:11.611958 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:32:11.612676 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:32:11.612976 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:32:11.613214 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:32:11.623719 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:32:11.624096 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:32:11.624361 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value", DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:32:11.630161 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
11:32:11.639772 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:32:11.640107 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:32:11.640748 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:32:11.656964 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:32:11.657330 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:32:11.657610 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:32:11.659488 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:32:11.666708 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:32:11.667027 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:32:11.667546 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:32:11.669637 [debug] [Thread-1  ]: finished collecting timing info
11:32:11.669963 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:32:11.670686 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20c16366-0478-4d45-b105-06ba5c66ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a4957a90>]}
11:32:11.671226 [info ] [Thread-1  ]: 1 of 1 OK created table model public.deal_value_per_week........................ [[32mSELECT 2037[0m in 0.11s]
11:32:11.672247 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:32:11.673990 [debug] [MainThread]: Acquiring new postgres connection "master"
11:32:11.674313 [debug] [MainThread]: Using postgres connection "master"
11:32:11.674577 [debug] [MainThread]: On master: BEGIN
11:32:11.674797 [debug] [MainThread]: Opening a new connection, currently in state closed
11:32:11.685205 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:32:11.685621 [debug] [MainThread]: On master: COMMIT
11:32:11.685919 [debug] [MainThread]: Using postgres connection "master"
11:32:11.686157 [debug] [MainThread]: On master: COMMIT
11:32:11.686500 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:32:11.686762 [debug] [MainThread]: On master: Close
11:32:11.687356 [info ] [MainThread]: 
11:32:11.688063 [info ] [MainThread]: Finished running 1 table model in 0.29s.
11:32:11.688738 [debug] [MainThread]: Connection 'master' was properly closed.
11:32:11.688979 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:32:11.689219 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:32:11.696553 [info ] [MainThread]: 
11:32:11.697055 [info ] [MainThread]: [32mCompleted successfully[0m
11:32:11.697569 [info ] [MainThread]: 
11:32:11.697984 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
11:32:11.698621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a6448a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a49f2b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a49e1160>]}


============================== 2022-03-06 11:37:12.923275 | 949dc2f4-c414-41f1-844f-1de23f5b7c83 ==============================
11:37:12.923275 [info ] [MainThread]: Running with dbt=1.0.3
11:37:12.924106 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:37:12.924445 [debug] [MainThread]: Tracking: tracking
11:37:12.929581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec527f460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec527f970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec527f070>]}
11:37:12.983391 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:37:12.984221 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
11:37:13.009790 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
11:37:13.044647 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:37:13.052861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '949dc2f4-c414-41f1-844f-1de23f5b7c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec2ec10d0>]}
11:37:13.070260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '949dc2f4-c414-41f1-844f-1de23f5b7c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec3809610>]}
11:37:13.070806 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:37:13.072820 [info ] [MainThread]: 
11:37:13.073677 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:13.074897 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:37:13.090730 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:37:13.091108 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:37:13.091358 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:13.109369 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
11:37:13.112237 [debug] [ThreadPool]: On list_adludio: Close
11:37:13.123319 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:37:13.132980 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:13.133352 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:37:13.133701 [debug] [ThreadPool]: Opening a new connection, currently in state closed
11:37:13.144920 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:37:13.145299 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:13.145608 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:37:13.153706 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.01 seconds
11:37:13.155952 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:37:13.156427 [debug] [ThreadPool]: On list_adludio_public: Close
11:37:13.164603 [debug] [MainThread]: Using postgres connection "master"
11:37:13.164981 [debug] [MainThread]: On master: BEGIN
11:37:13.165249 [debug] [MainThread]: Opening a new connection, currently in state init
11:37:13.185032 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:37:13.185516 [debug] [MainThread]: Using postgres connection "master"
11:37:13.185821 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:37:13.213993 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:37:13.216832 [debug] [MainThread]: On master: ROLLBACK
11:37:13.217411 [debug] [MainThread]: Using postgres connection "master"
11:37:13.217723 [debug] [MainThread]: On master: BEGIN
11:37:13.218168 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:37:13.218454 [debug] [MainThread]: On master: COMMIT
11:37:13.218694 [debug] [MainThread]: Using postgres connection "master"
11:37:13.218904 [debug] [MainThread]: On master: COMMIT
11:37:13.219231 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:13.219484 [debug] [MainThread]: On master: Close
11:37:13.222288 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:37:13.222783 [info ] [MainThread]: 
11:37:13.226796 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:37:13.227328 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:37:13.228374 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:13.228640 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:37:13.228902 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:37:13.232716 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:13.233275 [debug] [Thread-1  ]: finished collecting timing info
11:37:13.233589 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:37:13.357881 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:13.358595 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:13.358891 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:37:13.359120 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:37:13.373026 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:37:13.373396 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:13.373724 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value", DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
ORDER BY week ASC
group by week O

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:37:13.374267 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "group"
LINE 24: group by week O
         ^

11:37:13.374545 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
11:37:13.375084 [debug] [Thread-1  ]: finished collecting timing info
11:37:13.375362 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:37:13.375932 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "group"
  LINE 24: group by week O
           ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:13.379875 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '949dc2f4-c414-41f1-844f-1de23f5b7c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec3855670>]}
11:37:13.380470 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.15s]
11:37:13.381036 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:37:13.382768 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:13.383090 [debug] [MainThread]: Using postgres connection "master"
11:37:13.383323 [debug] [MainThread]: On master: BEGIN
11:37:13.383532 [debug] [MainThread]: Opening a new connection, currently in state closed
11:37:13.394097 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:37:13.394513 [debug] [MainThread]: On master: COMMIT
11:37:13.394787 [debug] [MainThread]: Using postgres connection "master"
11:37:13.395026 [debug] [MainThread]: On master: COMMIT
11:37:13.395381 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:13.395647 [debug] [MainThread]: On master: Close
11:37:13.396273 [info ] [MainThread]: 
11:37:13.397010 [info ] [MainThread]: Finished running 1 table model in 0.32s.
11:37:13.398520 [debug] [MainThread]: Connection 'master' was properly closed.
11:37:13.398798 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:37:13.410256 [info ] [MainThread]: 
11:37:13.411067 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
11:37:13.411731 [info ] [MainThread]: 
11:37:13.412295 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
11:37:13.412855 [error] [MainThread]:   syntax error at or near "group"
11:37:13.413395 [error] [MainThread]:   LINE 24: group by week O
11:37:13.413954 [error] [MainThread]:            ^
11:37:13.414492 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:13.414941 [info ] [MainThread]: 
11:37:13.415333 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
11:37:13.415893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec1e99a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec1e999a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec3855ee0>]}


============================== 2022-03-06 11:37:31.272660 | 122eacb6-708c-438d-97d4-448021060007 ==============================
11:37:31.272660 [info ] [MainThread]: Running with dbt=1.0.3
11:37:31.274045 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:37:31.274500 [debug] [MainThread]: Tracking: tracking
11:37:31.284338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c868e730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c868eac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c868e670>]}
11:37:31.341234 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
11:37:31.341673 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
11:37:31.342310 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:37:31.352841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '122eacb6-708c-438d-97d4-448021060007', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c6b1b0d0>]}
11:37:31.364174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '122eacb6-708c-438d-97d4-448021060007', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c6b65f10>]}
11:37:31.364787 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:37:31.366990 [info ] [MainThread]: 
11:37:31.367831 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:31.369200 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:37:31.384963 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:37:31.385396 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:37:31.385737 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:31.401258 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
11:37:31.403777 [debug] [ThreadPool]: On list_adludio: Close
11:37:31.405596 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:37:31.418977 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:31.419618 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:37:31.419958 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:31.430930 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:37:31.431365 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:31.431665 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:37:31.434930 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.0 seconds
11:37:31.437296 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:37:31.437783 [debug] [ThreadPool]: On list_adludio_public: Close
11:37:31.444570 [debug] [MainThread]: Using postgres connection "master"
11:37:31.444945 [debug] [MainThread]: On master: BEGIN
11:37:31.445229 [debug] [MainThread]: Opening a new connection, currently in state init
11:37:31.459074 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:37:31.459483 [debug] [MainThread]: Using postgres connection "master"
11:37:31.459731 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:37:31.493665 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:37:31.496206 [debug] [MainThread]: On master: ROLLBACK
11:37:31.496691 [debug] [MainThread]: Using postgres connection "master"
11:37:31.496983 [debug] [MainThread]: On master: BEGIN
11:37:31.497504 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:37:31.497788 [debug] [MainThread]: On master: COMMIT
11:37:31.498034 [debug] [MainThread]: Using postgres connection "master"
11:37:31.498282 [debug] [MainThread]: On master: COMMIT
11:37:31.498618 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:31.498876 [debug] [MainThread]: On master: Close
11:37:31.499497 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:37:31.500379 [info ] [MainThread]: 
11:37:31.508557 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:37:31.509060 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:37:31.509944 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:31.510209 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:37:31.510473 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:37:31.514321 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:31.514834 [debug] [Thread-1  ]: finished collecting timing info
11:37:31.515133 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:37:31.563230 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:31.564031 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:31.564336 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:37:31.564596 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:37:31.581968 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
11:37:31.582449 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:31.582767 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value", DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
ORDER BY week ASC
group by week O

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:37:31.583415 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "group"
LINE 24: group by week O
         ^

11:37:31.583717 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
11:37:31.584274 [debug] [Thread-1  ]: finished collecting timing info
11:37:31.584584 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:37:31.586574 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "group"
  LINE 24: group by week O
           ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:31.587080 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '122eacb6-708c-438d-97d4-448021060007', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c46af970>]}
11:37:31.587649 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.08s]
11:37:31.588223 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:37:31.590090 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:31.590397 [debug] [MainThread]: Using postgres connection "master"
11:37:31.590640 [debug] [MainThread]: On master: BEGIN
11:37:31.590856 [debug] [MainThread]: Opening a new connection, currently in state closed
11:37:31.607974 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:37:31.608438 [debug] [MainThread]: On master: COMMIT
11:37:31.608729 [debug] [MainThread]: Using postgres connection "master"
11:37:31.608992 [debug] [MainThread]: On master: COMMIT
11:37:31.609370 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:31.609697 [debug] [MainThread]: On master: Close
11:37:31.612172 [info ] [MainThread]: 
11:37:31.612899 [info ] [MainThread]: Finished running 1 table model in 0.24s.
11:37:31.613939 [debug] [MainThread]: Connection 'master' was properly closed.
11:37:31.614231 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:37:31.614483 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:37:31.624953 [info ] [MainThread]: 
11:37:31.625749 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
11:37:31.626283 [info ] [MainThread]: 
11:37:31.626870 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
11:37:31.627455 [error] [MainThread]:   syntax error at or near "group"
11:37:31.627917 [error] [MainThread]:   LINE 24: group by week O
11:37:31.628288 [error] [MainThread]:            ^
11:37:31.628668 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:31.632479 [info ] [MainThread]: 
11:37:31.632967 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
11:37:31.639741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c6aeac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c6aeaee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c46af910>]}


============================== 2022-03-06 11:37:37.289234 | 8373085d-6276-4ea6-94ca-c6bff77483be ==============================
11:37:37.289234 [info ] [MainThread]: Running with dbt=1.0.3
11:37:37.290097 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:37:37.290449 [debug] [MainThread]: Tracking: tracking
11:37:37.295363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb99f8c370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb99f8c310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb99f8c250>]}
11:37:37.334712 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:37:37.335669 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
11:37:37.355591 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
11:37:37.379697 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:37:37.387691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8373085d-6276-4ea6-94ca-c6bff77483be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb97bc80d0>]}
11:37:37.396435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8373085d-6276-4ea6-94ca-c6bff77483be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb99f66400>]}
11:37:37.396944 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:37:37.398910 [info ] [MainThread]: 
11:37:37.399880 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:37.401228 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:37:37.416390 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:37:37.416763 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:37:37.417072 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:37.429991 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
11:37:37.432313 [debug] [ThreadPool]: On list_adludio: Close
11:37:37.434033 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:37:37.443204 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:37.443539 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:37:37.443773 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:37.454349 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:37:37.454685 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:37.454925 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:37:37.458100 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.0 seconds
11:37:37.460285 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:37:37.460680 [debug] [ThreadPool]: On list_adludio_public: Close
11:37:37.467250 [debug] [MainThread]: Using postgres connection "master"
11:37:37.467558 [debug] [MainThread]: On master: BEGIN
11:37:37.467797 [debug] [MainThread]: Opening a new connection, currently in state init
11:37:37.478308 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:37:37.478664 [debug] [MainThread]: Using postgres connection "master"
11:37:37.478913 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:37:37.506426 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:37:37.508838 [debug] [MainThread]: On master: ROLLBACK
11:37:37.509354 [debug] [MainThread]: Using postgres connection "master"
11:37:37.509692 [debug] [MainThread]: On master: BEGIN
11:37:37.510153 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:37:37.510417 [debug] [MainThread]: On master: COMMIT
11:37:37.510657 [debug] [MainThread]: Using postgres connection "master"
11:37:37.510879 [debug] [MainThread]: On master: COMMIT
11:37:37.511201 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:37.511461 [debug] [MainThread]: On master: Close
11:37:37.512073 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:37:37.512827 [info ] [MainThread]: 
11:37:37.517544 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:37:37.518029 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:37:37.518840 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:37.519123 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:37:37.519481 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:37:37.523586 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:37.524102 [debug] [Thread-1  ]: finished collecting timing info
11:37:37.524381 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:37:37.625268 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:37.626014 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:37.626294 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:37:37.626515 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:37:37.637050 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:37:37.637416 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:37.637709 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value", DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week O

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:37:37.638213 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "O"
LINE 23: group by week O
                       ^

11:37:37.638477 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
11:37:37.638999 [debug] [Thread-1  ]: finished collecting timing info
11:37:37.639288 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:37:37.639891 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "O"
  LINE 23: group by week O
                         ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:37.640383 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8373085d-6276-4ea6-94ca-c6bff77483be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb9851ce20>]}
11:37:37.640953 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.12s]
11:37:37.641661 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:37:37.643181 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:37.643480 [debug] [MainThread]: Using postgres connection "master"
11:37:37.643726 [debug] [MainThread]: On master: BEGIN
11:37:37.643935 [debug] [MainThread]: Opening a new connection, currently in state closed
11:37:37.654411 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:37:37.654781 [debug] [MainThread]: On master: COMMIT
11:37:37.655029 [debug] [MainThread]: Using postgres connection "master"
11:37:37.655292 [debug] [MainThread]: On master: COMMIT
11:37:37.655622 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:37.655874 [debug] [MainThread]: On master: Close
11:37:37.656474 [info ] [MainThread]: 
11:37:37.657177 [info ] [MainThread]: Finished running 1 table model in 0.26s.
11:37:37.657896 [debug] [MainThread]: Connection 'master' was properly closed.
11:37:37.658148 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:37:37.658362 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:37:37.665443 [info ] [MainThread]: 
11:37:37.665946 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
11:37:37.666405 [info ] [MainThread]: 
11:37:37.666878 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
11:37:37.667290 [error] [MainThread]:   syntax error at or near "O"
11:37:37.667658 [error] [MainThread]:   LINE 23: group by week O
11:37:37.668088 [error] [MainThread]:                          ^
11:37:37.668508 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:37.668972 [info ] [MainThread]: 
11:37:37.669382 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
11:37:37.670029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcba4d3cdc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb9619b490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb9851c160>]}


============================== 2022-03-06 11:37:52.238856 | 6b8f27cb-9fb2-451d-b076-95502d53f23f ==============================
11:37:52.238856 [info ] [MainThread]: Running with dbt=1.0.3
11:37:52.243887 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:37:52.244396 [debug] [MainThread]: Tracking: tracking
11:37:52.250764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b5122c730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b5122cac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b5122c670>]}
11:37:52.301873 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:37:52.302621 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
11:37:52.325291 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
11:37:52.349527 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:37:52.357931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6b8f27cb-9fb2-451d-b076-95502d53f23f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b4ee700d0>]}
11:37:52.366765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6b8f27cb-9fb2-451d-b076-95502d53f23f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b4f7b6dc0>]}
11:37:52.367280 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:37:52.369647 [info ] [MainThread]: 
11:37:52.370627 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:52.372127 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:37:52.389069 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:37:52.389491 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:37:52.389722 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:52.417664 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.03 seconds
11:37:52.420248 [debug] [ThreadPool]: On list_adludio: Close
11:37:52.423968 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:37:52.434771 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:52.435176 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:37:52.435450 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:52.445853 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:37:52.446182 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:52.446430 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:37:52.456257 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.01 seconds
11:37:52.458459 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:37:52.458894 [debug] [ThreadPool]: On list_adludio_public: Close
11:37:52.465520 [debug] [MainThread]: Using postgres connection "master"
11:37:52.465814 [debug] [MainThread]: On master: BEGIN
11:37:52.466055 [debug] [MainThread]: Opening a new connection, currently in state init
11:37:52.482401 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:37:52.482768 [debug] [MainThread]: Using postgres connection "master"
11:37:52.483048 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:37:52.523785 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
11:37:52.526412 [debug] [MainThread]: On master: ROLLBACK
11:37:52.526923 [debug] [MainThread]: Using postgres connection "master"
11:37:52.527190 [debug] [MainThread]: On master: BEGIN
11:37:52.527639 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:37:52.527926 [debug] [MainThread]: On master: COMMIT
11:37:52.528201 [debug] [MainThread]: Using postgres connection "master"
11:37:52.528432 [debug] [MainThread]: On master: COMMIT
11:37:52.528796 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:52.529058 [debug] [MainThread]: On master: Close
11:37:52.529759 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:37:52.530706 [info ] [MainThread]: 
11:37:52.542930 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:37:52.543533 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:37:52.551468 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:52.551804 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:37:52.552107 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:37:52.556075 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:52.556615 [debug] [Thread-1  ]: finished collecting timing info
11:37:52.556893 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:37:52.676753 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:52.677445 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:52.677780 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:37:52.678009 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:37:52.688882 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:37:52.689262 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:52.689587 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value", DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:37:52.692736 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal_value" does not exist
LINE 21: select AVG(deal_value), week
                    ^
HINT:  Perhaps you meant to reference the column "source_data.Deal_Value".

11:37:52.693023 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
11:37:52.693598 [debug] [Thread-1  ]: finished collecting timing info
11:37:52.693897 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:37:52.694507 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  column "deal_value" does not exist
  LINE 21: select AVG(deal_value), week
                      ^
  HINT:  Perhaps you meant to reference the column "source_data.Deal_Value".
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:52.694964 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b8f27cb-9fb2-451d-b076-95502d53f23f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b4f802340>]}
11:37:52.695518 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.14s]
11:37:52.697940 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:37:52.702980 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:52.703299 [debug] [MainThread]: Using postgres connection "master"
11:37:52.703529 [debug] [MainThread]: On master: BEGIN
11:37:52.703738 [debug] [MainThread]: Opening a new connection, currently in state closed
11:37:52.714253 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:37:52.714607 [debug] [MainThread]: On master: COMMIT
11:37:52.714859 [debug] [MainThread]: Using postgres connection "master"
11:37:52.715092 [debug] [MainThread]: On master: COMMIT
11:37:52.715418 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:52.715678 [debug] [MainThread]: On master: Close
11:37:52.716288 [info ] [MainThread]: 
11:37:52.727717 [info ] [MainThread]: Finished running 1 table model in 0.35s.
11:37:52.728305 [debug] [MainThread]: Connection 'master' was properly closed.
11:37:52.728557 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:37:52.728761 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:37:52.737011 [info ] [MainThread]: 
11:37:52.737945 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
11:37:52.738930 [info ] [MainThread]: 
11:37:52.739593 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
11:37:52.740223 [error] [MainThread]:   column "deal_value" does not exist
11:37:52.741121 [error] [MainThread]:   LINE 21: select AVG(deal_value), week
11:37:52.741761 [error] [MainThread]:                       ^
11:37:52.742182 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "source_data.Deal_Value".
11:37:52.742701 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:52.743391 [info ] [MainThread]: 
11:37:52.743947 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
11:37:52.744859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b4ee08eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b4ee08ca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b4f802fd0>]}


============================== 2022-03-06 11:38:15.122034 | 0791b5f3-9c7f-4b06-9a26-4e80a7c4a719 ==============================
11:38:15.122034 [info ] [MainThread]: Running with dbt=1.0.3
11:38:15.123010 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:38:15.123356 [debug] [MainThread]: Tracking: tracking
11:38:15.128177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e7773eb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e7773e460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e7773e640>]}
11:38:15.209386 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:38:15.210135 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
11:38:15.233518 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
11:38:15.258121 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:38:15.266214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0791b5f3-9c7f-4b06-9a26-4e80a7c4a719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e753830d0>]}
11:38:15.275096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0791b5f3-9c7f-4b06-9a26-4e80a7c4a719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e777204c0>]}
11:38:15.275644 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:38:15.278237 [info ] [MainThread]: 
11:38:15.279065 [debug] [MainThread]: Acquiring new postgres connection "master"
11:38:15.280297 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:38:15.299772 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:38:15.300202 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:38:15.300519 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:38:15.317174 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
11:38:15.319621 [debug] [ThreadPool]: On list_adludio: Close
11:38:15.336833 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:38:15.348551 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:38:15.348904 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:38:15.349159 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:38:15.368747 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
11:38:15.369144 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:38:15.369503 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:38:15.371935 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.0 seconds
11:38:15.374106 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:38:15.374524 [debug] [ThreadPool]: On list_adludio_public: Close
11:38:15.381081 [debug] [MainThread]: Using postgres connection "master"
11:38:15.381351 [debug] [MainThread]: On master: BEGIN
11:38:15.381633 [debug] [MainThread]: Opening a new connection, currently in state init
11:38:15.391871 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:38:15.392185 [debug] [MainThread]: Using postgres connection "master"
11:38:15.392446 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:38:15.421711 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:38:15.424197 [debug] [MainThread]: On master: ROLLBACK
11:38:15.424705 [debug] [MainThread]: Using postgres connection "master"
11:38:15.424996 [debug] [MainThread]: On master: BEGIN
11:38:15.425482 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:38:15.425793 [debug] [MainThread]: On master: COMMIT
11:38:15.426057 [debug] [MainThread]: Using postgres connection "master"
11:38:15.426307 [debug] [MainThread]: On master: COMMIT
11:38:15.426650 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:38:15.426917 [debug] [MainThread]: On master: Close
11:38:15.427527 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:38:15.427997 [info ] [MainThread]: 
11:38:15.434029 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:38:15.434494 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:38:15.435257 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.435525 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:38:15.435815 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:38:15.439707 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:38:15.440615 [debug] [Thread-1  ]: finished collecting timing info
11:38:15.440894 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:38:15.559429 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:38:15.560128 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.560415 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:38:15.560653 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:38:15.575104 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:38:15.575460 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.575749 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:38:15.584767 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
11:38:15.598200 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.598510 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:38:15.599099 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:38:15.602479 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.602763 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:38:15.603381 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:38:15.622305 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:38:15.622642 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.622908 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:38:15.626926 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:38:15.637552 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.637840 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:38:15.640196 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:38:15.642054 [debug] [Thread-1  ]: finished collecting timing info
11:38:15.642370 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:38:15.643046 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0791b5f3-9c7f-4b06-9a26-4e80a7c4a719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e7437dfa0>]}
11:38:15.643593 [info ] [Thread-1  ]: 1 of 1 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.21s]
11:38:15.644577 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:38:15.650056 [debug] [MainThread]: Acquiring new postgres connection "master"
11:38:15.650376 [debug] [MainThread]: Using postgres connection "master"
11:38:15.650635 [debug] [MainThread]: On master: BEGIN
11:38:15.650867 [debug] [MainThread]: Opening a new connection, currently in state closed
11:38:15.672038 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:38:15.672404 [debug] [MainThread]: On master: COMMIT
11:38:15.672659 [debug] [MainThread]: Using postgres connection "master"
11:38:15.672921 [debug] [MainThread]: On master: COMMIT
11:38:15.673281 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:38:15.673578 [debug] [MainThread]: On master: Close
11:38:15.674174 [info ] [MainThread]: 
11:38:15.675047 [info ] [MainThread]: Finished running 1 table model in 0.40s.
11:38:15.676108 [debug] [MainThread]: Connection 'master' was properly closed.
11:38:15.676349 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:38:15.676561 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:38:15.685691 [info ] [MainThread]: 
11:38:15.686196 [info ] [MainThread]: [32mCompleted successfully[0m
11:38:15.686682 [info ] [MainThread]: 
11:38:15.687095 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
11:38:15.687622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e75315430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e75324490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e7437f8b0>]}


============================== 2022-03-06 11:38:42.311919 | 59983e57-39e7-46ba-af5a-19929a6da8b1 ==============================
11:38:42.311919 [info ] [MainThread]: Running with dbt=1.0.3
11:38:42.313050 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:38:42.313802 [debug] [MainThread]: Tracking: tracking
11:38:42.319722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58314d3b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58314d3460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58314d3640>]}
11:38:42.366688 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:38:42.367516 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
11:38:42.387569 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
11:38:42.414709 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:38:42.423158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '59983e57-39e7-46ba-af5a-19929a6da8b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f582f1180d0>]}
11:38:42.432576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '59983e57-39e7-46ba-af5a-19929a6da8b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58314b54c0>]}
11:38:42.433140 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:38:42.435179 [info ] [MainThread]: 
11:38:42.436038 [debug] [MainThread]: Acquiring new postgres connection "master"
11:38:42.437297 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:38:42.452955 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:38:42.453393 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:38:42.453736 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:38:42.469003 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
11:38:42.471550 [debug] [ThreadPool]: On list_adludio: Close
11:38:42.477055 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:38:42.489536 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:38:42.489962 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:38:42.490283 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:38:42.502408 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:38:42.502861 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:38:42.503154 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:38:42.506612 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.0 seconds
11:38:42.508874 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:38:42.511648 [debug] [ThreadPool]: On list_adludio_public: Close
11:38:42.521770 [debug] [MainThread]: Using postgres connection "master"
11:38:42.522162 [debug] [MainThread]: On master: BEGIN
11:38:42.526746 [debug] [MainThread]: Opening a new connection, currently in state init
11:38:42.537441 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:38:42.537885 [debug] [MainThread]: Using postgres connection "master"
11:38:42.538158 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:38:42.570923 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:38:42.573437 [debug] [MainThread]: On master: ROLLBACK
11:38:42.574011 [debug] [MainThread]: Using postgres connection "master"
11:38:42.574319 [debug] [MainThread]: On master: BEGIN
11:38:42.574821 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:38:42.575113 [debug] [MainThread]: On master: COMMIT
11:38:42.575381 [debug] [MainThread]: Using postgres connection "master"
11:38:42.575621 [debug] [MainThread]: On master: COMMIT
11:38:42.575952 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:38:42.576228 [debug] [MainThread]: On master: Close
11:38:42.576845 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:38:42.579512 [info ] [MainThread]: 
11:38:42.584061 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:38:42.584669 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:38:42.585918 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.586242 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:38:42.586588 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:38:42.593255 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:38:42.596915 [debug] [Thread-1  ]: finished collecting timing info
11:38:42.597279 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:38:42.722183 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:38:42.722901 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.723185 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:38:42.723406 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:38:42.733961 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:38:42.734342 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.734640 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:38:42.739600 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
11:38:42.749726 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.750082 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:38:42.750772 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:38:42.754551 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.754842 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:38:42.755538 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:38:42.772959 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:38:42.773342 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.773665 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:38:42.774893 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:38:42.782700 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.783038 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:38:42.788107 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:38:42.790343 [debug] [Thread-1  ]: finished collecting timing info
11:38:42.790702 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:38:42.793413 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59983e57-39e7-46ba-af5a-19929a6da8b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f582d711fa0>]}
11:38:42.794067 [info ] [Thread-1  ]: 1 of 1 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.21s]
11:38:42.794844 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:38:42.797148 [debug] [MainThread]: Acquiring new postgres connection "master"
11:38:42.797494 [debug] [MainThread]: Using postgres connection "master"
11:38:42.797751 [debug] [MainThread]: On master: BEGIN
11:38:42.797960 [debug] [MainThread]: Opening a new connection, currently in state closed
11:38:42.809669 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:38:42.810088 [debug] [MainThread]: On master: COMMIT
11:38:42.810420 [debug] [MainThread]: Using postgres connection "master"
11:38:42.810683 [debug] [MainThread]: On master: COMMIT
11:38:42.811028 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:38:42.811297 [debug] [MainThread]: On master: Close
11:38:42.811932 [info ] [MainThread]: 
11:38:42.812571 [info ] [MainThread]: Finished running 1 table model in 0.38s.
11:38:42.813045 [debug] [MainThread]: Connection 'master' was properly closed.
11:38:42.813239 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:38:42.813407 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:38:42.820716 [info ] [MainThread]: 
11:38:42.821224 [info ] [MainThread]: [32mCompleted successfully[0m
11:38:42.821779 [info ] [MainThread]: 
11:38:42.822203 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
11:38:42.822772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f582f0c2f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f582f0aec70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58314d3730>]}


============================== 2022-03-06 11:53:00.817872 | 24ee8774-1053-4fd5-933c-1ec33c05f739 ==============================
11:53:00.817872 [info ] [MainThread]: Running with dbt=1.0.3
11:53:00.819536 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:53:00.820040 [debug] [MainThread]: Tracking: tracking
11:53:00.827397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd790633460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd790633970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd790633070>]}
11:53:00.871595 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
11:53:00.872351 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
11:53:00.891976 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
11:53:00.920739 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:53:00.928974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '24ee8774-1053-4fd5-933c-1ec33c05f739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd78e2760d0>]}
11:53:00.938524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '24ee8774-1053-4fd5-933c-1ec33c05f739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd78ebbfac0>]}
11:53:00.939130 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:53:00.941695 [info ] [MainThread]: 
11:53:00.943045 [debug] [MainThread]: Acquiring new postgres connection "master"
11:53:00.944473 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:53:00.960677 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:53:00.961222 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:53:00.961535 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:53:00.974386 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
11:53:00.976899 [debug] [ThreadPool]: On list_adludio: Close
11:53:00.978898 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:53:00.988692 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:53:00.989072 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:53:00.989373 [debug] [ThreadPool]: Opening a new connection, currently in state closed
11:53:00.999880 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:53:01.000272 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:53:01.000522 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:53:01.003785 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.0 seconds
11:53:01.006075 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:53:01.006609 [debug] [ThreadPool]: On list_adludio_public: Close
11:53:01.013173 [debug] [MainThread]: Using postgres connection "master"
11:53:01.013581 [debug] [MainThread]: On master: BEGIN
11:53:01.013873 [debug] [MainThread]: Opening a new connection, currently in state init
11:53:01.024943 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:53:01.025344 [debug] [MainThread]: Using postgres connection "master"
11:53:01.025645 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:53:01.054810 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:53:01.057632 [debug] [MainThread]: On master: ROLLBACK
11:53:01.058141 [debug] [MainThread]: Using postgres connection "master"
11:53:01.058422 [debug] [MainThread]: On master: BEGIN
11:53:01.058872 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:53:01.059142 [debug] [MainThread]: On master: COMMIT
11:53:01.059396 [debug] [MainThread]: Using postgres connection "master"
11:53:01.059610 [debug] [MainThread]: On master: COMMIT
11:53:01.059932 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:53:01.060193 [debug] [MainThread]: On master: Close
11:53:01.060807 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:53:01.062048 [info ] [MainThread]: 
11:53:01.066614 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:53:01.067119 [info ] [Thread-1  ]: 1 of 2 START table model public.deal_value_per_week............................. [RUN]
11:53:01.068230 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.068519 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:53:01.068786 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:53:01.072824 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:53:01.073408 [debug] [Thread-1  ]: finished collecting timing info
11:53:01.073829 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:53:01.180294 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:53:01.181036 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.181340 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:53:01.181587 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:53:01.192012 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:53:01.192418 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.192671 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:53:01.197759 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
11:53:01.207739 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.208107 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:53:01.210769 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:53:01.214329 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.214621 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:53:01.215245 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:53:01.234812 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:53:01.235287 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.235581 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:53:01.236873 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:53:01.244669 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.245034 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:53:01.247360 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:53:01.249408 [debug] [Thread-1  ]: finished collecting timing info
11:53:01.249839 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:53:01.250616 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24ee8774-1053-4fd5-933c-1ec33c05f739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd790c44190>]}
11:53:01.251223 [info ] [Thread-1  ]: 1 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.18s]
11:53:01.252121 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:53:01.252481 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
11:53:01.253024 [info ] [Thread-1  ]: 2 of 2 START table model public.transformed_sales_number_data................... [RUN]
11:53:01.253962 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.254266 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
11:53:01.254543 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
11:53:01.258886 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.259441 [debug] [Thread-1  ]: finished collecting timing info
11:53:01.259746 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
11:53:01.264027 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.264597 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.264861 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
11:53:01.265089 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:53:01.277733 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:53:01.278199 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.278512 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:53:01.283555 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
11:53:01.287535 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.287857 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
11:53:01.288499 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:53:01.291348 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
11:53:01.291625 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.291891 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
11:53:01.296063 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:53:01.298836 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.299131 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
11:53:01.299656 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:53:01.301903 [debug] [Thread-1  ]: finished collecting timing info
11:53:01.302216 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
11:53:01.304996 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24ee8774-1053-4fd5-933c-1ec33c05f739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd78e210d90>]}
11:53:01.305650 [info ] [Thread-1  ]: 2 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 53[0m in 0.05s]
11:53:01.306208 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
11:53:01.307760 [debug] [MainThread]: Acquiring new postgres connection "master"
11:53:01.308083 [debug] [MainThread]: Using postgres connection "master"
11:53:01.308319 [debug] [MainThread]: On master: BEGIN
11:53:01.308535 [debug] [MainThread]: Opening a new connection, currently in state closed
11:53:01.319042 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:53:01.319453 [debug] [MainThread]: On master: COMMIT
11:53:01.319761 [debug] [MainThread]: Using postgres connection "master"
11:53:01.320026 [debug] [MainThread]: On master: COMMIT
11:53:01.320383 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:53:01.320647 [debug] [MainThread]: On master: Close
11:53:01.321367 [info ] [MainThread]: 
11:53:01.325019 [info ] [MainThread]: Finished running 2 table models in 0.38s.
11:53:01.325430 [debug] [MainThread]: Connection 'master' was properly closed.
11:53:01.325747 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
11:53:01.333084 [info ] [MainThread]: 
11:53:01.333664 [info ] [MainThread]: [32mCompleted successfully[0m
11:53:01.334281 [info ] [MainThread]: 
11:53:01.334704 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
11:53:01.335251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd78e212040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd78e21f1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd78d246d30>]}


============================== 2022-03-06 11:53:54.400226 | 2c056cdb-fae4-498d-a7fa-3548f3dd55d5 ==============================
11:53:54.400226 [info ] [MainThread]: Running with dbt=1.0.3
11:53:54.401837 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:53:54.402269 [debug] [MainThread]: Tracking: tracking
11:53:54.418268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a267c7b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a267c7460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a267c7640>]}
11:53:54.486173 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:53:54.486973 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
11:53:54.513144 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
11:53:54.550553 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:53:54.563078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2c056cdb-fae4-498d-a7fa-3548f3dd55d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a2440c0d0>]}
11:53:54.572764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2c056cdb-fae4-498d-a7fa-3548f3dd55d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a267a84c0>]}
11:53:54.573361 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:53:54.575515 [info ] [MainThread]: 
11:53:54.584844 [debug] [MainThread]: Acquiring new postgres connection "master"
11:53:54.586466 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:53:54.602393 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:53:54.602778 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:53:54.603058 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:53:54.619926 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
11:53:54.622536 [debug] [ThreadPool]: On list_adludio: Close
11:53:54.629877 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:53:54.639417 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:53:54.639786 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:53:54.640068 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:53:54.654607 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:53:54.655032 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:53:54.655393 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:53:54.660187 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
11:53:54.662565 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:53:54.670288 [debug] [ThreadPool]: On list_adludio_public: Close
11:53:54.680467 [debug] [MainThread]: Using postgres connection "master"
11:53:54.680826 [debug] [MainThread]: On master: BEGIN
11:53:54.681068 [debug] [MainThread]: Opening a new connection, currently in state init
11:53:54.697103 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:53:54.697606 [debug] [MainThread]: Using postgres connection "master"
11:53:54.697930 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:53:54.740864 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
11:53:54.744504 [debug] [MainThread]: On master: ROLLBACK
11:53:54.745063 [debug] [MainThread]: Using postgres connection "master"
11:53:54.745416 [debug] [MainThread]: On master: BEGIN
11:53:54.746106 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:53:54.747652 [debug] [MainThread]: On master: COMMIT
11:53:54.747931 [debug] [MainThread]: Using postgres connection "master"
11:53:54.748226 [debug] [MainThread]: On master: COMMIT
11:53:54.749429 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:53:54.749785 [debug] [MainThread]: On master: Close
11:53:54.752275 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:53:54.752809 [info ] [MainThread]: 
11:53:54.759868 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:53:54.760362 [info ] [Thread-1  ]: 1 of 2 START table model public.deal_value_per_week............................. [RUN]
11:53:54.761147 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.761390 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:53:54.761712 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:53:54.767105 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:53:54.767635 [debug] [Thread-1  ]: finished collecting timing info
11:53:54.767916 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:53:54.907625 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:53:54.908228 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.908450 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:53:54.908629 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:53:54.921595 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:53:54.922008 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.922276 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:53:54.927351 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
11:53:54.937586 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.937919 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:53:54.938590 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:53:54.942188 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.942472 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:53:54.946099 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:53:54.963442 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:53:54.963841 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.964324 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:53:54.965692 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:53:54.973270 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.986014 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:53:54.988497 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:53:54.990653 [debug] [Thread-1  ]: finished collecting timing info
11:53:54.991013 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:53:54.991725 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c056cdb-fae4-498d-a7fa-3548f3dd55d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a24c760d0>]}
11:53:54.992276 [info ] [Thread-1  ]: 1 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.23s]
11:53:54.993106 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:53:54.993416 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
11:53:54.993801 [info ] [Thread-1  ]: 2 of 2 START table model public.transformed_sales_number_data................... [RUN]
11:53:54.997917 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:55.001758 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
11:53:55.002037 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
11:53:55.009741 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:53:55.010381 [debug] [Thread-1  ]: finished collecting timing info
11:53:55.010677 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
11:53:55.015064 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:53:55.015608 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:55.018976 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
11:53:55.019255 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:53:55.031405 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:53:55.031834 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:55.032153 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select Deal_id as id, Deal_created_at as Deal_created_at
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency
"Deal_Region" as deal_region
from source_data
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:53:55.032725 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near ""Deal_Value""
LINE 22: "Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
         ^

11:53:55.033017 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: ROLLBACK
11:53:55.033601 [debug] [Thread-1  ]: finished collecting timing info
11:53:55.036992 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
11:53:55.037653 [debug] [Thread-1  ]: Database Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)
  syntax error at or near ""Deal_Value""
  LINE 22: "Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
           ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
11:53:55.038140 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c056cdb-fae4-498d-a7fa-3548f3dd55d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a27b5f4f0>]}
11:53:55.038793 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.transformed_sales_number_data.......... [[31mERROR[0m in 0.04s]
11:53:55.039790 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
11:53:55.043394 [debug] [MainThread]: Acquiring new postgres connection "master"
11:53:55.043711 [debug] [MainThread]: Using postgres connection "master"
11:53:55.043967 [debug] [MainThread]: On master: BEGIN
11:53:55.044176 [debug] [MainThread]: Opening a new connection, currently in state closed
11:53:55.061755 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:53:55.062206 [debug] [MainThread]: On master: COMMIT
11:53:55.062471 [debug] [MainThread]: Using postgres connection "master"
11:53:55.062706 [debug] [MainThread]: On master: COMMIT
11:53:55.063093 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:53:55.063364 [debug] [MainThread]: On master: Close
11:53:55.064004 [info ] [MainThread]: 
11:53:55.064956 [info ] [MainThread]: Finished running 2 table models in 0.48s.
11:53:55.065483 [debug] [MainThread]: Connection 'master' was properly closed.
11:53:55.065756 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:53:55.066001 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
11:53:55.076722 [info ] [MainThread]: 
11:53:55.077590 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
11:53:55.078299 [info ] [MainThread]: 
11:53:55.078725 [error] [MainThread]: [33mDatabase Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)[0m
11:53:55.079612 [error] [MainThread]:   syntax error at or near ""Deal_Value""
11:53:55.080164 [error] [MainThread]:   LINE 22: "Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
11:53:55.080724 [error] [MainThread]:            ^
11:53:55.081203 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
11:53:55.081722 [info ] [MainThread]: 
11:53:55.082203 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
11:53:55.083039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a243a0dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a169b71f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a24c77670>]}


============================== 2022-03-06 11:54:45.850200 | 02e381f8-25e1-4824-8104-10653bf07b87 ==============================
11:54:45.850200 [info ] [MainThread]: Running with dbt=1.0.3
11:54:45.851037 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:54:45.851387 [debug] [MainThread]: Tracking: tracking
11:54:45.856150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148f785b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148f7851c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148f785b50>]}
11:54:45.895467 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:54:45.896235 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
11:54:45.915372 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
11:54:45.939755 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:54:45.947849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '02e381f8-25e1-4824-8104-10653bf07b87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148d3db0d0>]}
11:54:45.956581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '02e381f8-25e1-4824-8104-10653bf07b87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148f779460>]}
11:54:45.957141 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:54:45.959200 [info ] [MainThread]: 
11:54:45.960003 [debug] [MainThread]: Acquiring new postgres connection "master"
11:54:45.961295 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:54:45.976331 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:54:45.976709 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:54:45.977039 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:54:45.989924 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
11:54:45.992280 [debug] [ThreadPool]: On list_adludio: Close
11:54:45.994004 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:54:46.003136 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:54:46.003454 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:54:46.003684 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:54:46.014262 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:54:46.014612 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:54:46.014892 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:54:46.018087 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
11:54:46.020270 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:54:46.020658 [debug] [ThreadPool]: On list_adludio_public: Close
11:54:46.027285 [debug] [MainThread]: Using postgres connection "master"
11:54:46.027597 [debug] [MainThread]: On master: BEGIN
11:54:46.027831 [debug] [MainThread]: Opening a new connection, currently in state init
11:54:46.038310 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:54:46.038646 [debug] [MainThread]: Using postgres connection "master"
11:54:46.038898 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:54:46.071301 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:54:46.073799 [debug] [MainThread]: On master: ROLLBACK
11:54:46.074286 [debug] [MainThread]: Using postgres connection "master"
11:54:46.074561 [debug] [MainThread]: On master: BEGIN
11:54:46.075048 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:54:46.075324 [debug] [MainThread]: On master: COMMIT
11:54:46.075568 [debug] [MainThread]: Using postgres connection "master"
11:54:46.075793 [debug] [MainThread]: On master: COMMIT
11:54:46.076123 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:54:46.076383 [debug] [MainThread]: On master: Close
11:54:46.076994 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:54:46.077810 [info ] [MainThread]: 
11:54:46.082292 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:54:46.082755 [info ] [Thread-1  ]: 1 of 2 START table model public.deal_value_per_week............................. [RUN]
11:54:46.083879 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.084155 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:54:46.084450 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:54:46.089613 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:54:46.090155 [debug] [Thread-1  ]: finished collecting timing info
11:54:46.090459 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:54:46.187089 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:54:46.187783 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.188072 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:54:46.188296 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:54:46.198942 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:54:46.199258 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.199521 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:54:46.204364 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
11:54:46.214269 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.214598 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:54:46.215272 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:54:46.218729 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.219004 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:54:46.219596 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:54:46.236128 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:54:46.236507 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.236764 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:54:46.238031 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:54:46.245430 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.245780 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:54:46.247911 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:54:46.249753 [debug] [Thread-1  ]: finished collecting timing info
11:54:46.250074 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:54:46.250802 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '02e381f8-25e1-4824-8104-10653bf07b87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148fdc5190>]}
11:54:46.251393 [info ] [Thread-1  ]: 1 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.17s]
11:54:46.252371 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:54:46.252701 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
11:54:46.253248 [info ] [Thread-1  ]: 2 of 2 START table model public.transformed_sales_number_data................... [RUN]
11:54:46.254097 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:54:46.254352 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
11:54:46.254594 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
11:54:46.258432 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:54:46.258949 [debug] [Thread-1  ]: finished collecting timing info
11:54:46.259211 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
11:54:46.263370 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:54:46.263906 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:54:46.264156 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
11:54:46.264378 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:54:46.274899 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:54:46.275233 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:54:46.275479 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select Deal_id as id, Deal_created_at as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region
from source_data
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:54:46.276320 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal_id" does not exist
LINE 21: select Deal_id as id, Deal_created_at as Deal_created_at,
                ^
HINT:  Perhaps you meant to reference the column "source_data.Deal_id".

11:54:46.276586 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: ROLLBACK
11:54:46.277113 [debug] [Thread-1  ]: finished collecting timing info
11:54:46.277382 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
11:54:46.277969 [debug] [Thread-1  ]: Database Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)
  column "deal_id" does not exist
  LINE 21: select Deal_id as id, Deal_created_at as Deal_created_at,
                  ^
  HINT:  Perhaps you meant to reference the column "source_data.Deal_id".
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
11:54:46.278433 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '02e381f8-25e1-4824-8104-10653bf07b87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1490b41490>]}
11:54:46.279000 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.transformed_sales_number_data.......... [[31mERROR[0m in 0.02s]
11:54:46.279681 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
11:54:46.281381 [debug] [MainThread]: Acquiring new postgres connection "master"
11:54:46.281719 [debug] [MainThread]: Using postgres connection "master"
11:54:46.281961 [debug] [MainThread]: On master: BEGIN
11:54:46.282181 [debug] [MainThread]: Opening a new connection, currently in state closed
11:54:46.292618 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:54:46.292970 [debug] [MainThread]: On master: COMMIT
11:54:46.293221 [debug] [MainThread]: Using postgres connection "master"
11:54:46.293493 [debug] [MainThread]: On master: COMMIT
11:54:46.293842 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:54:46.294110 [debug] [MainThread]: On master: Close
11:54:46.294726 [info ] [MainThread]: 
11:54:46.296907 [info ] [MainThread]: Finished running 2 table models in 0.33s.
11:54:46.297656 [debug] [MainThread]: Connection 'master' was properly closed.
11:54:46.298160 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:54:46.298398 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
11:54:46.305877 [info ] [MainThread]: 
11:54:46.306375 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
11:54:46.306846 [info ] [MainThread]: 
11:54:46.307247 [error] [MainThread]: [33mDatabase Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)[0m
11:54:46.307639 [error] [MainThread]:   column "deal_id" does not exist
11:54:46.308010 [error] [MainThread]:   LINE 21: select Deal_id as id, Deal_created_at as Deal_created_at,
11:54:46.308382 [error] [MainThread]:                   ^
11:54:46.308772 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "source_data.Deal_id".
11:54:46.309140 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
11:54:46.309559 [info ] [MainThread]: 
11:54:46.309957 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
11:54:46.310480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148d371e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148d3775b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1490b34a60>]}


============================== 2022-03-06 11:55:04.065816 | 2510d18b-bb50-4eee-9e9f-4b8a6ca17c84 ==============================
11:55:04.065816 [info ] [MainThread]: Running with dbt=1.0.3
11:55:04.067410 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:55:04.067833 [debug] [MainThread]: Tracking: tracking
11:55:04.073597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0cfb1b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0cfb1460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0cfb1640>]}
11:55:04.142629 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:55:04.143435 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
11:55:04.163107 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
11:55:04.191063 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:55:04.199178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2510d18b-bb50-4eee-9e9f-4b8a6ca17c84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0abf50d0>]}
11:55:04.207989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2510d18b-bb50-4eee-9e9f-4b8a6ca17c84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0cf934c0>]}
11:55:04.208494 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:55:04.210589 [info ] [MainThread]: 
11:55:04.211392 [debug] [MainThread]: Acquiring new postgres connection "master"
11:55:04.212657 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:55:04.233207 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:55:04.233603 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:55:04.233894 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:55:04.250167 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
11:55:04.252661 [debug] [ThreadPool]: On list_adludio: Close
11:55:04.256574 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:55:04.266243 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:55:04.266582 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:55:04.266855 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:55:04.283428 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
11:55:04.283860 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:55:04.284158 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:55:04.287528 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
11:55:04.289810 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:55:04.290195 [debug] [ThreadPool]: On list_adludio_public: Close
11:55:04.298661 [debug] [MainThread]: Using postgres connection "master"
11:55:04.299007 [debug] [MainThread]: On master: BEGIN
11:55:04.299279 [debug] [MainThread]: Opening a new connection, currently in state init
11:55:04.317828 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:55:04.318248 [debug] [MainThread]: Using postgres connection "master"
11:55:04.318509 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:55:04.355290 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
11:55:04.357810 [debug] [MainThread]: On master: ROLLBACK
11:55:04.358303 [debug] [MainThread]: Using postgres connection "master"
11:55:04.358617 [debug] [MainThread]: On master: BEGIN
11:55:04.359077 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:55:04.359357 [debug] [MainThread]: On master: COMMIT
11:55:04.359600 [debug] [MainThread]: Using postgres connection "master"
11:55:04.359846 [debug] [MainThread]: On master: COMMIT
11:55:04.365313 [debug] [MainThread]: SQL status: COMMIT in 0.01 seconds
11:55:04.365681 [debug] [MainThread]: On master: Close
11:55:04.368288 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:55:04.372277 [info ] [MainThread]: 
11:55:04.376648 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:55:04.377149 [info ] [Thread-1  ]: 1 of 2 START table model public.deal_value_per_week............................. [RUN]
11:55:04.378285 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.378564 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:55:04.378847 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:55:04.383022 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:55:04.383630 [debug] [Thread-1  ]: finished collecting timing info
11:55:04.383923 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:55:04.500021 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:55:04.500688 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.500967 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:55:04.501243 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:55:04.514680 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:55:04.515067 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.515342 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:55:04.522919 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
11:55:04.533406 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.533794 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:55:04.534475 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:55:04.538255 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.538559 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:55:04.539245 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:55:04.559598 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:55:04.559938 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.560165 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:55:04.561386 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:55:04.571148 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.571527 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:55:04.574444 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:55:04.579535 [debug] [Thread-1  ]: finished collecting timing info
11:55:04.579880 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:55:04.582699 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2510d18b-bb50-4eee-9e9f-4b8a6ca17c84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f091d49d0>]}
11:55:04.583261 [info ] [Thread-1  ]: 1 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.20s]
11:55:04.583786 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:55:04.584080 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
11:55:04.584435 [info ] [Thread-1  ]: 2 of 2 START table model public.transformed_sales_number_data................... [RUN]
11:55:04.585109 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.585354 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
11:55:04.585655 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
11:55:04.591214 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.591696 [debug] [Thread-1  ]: finished collecting timing info
11:55:04.591951 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
11:55:04.596396 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.596949 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.597200 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
11:55:04.597417 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:55:04.615251 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
11:55:04.615669 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.616018 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region
from source_data
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:55:04.622775 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
11:55:04.626778 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.627068 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
11:55:04.627659 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:55:04.631193 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.631464 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
11:55:04.631995 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:55:04.634473 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
11:55:04.634729 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.634948 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
11:55:04.644152 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
11:55:04.647141 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.647422 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
11:55:04.653404 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
11:55:04.655400 [debug] [Thread-1  ]: finished collecting timing info
11:55:04.655747 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
11:55:04.657942 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2510d18b-bb50-4eee-9e9f-4b8a6ca17c84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f091f3e20>]}
11:55:04.658498 [info ] [Thread-1  ]: 2 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.07s]
11:55:04.659044 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
11:55:04.664475 [debug] [MainThread]: Acquiring new postgres connection "master"
11:55:04.664800 [debug] [MainThread]: Using postgres connection "master"
11:55:04.665033 [debug] [MainThread]: On master: BEGIN
11:55:04.665252 [debug] [MainThread]: Opening a new connection, currently in state closed
11:55:04.676099 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:55:04.676457 [debug] [MainThread]: On master: COMMIT
11:55:04.676706 [debug] [MainThread]: Using postgres connection "master"
11:55:04.677142 [debug] [MainThread]: On master: COMMIT
11:55:04.677488 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:55:04.677763 [debug] [MainThread]: On master: Close
11:55:04.680112 [info ] [MainThread]: 
11:55:04.680780 [info ] [MainThread]: Finished running 2 table models in 0.47s.
11:55:04.681342 [debug] [MainThread]: Connection 'master' was properly closed.
11:55:04.681601 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:55:04.681808 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
11:55:04.693171 [info ] [MainThread]: 
11:55:04.694083 [info ] [MainThread]: [32mCompleted successfully[0m
11:55:04.694798 [info ] [MainThread]: 
11:55:04.695359 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
11:55:04.696004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0ab90160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0aba11c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f091c6d30>]}


============================== 2022-03-06 11:59:15.783129 | ac4f8ee6-53b5-42c7-a82b-225fd2f84359 ==============================
11:59:15.783129 [info ] [MainThread]: Running with dbt=1.0.3
11:59:15.784489 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:59:15.784850 [debug] [MainThread]: Tracking: tracking
11:59:15.789997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442d9a43a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442d9a4c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442d9a4520>]}
11:59:15.886681 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:59:15.887483 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
11:59:15.917446 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
11:59:15.946200 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:59:15.954553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ac4f8ee6-53b5-42c7-a82b-225fd2f84359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442b5fa0d0>]}
11:59:15.963494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ac4f8ee6-53b5-42c7-a82b-225fd2f84359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442d997460>]}
11:59:15.964028 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:59:15.966071 [info ] [MainThread]: 
11:59:15.966864 [debug] [MainThread]: Acquiring new postgres connection "master"
11:59:15.968240 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:59:15.994902 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:59:15.995248 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:59:15.995497 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:59:16.010035 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
11:59:16.012482 [debug] [ThreadPool]: On list_adludio: Close
11:59:16.016541 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:59:16.025832 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:59:16.026186 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:59:16.026447 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:59:16.036966 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:59:16.040129 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:59:16.040391 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:59:16.046051 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.01 seconds
11:59:16.048032 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:59:16.058570 [debug] [ThreadPool]: On list_adludio_public: Close
11:59:16.076569 [debug] [MainThread]: Using postgres connection "master"
11:59:16.076916 [debug] [MainThread]: On master: BEGIN
11:59:16.077216 [debug] [MainThread]: Opening a new connection, currently in state init
11:59:16.109006 [debug] [MainThread]: SQL status: BEGIN in 0.03 seconds
11:59:16.109376 [debug] [MainThread]: Using postgres connection "master"
11:59:16.123932 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:59:16.165810 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
11:59:16.168459 [debug] [MainThread]: On master: ROLLBACK
11:59:16.168942 [debug] [MainThread]: Using postgres connection "master"
11:59:16.169269 [debug] [MainThread]: On master: BEGIN
11:59:16.169779 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:59:16.170061 [debug] [MainThread]: On master: COMMIT
11:59:16.170325 [debug] [MainThread]: Using postgres connection "master"
11:59:16.170560 [debug] [MainThread]: On master: COMMIT
11:59:16.170886 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:59:16.171160 [debug] [MainThread]: On master: Close
11:59:16.171823 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:59:16.172756 [info ] [MainThread]: 
11:59:16.178188 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:59:16.178654 [info ] [Thread-1  ]: 1 of 2 START table model public.deal_value_per_week............................. [RUN]
11:59:16.179463 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.179751 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:59:16.180028 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:59:16.186192 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:59:16.187198 [debug] [Thread-1  ]: finished collecting timing info
11:59:16.187852 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:59:16.345753 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:59:16.346514 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.346877 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:59:16.347156 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:59:16.357358 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:59:16.357768 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.358038 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:59:16.363075 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
11:59:16.390841 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.391249 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:59:16.391896 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:59:16.395600 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.395918 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:59:16.396486 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:59:16.433027 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:59:16.433381 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.433673 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:59:16.437938 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:59:16.446613 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.446963 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:59:16.453603 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
11:59:16.459163 [debug] [Thread-1  ]: finished collecting timing info
11:59:16.459512 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:59:16.460340 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ac4f8ee6-53b5-42c7-a82b-225fd2f84359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442d97ce20>]}
11:59:16.460893 [info ] [Thread-1  ]: 1 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.28s]
11:59:16.461587 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:59:16.462226 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
11:59:16.463429 [info ] [Thread-1  ]: 2 of 2 START table model public.transformed_sales_number_data................... [RUN]
11:59:16.464563 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.464885 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
11:59:16.465176 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
11:59:16.469944 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.470651 [debug] [Thread-1  ]: finished collecting timing info
11:59:16.471128 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
11:59:16.479299 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.479927 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.480222 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
11:59:16.480486 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:59:16.496271 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
11:59:16.496588 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.496797 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:59:16.505911 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
11:59:16.521871 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.522249 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
11:59:16.522986 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:59:16.545078 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.545540 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
11:59:16.549910 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:59:16.560554 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
11:59:16.560810 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.561000 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
11:59:16.563888 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:59:16.566205 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.566385 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
11:59:16.569415 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:59:16.570597 [debug] [Thread-1  ]: finished collecting timing info
11:59:16.570811 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
11:59:16.571372 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ac4f8ee6-53b5-42c7-a82b-225fd2f84359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4429be69a0>]}
11:59:16.571882 [info ] [Thread-1  ]: 2 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.11s]
11:59:16.572350 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
11:59:16.574026 [debug] [MainThread]: Acquiring new postgres connection "master"
11:59:16.574347 [debug] [MainThread]: Using postgres connection "master"
11:59:16.574594 [debug] [MainThread]: On master: BEGIN
11:59:16.574807 [debug] [MainThread]: Opening a new connection, currently in state closed
11:59:16.589392 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:59:16.589725 [debug] [MainThread]: On master: COMMIT
11:59:16.589918 [debug] [MainThread]: Using postgres connection "master"
11:59:16.590094 [debug] [MainThread]: On master: COMMIT
11:59:16.590365 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:59:16.590582 [debug] [MainThread]: On master: Close
11:59:16.591100 [info ] [MainThread]: 
11:59:16.591658 [info ] [MainThread]: Finished running 2 table models in 0.62s.
11:59:16.592102 [debug] [MainThread]: Connection 'master' was properly closed.
11:59:16.592291 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:59:16.592456 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
11:59:16.602734 [info ] [MainThread]: 
11:59:16.603390 [info ] [MainThread]: [32mCompleted successfully[0m
11:59:16.603872 [info ] [MainThread]: 
11:59:16.607693 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
11:59:16.608386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442e6f7b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4429bd82e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4429bc8c40>]}


============================== 2022-03-06 12:00:53.472818 | 79f1601e-c6f7-4020-bd10-349a62e1c7ed ==============================
12:00:53.472818 [info ] [MainThread]: Running with dbt=1.0.3
12:00:53.474328 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:00:53.474734 [debug] [MainThread]: Tracking: tracking
12:00:53.480572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1d3f0b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1d3f01c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1d3f00d0>]}
12:00:53.528724 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
12:00:53.529578 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:00:53.530103 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
12:00:53.556708 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:00:53.577314 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
12:00:53.586367 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:00:53.595395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '79f1601e-c6f7-4020-bd10-349a62e1c7ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1b02d0d0>]}
12:00:53.606605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '79f1601e-c6f7-4020-bd10-349a62e1c7ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1d3ca7c0>]}
12:00:53.607130 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:00:53.609216 [info ] [MainThread]: 
12:00:53.610033 [debug] [MainThread]: Acquiring new postgres connection "master"
12:00:53.611362 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:00:53.626815 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:00:53.627167 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:00:53.627433 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:00:53.641492 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
12:00:53.643884 [debug] [ThreadPool]: On list_adludio: Close
12:00:53.648149 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:00:53.658977 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:00:53.659301 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:00:53.659544 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:00:53.670420 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:00:53.670787 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:00:53.671041 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:00:53.674274 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:00:53.676478 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:00:53.676844 [debug] [ThreadPool]: On list_adludio_public: Close
12:00:53.694021 [debug] [MainThread]: Using postgres connection "master"
12:00:53.694414 [debug] [MainThread]: On master: BEGIN
12:00:53.694746 [debug] [MainThread]: Opening a new connection, currently in state init
12:00:53.712421 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
12:00:53.712990 [debug] [MainThread]: Using postgres connection "master"
12:00:53.713249 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:00:53.752225 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
12:00:53.754751 [debug] [MainThread]: On master: ROLLBACK
12:00:53.755224 [debug] [MainThread]: Using postgres connection "master"
12:00:53.755492 [debug] [MainThread]: On master: BEGIN
12:00:53.755959 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:00:53.756219 [debug] [MainThread]: On master: COMMIT
12:00:53.756458 [debug] [MainThread]: Using postgres connection "master"
12:00:53.756679 [debug] [MainThread]: On master: COMMIT
12:00:53.756995 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:00:53.757264 [debug] [MainThread]: On master: Close
12:00:53.757952 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:00:53.758972 [info ] [MainThread]: 
12:00:53.768723 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:00:53.769228 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:00:53.770052 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.770564 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:00:53.770866 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:00:53.777958 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.778527 [debug] [Thread-1  ]: finished collecting timing info
12:00:53.778824 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:00:53.898609 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.899344 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.899659 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:00:53.900180 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:00:53.913960 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:00:53.914349 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.914632 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:00:53.923617 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:00:53.934167 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.934549 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:00:53.935244 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:00:53.938685 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.938955 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:00:53.939579 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:00:53.958892 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:00:53.959309 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.959601 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:00:53.965007 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
12:00:53.975293 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.975599 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:00:53.978337 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:00:53.980214 [debug] [Thread-1  ]: finished collecting timing info
12:00:53.980519 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:00:53.981172 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '79f1601e-c6f7-4020-bd10-349a62e1c7ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1da0ee20>]}
12:00:53.985688 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.21s]
12:00:53.986241 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:00:53.986955 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:00:53.987301 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:00:53.987904 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:00:53.988131 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:00:53.988423 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:00:53.992644 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:00:53.993233 [debug] [Thread-1  ]: finished collecting timing info
12:00:53.994343 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:00:53.998872 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:00:53.999442 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:00:53.999722 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:00:53.999970 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:00:54.011904 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:00:54.012242 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:00:54.012513 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/


select .flow_99,
select AVG("adludio"."public"."transformed_sales_number_data".deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:00:54.013008 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "."
LINE 15: select .flow_99,
                ^

12:00:54.013296 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:00:54.017507 [debug] [Thread-1  ]: finished collecting timing info
12:00:54.017853 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:00:54.018472 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "."
  LINE 15: select .flow_99,
                  ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:00:54.018947 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '79f1601e-c6f7-4020-bd10-349a62e1c7ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1b9d2d90>]}
12:00:54.019521 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.03s]
12:00:54.020717 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:00:54.022418 [debug] [MainThread]: Acquiring new postgres connection "master"
12:00:54.022717 [debug] [MainThread]: Using postgres connection "master"
12:00:54.022947 [debug] [MainThread]: On master: BEGIN
12:00:54.023162 [debug] [MainThread]: Opening a new connection, currently in state closed
12:00:54.038177 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:00:54.038587 [debug] [MainThread]: On master: COMMIT
12:00:54.038883 [debug] [MainThread]: Using postgres connection "master"
12:00:54.039162 [debug] [MainThread]: On master: COMMIT
12:00:54.039518 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:00:54.039835 [debug] [MainThread]: On master: Close
12:00:54.042012 [info ] [MainThread]: 
12:00:54.042377 [info ] [MainThread]: Finished running 2 table models in 0.43s.
12:00:54.042752 [debug] [MainThread]: Connection 'master' was properly closed.
12:00:54.042929 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:00:54.043071 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:00:54.053090 [info ] [MainThread]: 
12:00:54.053503 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:00:54.053916 [info ] [MainThread]: 
12:00:54.054231 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:00:54.054577 [error] [MainThread]:   syntax error at or near "."
12:00:54.054877 [error] [MainThread]:   LINE 15: select .flow_99,
12:00:54.055180 [error] [MainThread]:                   ^
12:00:54.055471 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:00:54.055780 [info ] [MainThread]: 
12:00:54.056082 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:00:54.056512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1afc9c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d19609730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1b9d2160>]}


============================== 2022-03-06 12:17:54.233439 | 1bd23033-c94f-4147-a0e7-966013a8336e ==============================
12:17:54.233439 [info ] [MainThread]: Running with dbt=1.0.3
12:17:54.234270 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:17:54.234622 [debug] [MainThread]: Tracking: tracking
12:17:54.242413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5546981370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5546981310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5546981a90>]}
12:17:54.286020 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:17:54.286861 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:17:54.309606 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:17:54.334344 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:17:54.342567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1bd23033-c94f-4147-a0e7-966013a8336e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55445d70d0>]}
12:17:54.351812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1bd23033-c94f-4147-a0e7-966013a8336e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55469741c0>]}
12:17:54.352368 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:17:54.354513 [info ] [MainThread]: 
12:17:54.355447 [debug] [MainThread]: Acquiring new postgres connection "master"
12:17:54.356837 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:17:54.372511 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:17:54.372971 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:17:54.373297 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:17:54.387631 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
12:17:54.390208 [debug] [ThreadPool]: On list_adludio: Close
12:17:54.392054 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:17:54.401329 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:17:54.401766 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:17:54.402070 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:17:54.413812 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:17:54.414264 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:17:54.414569 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:17:54.417834 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:17:54.420170 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:17:54.420619 [debug] [ThreadPool]: On list_adludio_public: Close
12:17:54.427384 [debug] [MainThread]: Using postgres connection "master"
12:17:54.427772 [debug] [MainThread]: On master: BEGIN
12:17:54.428050 [debug] [MainThread]: Opening a new connection, currently in state init
12:17:54.441726 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:17:54.442118 [debug] [MainThread]: Using postgres connection "master"
12:17:54.442370 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:17:54.476105 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
12:17:54.478628 [debug] [MainThread]: On master: ROLLBACK
12:17:54.479156 [debug] [MainThread]: Using postgres connection "master"
12:17:54.479430 [debug] [MainThread]: On master: BEGIN
12:17:54.479881 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:17:54.480193 [debug] [MainThread]: On master: COMMIT
12:17:54.480438 [debug] [MainThread]: Using postgres connection "master"
12:17:54.480747 [debug] [MainThread]: On master: COMMIT
12:17:54.484361 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:17:54.484721 [debug] [MainThread]: On master: Close
12:17:54.485383 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:17:54.485970 [info ] [MainThread]: 
12:17:54.493404 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:17:54.493935 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:17:54.494713 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.494982 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:17:54.495253 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:17:54.500926 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.501593 [debug] [Thread-1  ]: finished collecting timing info
12:17:54.501914 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:17:54.603732 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.604450 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.604744 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:17:54.604975 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:17:54.615694 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:17:54.616052 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.616305 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:17:54.626311 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:17:54.636750 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.637174 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:17:54.637924 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:17:54.641440 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.641775 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:17:54.642420 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:17:54.659173 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:17:54.659658 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.659994 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:17:54.662630 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:17:54.670154 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.670498 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:17:54.674018 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:17:54.675945 [debug] [Thread-1  ]: finished collecting timing info
12:17:54.676294 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:17:54.676989 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1bd23033-c94f-4147-a0e7-966013a8336e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5544e428b0>]}
12:17:54.677615 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.18s]
12:17:54.678189 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:17:54.679443 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:17:54.679857 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:17:54.680578 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:17:54.680832 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:17:54.681072 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:17:54.685698 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:17:54.686307 [debug] [Thread-1  ]: finished collecting timing info
12:17:54.686618 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:17:54.690880 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:17:54.691450 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:17:54.691709 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:17:54.691928 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:17:54.702642 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:17:54.703098 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:17:54.703401 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



select AVG("adludio"."public"."transformed_sales_number_data".deal_value),
"adludio"."public"."transformed_sales_number_data".week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:17:54.704009 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "source_data" does not exist
LINE 18: from source_data
              ^

12:17:54.704318 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:17:54.704891 [debug] [Thread-1  ]: finished collecting timing info
12:17:54.705191 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:17:54.705960 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  relation "source_data" does not exist
  LINE 18: from source_data
                ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:17:54.706436 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1bd23033-c94f-4147-a0e7-966013a8336e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5547d45430>]}
12:17:54.707006 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.03s]
12:17:54.708112 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:17:54.709964 [debug] [MainThread]: Acquiring new postgres connection "master"
12:17:54.710278 [debug] [MainThread]: Using postgres connection "master"
12:17:54.710519 [debug] [MainThread]: On master: BEGIN
12:17:54.710743 [debug] [MainThread]: Opening a new connection, currently in state closed
12:17:54.721242 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:17:54.721704 [debug] [MainThread]: On master: COMMIT
12:17:54.722003 [debug] [MainThread]: Using postgres connection "master"
12:17:54.722275 [debug] [MainThread]: On master: COMMIT
12:17:54.722683 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:17:54.722992 [debug] [MainThread]: On master: Close
12:17:54.723814 [info ] [MainThread]: 
12:17:54.724532 [info ] [MainThread]: Finished running 2 table models in 0.37s.
12:17:54.725232 [debug] [MainThread]: Connection 'master' was properly closed.
12:17:54.725541 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:17:54.725847 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:17:54.733564 [info ] [MainThread]: 
12:17:54.734323 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:17:54.734975 [info ] [MainThread]: 
12:17:54.735514 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:17:54.736074 [error] [MainThread]:   relation "source_data" does not exist
12:17:54.736593 [error] [MainThread]:   LINE 18: from source_data
12:17:54.737101 [error] [MainThread]:                 ^
12:17:54.737657 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:17:54.738212 [info ] [MainThread]: 
12:17:54.738793 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:17:54.739522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55472291c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f554408c040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5547d3c940>]}


============================== 2022-03-06 12:18:33.665782 | e67dcff2-5173-487d-8305-a80edf0b196b ==============================
12:18:33.665782 [info ] [MainThread]: Running with dbt=1.0.3
12:18:33.667186 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:18:33.667648 [debug] [MainThread]: Tracking: tracking
12:18:33.673302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04008683a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0400868c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0400868520>]}
12:18:33.716963 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
12:18:33.717518 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
12:18:33.718294 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:18:33.730298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e67dcff2-5173-487d-8305-a80edf0b196b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04008685e0>]}
12:18:33.739419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e67dcff2-5173-487d-8305-a80edf0b196b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03fed51f40>]}
12:18:33.740005 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:18:33.744758 [info ] [MainThread]: 
12:18:33.745964 [debug] [MainThread]: Acquiring new postgres connection "master"
12:18:33.747654 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:18:33.763063 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:18:33.763441 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:18:33.763709 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:18:33.779889 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:18:33.782504 [debug] [ThreadPool]: On list_adludio: Close
12:18:33.784538 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:18:33.797562 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:18:33.798030 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:18:33.798340 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:18:33.808871 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:18:33.809317 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:18:33.809671 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:18:33.813006 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:18:33.815360 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:18:33.815800 [debug] [ThreadPool]: On list_adludio_public: Close
12:18:33.822623 [debug] [MainThread]: Using postgres connection "master"
12:18:33.822991 [debug] [MainThread]: On master: BEGIN
12:18:33.823265 [debug] [MainThread]: Opening a new connection, currently in state init
12:18:33.834000 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:18:33.834477 [debug] [MainThread]: Using postgres connection "master"
12:18:33.834776 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:18:33.867256 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
12:18:33.870021 [debug] [MainThread]: On master: ROLLBACK
12:18:33.870571 [debug] [MainThread]: Using postgres connection "master"
12:18:33.870890 [debug] [MainThread]: On master: BEGIN
12:18:33.871430 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:18:33.871728 [debug] [MainThread]: On master: COMMIT
12:18:33.871995 [debug] [MainThread]: Using postgres connection "master"
12:18:33.872244 [debug] [MainThread]: On master: COMMIT
12:18:33.872601 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:18:33.872894 [debug] [MainThread]: On master: Close
12:18:33.873594 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:18:33.874619 [info ] [MainThread]: 
12:18:33.880330 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:18:33.880863 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:18:33.881999 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.882283 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:18:33.882566 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:18:33.886664 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.887409 [debug] [Thread-1  ]: finished collecting timing info
12:18:33.887713 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:18:33.942822 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.943662 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.943986 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:18:33.944271 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:18:33.957212 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:18:33.957657 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.957957 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:18:33.972333 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:18:33.982547 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.982919 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:18:33.983628 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:18:33.987433 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.987717 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:18:33.988310 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:18:34.010537 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:18:34.011045 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:34.011380 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:18:34.019187 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
12:18:34.033018 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:34.033480 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:18:34.037410 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:18:34.039469 [debug] [Thread-1  ]: finished collecting timing info
12:18:34.039853 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:18:34.040668 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e67dcff2-5173-487d-8305-a80edf0b196b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03fc8bff70>]}
12:18:34.041287 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.16s]
12:18:34.041948 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:18:34.042973 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:18:34.043348 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:18:34.044693 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:18:34.044946 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:18:34.045173 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:18:34.127671 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:18:34.128457 [debug] [Thread-1  ]: finished collecting timing info
12:18:34.128868 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:18:34.133630 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:18:34.134326 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:18:34.134615 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:18:34.135089 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:18:34.148612 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:18:34.149134 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:18:34.149520 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



select AVG("adludio"."public"."transformed_sales_number_data".deal_value),
"adludio"."public"."transformed_sales_number_data".week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:18:34.150171 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "source_data" does not exist
LINE 18: from source_data
              ^

12:18:34.150467 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:18:34.151021 [debug] [Thread-1  ]: finished collecting timing info
12:18:34.151338 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:18:34.151988 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  relation "source_data" does not exist
  LINE 18: from source_data
                ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:18:34.152465 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e67dcff2-5173-487d-8305-a80edf0b196b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0400eaceb0>]}
12:18:34.153044 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.11s]
12:18:34.154213 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:18:34.155661 [debug] [MainThread]: Acquiring new postgres connection "master"
12:18:34.155965 [debug] [MainThread]: Using postgres connection "master"
12:18:34.156196 [debug] [MainThread]: On master: BEGIN
12:18:34.156436 [debug] [MainThread]: Opening a new connection, currently in state closed
12:18:34.171320 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:18:34.171784 [debug] [MainThread]: On master: COMMIT
12:18:34.172110 [debug] [MainThread]: Using postgres connection "master"
12:18:34.172370 [debug] [MainThread]: On master: COMMIT
12:18:34.172729 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:18:34.173025 [debug] [MainThread]: On master: Close
12:18:34.182172 [info ] [MainThread]: 
12:18:34.183195 [info ] [MainThread]: Finished running 2 table models in 0.44s.
12:18:34.184383 [debug] [MainThread]: Connection 'master' was properly closed.
12:18:34.184862 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:18:34.185119 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:18:34.196437 [info ] [MainThread]: 
12:18:34.197425 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:18:34.198766 [info ] [MainThread]: 
12:18:34.199502 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:18:34.200584 [error] [MainThread]:   relation "source_data" does not exist
12:18:34.201258 [error] [MainThread]:   LINE 18: from source_data
12:18:34.202516 [error] [MainThread]:                 ^
12:18:34.203490 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:18:34.204704 [info ] [MainThread]: 
12:18:34.205401 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:18:34.206518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0400e797c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0400ea7700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0400eac460>]}


============================== 2022-03-06 12:20:01.548542 | 959fc47d-d524-4136-a312-35c5589eb33d ==============================
12:20:01.548542 [info ] [MainThread]: Running with dbt=1.0.3
12:20:01.550005 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:20:01.550453 [debug] [MainThread]: Tracking: tracking
12:20:01.556680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845e69c3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845e69c7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845e69c520>]}
12:20:01.603177 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
12:20:01.603679 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
12:20:01.604413 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:20:01.616668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '959fc47d-d524-4136-a312-35c5589eb33d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845e69c0d0>]}
12:20:01.626215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '959fc47d-d524-4136-a312-35c5589eb33d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845cb86f40>]}
12:20:01.626802 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:20:01.629889 [info ] [MainThread]: 
12:20:01.630989 [debug] [MainThread]: Acquiring new postgres connection "master"
12:20:01.632581 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:20:01.648297 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:20:01.648738 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:20:01.649024 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:20:01.664911 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:20:01.667410 [debug] [ThreadPool]: On list_adludio: Close
12:20:01.669265 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:20:01.678523 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:20:01.678902 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:20:01.679165 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:20:01.691428 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:20:01.691809 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:20:01.692053 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:20:01.695328 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:20:01.697609 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:20:01.698256 [debug] [ThreadPool]: On list_adludio_public: Close
12:20:01.705104 [debug] [MainThread]: Using postgres connection "master"
12:20:01.705554 [debug] [MainThread]: On master: BEGIN
12:20:01.705861 [debug] [MainThread]: Opening a new connection, currently in state init
12:20:01.716386 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:20:01.716809 [debug] [MainThread]: Using postgres connection "master"
12:20:01.717095 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:20:01.749923 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
12:20:01.752447 [debug] [MainThread]: On master: ROLLBACK
12:20:01.753050 [debug] [MainThread]: Using postgres connection "master"
12:20:01.753351 [debug] [MainThread]: On master: BEGIN
12:20:01.753893 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:20:01.754194 [debug] [MainThread]: On master: COMMIT
12:20:01.754475 [debug] [MainThread]: Using postgres connection "master"
12:20:01.754739 [debug] [MainThread]: On master: COMMIT
12:20:01.755096 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:20:01.755406 [debug] [MainThread]: On master: Close
12:20:01.756060 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:20:01.756797 [info ] [MainThread]: 
12:20:01.762606 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:20:01.763067 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:20:01.763832 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.764106 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:20:01.764400 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:20:01.768382 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.768899 [debug] [Thread-1  ]: finished collecting timing info
12:20:01.769182 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:20:01.812540 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.813289 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.813599 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:20:01.813843 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:20:01.824534 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:20:01.825029 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.825342 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:20:01.836760 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:20:01.846780 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.847185 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:20:01.847894 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:20:01.851583 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.851894 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:20:01.852526 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:20:01.869082 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:20:01.869688 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.870038 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:20:01.872890 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:20:01.882255 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.882620 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:20:01.885155 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:20:01.887017 [debug] [Thread-1  ]: finished collecting timing info
12:20:01.887373 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:20:01.888102 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '959fc47d-d524-4136-a312-35c5589eb33d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8455cf42e0>]}
12:20:01.888677 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.12s]
12:20:01.889268 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:20:01.890490 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:20:01.890890 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:20:01.891604 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:20:01.891873 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:20:01.893822 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:20:01.951732 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:20:01.952517 [debug] [Thread-1  ]: finished collecting timing info
12:20:01.952859 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:20:01.957239 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:20:01.957941 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:20:01.958251 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:20:01.958511 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:20:01.969168 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:20:01.969552 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:20:01.969818 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



select AVG("adludio"."public"."transformed_sales_number_data".deal_value),
"adludio"."public"."transformed_sales_number_data".week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:20:01.970423 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "source_data" does not exist
LINE 18: from source_data
              ^

12:20:01.970694 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:20:01.971216 [debug] [Thread-1  ]: finished collecting timing info
12:20:01.971505 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:20:01.972070 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  relation "source_data" does not exist
  LINE 18: from source_data
                ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:20:01.972496 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '959fc47d-d524-4136-a312-35c5589eb33d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845ece1dc0>]}
12:20:01.973014 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.08s]
12:20:01.973616 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:20:01.975262 [debug] [MainThread]: Acquiring new postgres connection "master"
12:20:01.975571 [debug] [MainThread]: Using postgres connection "master"
12:20:01.975791 [debug] [MainThread]: On master: BEGIN
12:20:01.976000 [debug] [MainThread]: Opening a new connection, currently in state closed
12:20:01.986945 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:20:01.987350 [debug] [MainThread]: On master: COMMIT
12:20:01.987645 [debug] [MainThread]: Using postgres connection "master"
12:20:01.987916 [debug] [MainThread]: On master: COMMIT
12:20:01.988306 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:20:01.988634 [debug] [MainThread]: On master: Close
12:20:01.989346 [info ] [MainThread]: 
12:20:01.989882 [info ] [MainThread]: Finished running 2 table models in 0.36s.
12:20:01.990328 [debug] [MainThread]: Connection 'master' was properly closed.
12:20:01.990577 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:20:01.991062 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:20:01.999119 [info ] [MainThread]: 
12:20:01.999668 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:20:02.000433 [info ] [MainThread]: 
12:20:02.001428 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:20:02.002070 [error] [MainThread]:   relation "source_data" does not exist
12:20:02.002659 [error] [MainThread]:   LINE 18: from source_data
12:20:02.003530 [error] [MainThread]:                 ^
12:20:02.004101 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:20:02.004564 [info ] [MainThread]: 
12:20:02.005343 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:20:02.006261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845ecbff70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845ecdc220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845ece1400>]}


============================== 2022-03-06 12:21:09.922212 | a2feabf8-2df0-4fd9-a9ff-d0d26ba8588e ==============================
12:21:09.922212 [info ] [MainThread]: Running with dbt=1.0.3
12:21:09.923122 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:21:09.923509 [debug] [MainThread]: Tracking: tracking
12:21:09.929249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758a32f130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758a32f2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758a32f3d0>]}
12:21:09.987921 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:21:09.988727 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:21:10.014962 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:21:10.043397 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:21:10.051577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a2feabf8-2df0-4fd9-a9ff-d0d26ba8588e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7587f870d0>]}
12:21:10.071015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a2feabf8-2df0-4fd9-a9ff-d0d26ba8588e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758a323f10>]}
12:21:10.071570 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:21:10.073986 [info ] [MainThread]: 
12:21:10.074987 [debug] [MainThread]: Acquiring new postgres connection "master"
12:21:10.076288 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:21:10.099095 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:21:10.099497 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:21:10.099786 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:21:10.115723 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:21:10.118190 [debug] [ThreadPool]: On list_adludio: Close
12:21:10.120004 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:21:10.131405 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:21:10.131800 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:21:10.132113 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:21:10.142677 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:21:10.149637 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:21:10.150032 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:21:10.153405 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:21:10.155896 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:21:10.156336 [debug] [ThreadPool]: On list_adludio_public: Close
12:21:10.168304 [debug] [MainThread]: Using postgres connection "master"
12:21:10.168753 [debug] [MainThread]: On master: BEGIN
12:21:10.169041 [debug] [MainThread]: Opening a new connection, currently in state init
12:21:10.179436 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:21:10.179791 [debug] [MainThread]: Using postgres connection "master"
12:21:10.180061 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:21:10.219623 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
12:21:10.222391 [debug] [MainThread]: On master: ROLLBACK
12:21:10.222866 [debug] [MainThread]: Using postgres connection "master"
12:21:10.223138 [debug] [MainThread]: On master: BEGIN
12:21:10.223597 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:21:10.223853 [debug] [MainThread]: On master: COMMIT
12:21:10.224076 [debug] [MainThread]: Using postgres connection "master"
12:21:10.224285 [debug] [MainThread]: On master: COMMIT
12:21:10.224815 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:21:10.225073 [debug] [MainThread]: On master: Close
12:21:10.229007 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:21:10.229693 [info ] [MainThread]: 
12:21:10.238124 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:21:10.238641 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:21:10.239437 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.239698 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:21:10.239950 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:21:10.245035 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.245721 [debug] [Thread-1  ]: finished collecting timing info
12:21:10.246042 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:21:10.362905 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.363813 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.364183 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:21:10.364474 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:21:10.378822 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:21:10.379201 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.379472 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:21:10.389988 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:21:10.403642 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.404041 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:21:10.404762 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:21:10.408365 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.408664 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:21:10.409300 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:21:10.426879 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:21:10.427340 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.427660 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:21:10.434733 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
12:21:10.442882 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.443276 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:21:10.445974 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:21:10.447899 [debug] [Thread-1  ]: finished collecting timing info
12:21:10.448244 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:21:10.448995 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a2feabf8-2df0-4fd9-a9ff-d0d26ba8588e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758a953f40>]}
12:21:10.449630 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.21s]
12:21:10.450477 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:21:10.451439 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:21:10.451862 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:21:10.452768 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:21:10.453061 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:21:10.453340 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:21:10.457695 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:21:10.461393 [debug] [Thread-1  ]: finished collecting timing info
12:21:10.461778 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:21:10.466450 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:21:10.467230 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:21:10.467557 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:21:10.467889 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:21:10.478737 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:21:10.479138 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:21:10.479451 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(source_data.deal_value), source_data.week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:21:10.479989 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "select"
LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
             ^

12:21:10.480299 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:21:10.480904 [debug] [Thread-1  ]: finished collecting timing info
12:21:10.481235 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:21:10.481925 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "select"
  LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
               ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:21:10.482358 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a2feabf8-2df0-4fd9-a9ff-d0d26ba8588e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758b6e7400>]}
12:21:10.482890 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.03s]
12:21:10.489148 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:21:10.501376 [debug] [MainThread]: Acquiring new postgres connection "master"
12:21:10.501808 [debug] [MainThread]: Using postgres connection "master"
12:21:10.502062 [debug] [MainThread]: On master: BEGIN
12:21:10.502283 [debug] [MainThread]: Opening a new connection, currently in state closed
12:21:10.521763 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
12:21:10.522979 [debug] [MainThread]: On master: COMMIT
12:21:10.523346 [debug] [MainThread]: Using postgres connection "master"
12:21:10.523632 [debug] [MainThread]: On master: COMMIT
12:21:10.524075 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:21:10.524406 [debug] [MainThread]: On master: Close
12:21:10.525689 [info ] [MainThread]: 
12:21:10.527340 [info ] [MainThread]: Finished running 2 table models in 0.45s.
12:21:10.528252 [debug] [MainThread]: Connection 'master' was properly closed.
12:21:10.528678 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:21:10.528967 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:21:10.540343 [info ] [MainThread]: 
12:21:10.540987 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:21:10.541553 [info ] [MainThread]: 
12:21:10.542165 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:21:10.542764 [error] [MainThread]:   syntax error at or near "select"
12:21:10.545946 [error] [MainThread]:   LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
12:21:10.546409 [error] [MainThread]:                ^
12:21:10.546844 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:21:10.547255 [info ] [MainThread]: 
12:21:10.547683 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:21:10.548233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7587f06910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7586557370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758b6dd910>]}


============================== 2022-03-06 12:22:14.019597 | ddb7b315-4776-47ab-b045-895846062bf1 ==============================
12:22:14.019597 [info ] [MainThread]: Running with dbt=1.0.3
12:22:14.024260 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:22:14.024768 [debug] [MainThread]: Tracking: tracking
12:22:14.031660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e4df4b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e4df4820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e4df4b50>]}
12:22:14.108720 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:22:14.109646 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:22:14.135575 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:22:14.165866 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:22:14.178354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ddb7b315-4776-47ab-b045-895846062bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e2a4d0d0>]}
12:22:14.188106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ddb7b315-4776-47ab-b045-895846062bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e4de8460>]}
12:22:14.188651 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:22:14.191772 [info ] [MainThread]: 
12:22:14.192793 [debug] [MainThread]: Acquiring new postgres connection "master"
12:22:14.194182 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:22:14.213143 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:22:14.213604 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:22:14.213968 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:22:14.232484 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:22:14.234943 [debug] [ThreadPool]: On list_adludio: Close
12:22:14.237505 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:22:14.247145 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:22:14.247511 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:22:14.247799 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:22:14.258481 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:22:14.258891 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:22:14.259207 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:22:14.262764 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:22:14.268592 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:22:14.269069 [debug] [ThreadPool]: On list_adludio_public: Close
12:22:14.278149 [debug] [MainThread]: Using postgres connection "master"
12:22:14.278558 [debug] [MainThread]: On master: BEGIN
12:22:14.278827 [debug] [MainThread]: Opening a new connection, currently in state init
12:22:14.289661 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:22:14.290050 [debug] [MainThread]: Using postgres connection "master"
12:22:14.290326 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:22:14.324516 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
12:22:14.327152 [debug] [MainThread]: On master: ROLLBACK
12:22:14.327635 [debug] [MainThread]: Using postgres connection "master"
12:22:14.327911 [debug] [MainThread]: On master: BEGIN
12:22:14.328367 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:22:14.328629 [debug] [MainThread]: On master: COMMIT
12:22:14.328859 [debug] [MainThread]: Using postgres connection "master"
12:22:14.329119 [debug] [MainThread]: On master: COMMIT
12:22:14.329547 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:22:14.330024 [debug] [MainThread]: On master: Close
12:22:14.332847 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:22:14.333353 [info ] [MainThread]: 
12:22:14.338420 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:22:14.338883 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:22:14.339689 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.339948 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:22:14.340208 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:22:14.346293 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.346844 [debug] [Thread-1  ]: finished collecting timing info
12:22:14.347168 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:22:14.456340 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.457106 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.457407 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:22:14.457687 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:22:14.468361 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:22:14.468763 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.469070 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:22:14.479531 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:22:14.493353 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.493771 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:22:14.494469 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:22:14.498280 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.498583 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:22:14.499297 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:22:14.523771 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:22:14.524243 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.524588 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:22:14.527363 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:22:14.535230 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.535566 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:22:14.538272 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:22:14.547520 [debug] [Thread-1  ]: finished collecting timing info
12:22:14.547941 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:22:14.550597 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddb7b315-4776-47ab-b045-895846062bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e54279d0>]}
12:22:14.551295 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.21s]
12:22:14.552398 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:22:14.553623 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:22:14.554034 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:22:14.554848 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:22:14.561956 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:22:14.562315 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:22:14.566824 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:22:14.567504 [debug] [Thread-1  ]: finished collecting timing info
12:22:14.567828 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:22:14.572242 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:22:14.572862 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:22:14.573145 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:22:14.573401 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:22:14.583415 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:22:14.583798 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:22:14.584105 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:22:14.584606 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "select"
LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
             ^

12:22:14.584934 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:22:14.585551 [debug] [Thread-1  ]: finished collecting timing info
12:22:14.585895 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:22:14.586565 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "select"
  LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
               ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:22:14.587119 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddb7b315-4776-47ab-b045-895846062bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e61b8490>]}
12:22:14.592300 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.03s]
12:22:14.592864 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:22:14.595263 [debug] [MainThread]: Acquiring new postgres connection "master"
12:22:14.595580 [debug] [MainThread]: Using postgres connection "master"
12:22:14.595874 [debug] [MainThread]: On master: BEGIN
12:22:14.596092 [debug] [MainThread]: Opening a new connection, currently in state closed
12:22:14.606662 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:22:14.607103 [debug] [MainThread]: On master: COMMIT
12:22:14.607428 [debug] [MainThread]: Using postgres connection "master"
12:22:14.607709 [debug] [MainThread]: On master: COMMIT
12:22:14.608094 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:22:14.608400 [debug] [MainThread]: On master: Close
12:22:14.609117 [info ] [MainThread]: 
12:22:14.609712 [info ] [MainThread]: Finished running 2 table models in 0.42s.
12:22:14.610212 [debug] [MainThread]: Connection 'master' was properly closed.
12:22:14.610494 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:22:14.610766 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:22:14.618392 [info ] [MainThread]: 
12:22:14.618946 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:22:14.619656 [info ] [MainThread]: 
12:22:14.620144 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:22:14.620599 [error] [MainThread]:   syntax error at or near "select"
12:22:14.621038 [error] [MainThread]:   LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
12:22:14.621488 [error] [MainThread]:                ^
12:22:14.622056 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:22:14.622524 [info ] [MainThread]: 
12:22:14.623076 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:22:14.623832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e29e5dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e29e55e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e6199460>]}


============================== 2022-03-06 12:23:18.665314 | 3cfdcea3-da2f-4dd4-8815-162d6ce160be ==============================
12:23:18.665314 [info ] [MainThread]: Running with dbt=1.0.3
12:23:18.666900 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:23:18.667342 [debug] [MainThread]: Tracking: tracking
12:23:18.676439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46c9944d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46c99446d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46c9944160>]}
12:23:18.715926 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:23:18.716826 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:23:18.743676 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:23:18.779113 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:23:18.787300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3cfdcea3-da2f-4dd4-8815-162d6ce160be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46c8ff50d0>]}
12:23:18.796214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3cfdcea3-da2f-4dd4-8815-162d6ce160be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46c9945e80>]}
12:23:18.796764 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:23:18.799072 [info ] [MainThread]: 
12:23:18.799911 [debug] [MainThread]: Acquiring new postgres connection "master"
12:23:18.801298 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:23:18.816906 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:23:18.817349 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:23:18.817709 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:23:18.834685 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:23:18.837072 [debug] [ThreadPool]: On list_adludio: Close
12:23:18.838891 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:23:18.848272 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:23:18.848597 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:23:18.848876 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:23:18.859508 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:23:18.859883 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:23:18.860167 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:23:18.863460 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:23:18.865811 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:23:18.866255 [debug] [ThreadPool]: On list_adludio_public: Close
12:23:18.873830 [debug] [MainThread]: Using postgres connection "master"
12:23:18.874120 [debug] [MainThread]: On master: BEGIN
12:23:18.874321 [debug] [MainThread]: Opening a new connection, currently in state init
12:23:18.886471 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:23:18.886837 [debug] [MainThread]: Using postgres connection "master"
12:23:18.887118 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:23:18.926551 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
12:23:18.929204 [debug] [MainThread]: On master: ROLLBACK
12:23:18.929771 [debug] [MainThread]: Using postgres connection "master"
12:23:18.930101 [debug] [MainThread]: On master: BEGIN
12:23:18.930611 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:23:18.930987 [debug] [MainThread]: On master: COMMIT
12:23:18.931276 [debug] [MainThread]: Using postgres connection "master"
12:23:18.931546 [debug] [MainThread]: On master: COMMIT
12:23:18.931919 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:23:18.932264 [debug] [MainThread]: On master: Close
12:23:18.932908 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:23:18.933946 [info ] [MainThread]: 
12:23:18.942911 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:23:18.943397 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:23:18.944159 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:18.944418 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:23:18.944680 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:23:18.949983 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:23:18.950591 [debug] [Thread-1  ]: finished collecting timing info
12:23:18.950879 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:23:19.061980 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.062693 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.062973 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:23:19.063200 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:23:19.076505 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:23:19.076855 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.077088 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:23:19.087747 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:23:19.099433 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.099755 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:23:19.100727 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:23:19.104279 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.104593 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:23:19.105182 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:23:19.123130 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:23:19.123537 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.123879 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:23:19.134675 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
12:23:19.146322 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.146734 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:23:19.149614 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:23:19.151488 [debug] [Thread-1  ]: finished collecting timing info
12:23:19.151796 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:23:19.153937 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3cfdcea3-da2f-4dd4-8815-162d6ce160be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46c8f72df0>]}
12:23:19.154571 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.21s]
12:23:19.155159 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:23:19.156096 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:23:19.156498 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:23:19.157414 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:23:19.157718 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:23:19.157969 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:23:19.162679 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:23:19.163204 [debug] [Thread-1  ]: finished collecting timing info
12:23:19.163492 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:23:19.173385 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:23:19.174027 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:23:19.174284 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:23:19.174556 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:23:19.189486 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:23:19.189907 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:23:19.190203 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as
    select * from "adludio"."public"."transformed_sales_number_data"
)

select *
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:23:19.190730 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "select"
LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
             ^

12:23:19.191050 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:23:19.191706 [debug] [Thread-1  ]: finished collecting timing info
12:23:19.192041 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:23:19.192714 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "select"
  LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
               ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:23:19.193200 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3cfdcea3-da2f-4dd4-8815-162d6ce160be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46cc7603d0>]}
12:23:19.193817 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.04s]
12:23:19.194517 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:23:19.204187 [debug] [MainThread]: Acquiring new postgres connection "master"
12:23:19.204565 [debug] [MainThread]: Using postgres connection "master"
12:23:19.204821 [debug] [MainThread]: On master: BEGIN
12:23:19.205067 [debug] [MainThread]: Opening a new connection, currently in state closed
12:23:19.221385 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
12:23:19.221871 [debug] [MainThread]: On master: COMMIT
12:23:19.222212 [debug] [MainThread]: Using postgres connection "master"
12:23:19.222496 [debug] [MainThread]: On master: COMMIT
12:23:19.223075 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:23:19.223391 [debug] [MainThread]: On master: Close
12:23:19.224086 [info ] [MainThread]: 
12:23:19.224628 [info ] [MainThread]: Finished running 2 table models in 0.42s.
12:23:19.225099 [debug] [MainThread]: Connection 'master' was properly closed.
12:23:19.225398 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:23:19.226229 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:23:19.238180 [info ] [MainThread]: 
12:23:19.239212 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:23:19.240439 [info ] [MainThread]: 
12:23:19.240953 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:23:19.241426 [error] [MainThread]:   syntax error at or near "select"
12:23:19.241983 [error] [MainThread]:   LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
12:23:19.242406 [error] [MainThread]:                ^
12:23:19.242895 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:23:19.243361 [info ] [MainThread]: 
12:23:19.243872 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:23:19.244479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46b35c7160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46b35c70a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46cc7583a0>]}


============================== 2022-03-06 12:24:20.886314 | ef68da2d-fd60-4578-a06c-7505ec664095 ==============================
12:24:20.886314 [info ] [MainThread]: Running with dbt=1.0.3
12:24:20.887624 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:24:20.888055 [debug] [MainThread]: Tracking: tracking
12:24:20.900906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc9e01d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc9e016d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc9e01160>]}
12:24:20.955159 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:24:20.956016 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:24:20.979377 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:24:21.015101 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:24:21.024844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ef68da2d-fd60-4578-a06c-7505ec664095', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc94b10d0>]}
12:24:21.036686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ef68da2d-fd60-4578-a06c-7505ec664095', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc9df3e80>]}
12:24:21.037271 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:24:21.039947 [info ] [MainThread]: 
12:24:21.040839 [debug] [MainThread]: Acquiring new postgres connection "master"
12:24:21.042368 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:24:21.061527 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:24:21.061982 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:24:21.062296 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:24:21.078378 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:24:21.080864 [debug] [ThreadPool]: On list_adludio: Close
12:24:21.082764 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:24:21.092422 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:24:21.092804 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:24:21.093080 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:24:21.104759 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:24:21.105292 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:24:21.105668 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:24:21.109095 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:24:21.112007 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:24:21.112344 [debug] [ThreadPool]: On list_adludio_public: Close
12:24:21.125631 [debug] [MainThread]: Using postgres connection "master"
12:24:21.126055 [debug] [MainThread]: On master: BEGIN
12:24:21.126355 [debug] [MainThread]: Opening a new connection, currently in state init
12:24:21.140118 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:24:21.140550 [debug] [MainThread]: Using postgres connection "master"
12:24:21.140844 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:24:21.176856 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
12:24:21.179463 [debug] [MainThread]: On master: ROLLBACK
12:24:21.179999 [debug] [MainThread]: Using postgres connection "master"
12:24:21.180326 [debug] [MainThread]: On master: BEGIN
12:24:21.180840 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:24:21.181152 [debug] [MainThread]: On master: COMMIT
12:24:21.181435 [debug] [MainThread]: Using postgres connection "master"
12:24:21.181765 [debug] [MainThread]: On master: COMMIT
12:24:21.182151 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:24:21.182481 [debug] [MainThread]: On master: Close
12:24:21.183121 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:24:21.184005 [info ] [MainThread]: 
12:24:21.198608 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:24:21.199218 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:24:21.200032 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.200300 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:24:21.200573 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:24:21.212587 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.213310 [debug] [Thread-1  ]: finished collecting timing info
12:24:21.219462 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:24:21.340902 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.341681 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.341969 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:24:21.342201 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:24:21.352707 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:24:21.353106 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.353501 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:24:21.363438 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:24:21.373876 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.374314 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:24:21.375042 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:24:21.381940 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.382269 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:24:21.382917 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:24:21.400061 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:24:21.400439 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.400679 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:24:21.403560 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:24:21.415793 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.416111 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:24:21.424137 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:24:21.426558 [debug] [Thread-1  ]: finished collecting timing info
12:24:21.427007 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:24:21.427769 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ef68da2d-fd60-4578-a06c-7505ec664095', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc942a220>]}
12:24:21.428390 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.23s]
12:24:21.429243 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:24:21.430114 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:24:21.430477 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:24:21.431092 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.431303 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:24:21.431505 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:24:21.435861 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:24:21.439736 [debug] [Thread-1  ]: finished collecting timing info
12:24:21.440046 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:24:21.444549 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:24:21.445131 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.445377 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:24:21.445709 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:24:21.456421 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:24:21.456862 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.457173 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:24:21.460751 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
12:24:21.464922 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.465258 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
12:24:21.465981 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:24:21.470083 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.470356 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
12:24:21.470929 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:24:21.473437 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:24:21.473752 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.473962 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:24:21.475046 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:24:21.477819 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.478100 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
12:24:21.480067 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:24:21.482039 [debug] [Thread-1  ]: finished collecting timing info
12:24:21.482379 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:24:21.483299 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ef68da2d-fd60-4578-a06c-7505ec664095', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdcc0f89d0>]}
12:24:21.483860 [info ] [Thread-1  ]: 2 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.05s]
12:24:21.484616 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:24:21.486061 [debug] [MainThread]: Acquiring new postgres connection "master"
12:24:21.486356 [debug] [MainThread]: Using postgres connection "master"
12:24:21.486584 [debug] [MainThread]: On master: BEGIN
12:24:21.486831 [debug] [MainThread]: Opening a new connection, currently in state closed
12:24:21.497298 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:24:21.497782 [debug] [MainThread]: On master: COMMIT
12:24:21.498191 [debug] [MainThread]: Using postgres connection "master"
12:24:21.498464 [debug] [MainThread]: On master: COMMIT
12:24:21.498861 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:24:21.499199 [debug] [MainThread]: On master: Close
12:24:21.499940 [info ] [MainThread]: 
12:24:21.500708 [info ] [MainThread]: Finished running 2 table models in 0.46s.
12:24:21.501342 [debug] [MainThread]: Connection 'master' was properly closed.
12:24:21.501687 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:24:21.502041 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:24:21.510902 [info ] [MainThread]: 
12:24:21.511495 [info ] [MainThread]: [32mCompleted successfully[0m
12:24:21.512052 [info ] [MainThread]: 
12:24:21.515612 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
12:24:21.516208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc9d429d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc944a430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc8488ee0>]}


============================== 2022-03-06 12:25:28.695073 | f9ad3b01-6dcd-4163-8bcf-2d783f988d4c ==============================
12:25:28.695073 [info ] [MainThread]: Running with dbt=1.0.3
12:25:28.696285 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:25:28.696677 [debug] [MainThread]: Tracking: tracking
12:25:28.701989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442ec153a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442ec15160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442ec15520>]}
12:25:28.749332 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:25:28.750213 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:25:28.780494 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:25:28.810827 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:25:28.820039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f9ad3b01-6dcd-4163-8bcf-2d783f988d4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442c86c0d0>]}
12:25:28.833151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f9ad3b01-6dcd-4163-8bcf-2d783f988d4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442ec08460>]}
12:25:28.833798 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:25:28.836828 [info ] [MainThread]: 
12:25:28.838040 [debug] [MainThread]: Acquiring new postgres connection "master"
12:25:28.841097 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:25:28.862820 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:25:28.863221 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:25:28.863527 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:25:28.881844 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:25:28.884348 [debug] [ThreadPool]: On list_adludio: Close
12:25:28.888249 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:25:28.899091 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:25:28.899548 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:25:28.899859 [debug] [ThreadPool]: Opening a new connection, currently in state closed
12:25:28.913762 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:25:28.914147 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:25:28.914404 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:25:28.922515 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.01 seconds
12:25:28.924832 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:25:28.925293 [debug] [ThreadPool]: On list_adludio_public: Close
12:25:28.931949 [debug] [MainThread]: Using postgres connection "master"
12:25:28.932284 [debug] [MainThread]: On master: BEGIN
12:25:28.932522 [debug] [MainThread]: Opening a new connection, currently in state init
12:25:28.944954 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:25:28.945331 [debug] [MainThread]: Using postgres connection "master"
12:25:28.945653 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:25:28.994634 [debug] [MainThread]: SQL status: SELECT 4 in 0.05 seconds
12:25:28.997293 [debug] [MainThread]: On master: ROLLBACK
12:25:28.997925 [debug] [MainThread]: Using postgres connection "master"
12:25:28.998264 [debug] [MainThread]: On master: BEGIN
12:25:28.998746 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:25:28.999065 [debug] [MainThread]: On master: COMMIT
12:25:28.999370 [debug] [MainThread]: Using postgres connection "master"
12:25:28.999649 [debug] [MainThread]: On master: COMMIT
12:25:29.000016 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:25:29.000310 [debug] [MainThread]: On master: Close
12:25:29.000951 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:25:29.002084 [info ] [MainThread]: 
12:25:29.007329 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:25:29.007845 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:25:29.008678 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.008943 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:25:29.009205 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:25:29.023045 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.023951 [debug] [Thread-1  ]: finished collecting timing info
12:25:29.024314 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:25:29.145998 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.146864 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.147171 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:25:29.147422 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:25:29.161338 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:25:29.161861 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.162147 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:25:29.172173 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:25:29.182779 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.183299 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:25:29.184103 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:25:29.187784 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.188103 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:25:29.191805 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:25:29.212405 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:25:29.212895 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.213182 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:25:29.215900 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:25:29.223727 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.224196 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:25:29.229513 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:25:29.231959 [debug] [Thread-1  ]: finished collecting timing info
12:25:29.232388 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:25:29.235388 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9ad3b01-6dcd-4163-8bcf-2d783f988d4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442d0d6a00>]}
12:25:29.236095 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.23s]
12:25:29.240257 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:25:29.241803 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:25:29.245972 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:25:29.258532 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.258979 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:25:29.259444 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:25:29.264921 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:25:29.265717 [debug] [Thread-1  ]: finished collecting timing info
12:25:29.269158 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:25:29.273819 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:25:29.274608 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.274922 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:25:29.275198 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:25:29.298449 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
12:25:29.298938 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.299305 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:25:29.308279 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
12:25:29.311177 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.311452 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
12:25:29.313132 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:25:29.317010 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.317358 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
12:25:29.318063 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:25:29.320780 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:25:29.321107 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.321404 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:25:29.322845 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:25:29.325671 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.325986 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
12:25:29.331208 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:25:29.333237 [debug] [Thread-1  ]: finished collecting timing info
12:25:29.333643 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:25:29.334355 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9ad3b01-6dcd-4163-8bcf-2d783f988d4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442c806460>]}
12:25:29.334995 [info ] [Thread-1  ]: 2 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.08s]
12:25:29.335829 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:25:29.337646 [debug] [MainThread]: Acquiring new postgres connection "master"
12:25:29.337977 [debug] [MainThread]: Using postgres connection "master"
12:25:29.338201 [debug] [MainThread]: On master: BEGIN
12:25:29.338409 [debug] [MainThread]: Opening a new connection, currently in state closed
12:25:29.348814 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:25:29.349280 [debug] [MainThread]: On master: COMMIT
12:25:29.349653 [debug] [MainThread]: Using postgres connection "master"
12:25:29.349922 [debug] [MainThread]: On master: COMMIT
12:25:29.350304 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:25:29.350618 [debug] [MainThread]: On master: Close
12:25:29.351315 [info ] [MainThread]: 
12:25:29.352223 [info ] [MainThread]: Finished running 2 table models in 0.51s.
12:25:29.353380 [debug] [MainThread]: Connection 'master' was properly closed.
12:25:29.353709 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:25:29.361721 [info ] [MainThread]: 
12:25:29.362754 [info ] [MainThread]: [32mCompleted successfully[0m
12:25:29.367071 [info ] [MainThread]: 
12:25:29.368041 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
12:25:29.369434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442c24c220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442c8126d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442c23ef70>]}


============================== 2022-03-06 12:39:59.224101 | 05942e40-1c2b-4a64-91aa-03ea1ded67c6 ==============================
12:39:59.224101 [info ] [MainThread]: Running with dbt=1.0.3
12:39:59.225656 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:39:59.226160 [debug] [MainThread]: Tracking: tracking
12:39:59.231938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c3a7ab80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c3a7a1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c3a7a0d0>]}
12:39:59.276639 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
12:39:59.277359 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/email_per_week copy.sql
12:39:59.307429 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/email_per_week copy.sql
12:39:59.339342 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:39:59.349035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '05942e40-1c2b-4a64-91aa-03ea1ded67c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c16d30d0>]}
12:39:59.358163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '05942e40-1c2b-4a64-91aa-03ea1ded67c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c3a6ed30>]}
12:39:59.358709 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:39:59.361193 [info ] [MainThread]: 
12:39:59.362034 [debug] [MainThread]: Acquiring new postgres connection "master"
12:39:59.363707 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:39:59.382326 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:39:59.382725 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:39:59.383024 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:39:59.396196 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
12:39:59.398727 [debug] [ThreadPool]: On list_adludio: Close
12:39:59.401336 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:39:59.410506 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:39:59.410868 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:39:59.411170 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:39:59.421735 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:39:59.422075 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:39:59.422438 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:39:59.425732 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:39:59.427986 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:39:59.428378 [debug] [ThreadPool]: On list_adludio_public: Close
12:39:59.435143 [debug] [MainThread]: Using postgres connection "master"
12:39:59.435441 [debug] [MainThread]: On master: BEGIN
12:39:59.435785 [debug] [MainThread]: Opening a new connection, currently in state init
12:39:59.447331 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:39:59.447667 [debug] [MainThread]: Using postgres connection "master"
12:39:59.447917 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:39:59.489642 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
12:39:59.493825 [debug] [MainThread]: On master: ROLLBACK
12:39:59.494336 [debug] [MainThread]: Using postgres connection "master"
12:39:59.494677 [debug] [MainThread]: On master: BEGIN
12:39:59.495196 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:39:59.495521 [debug] [MainThread]: On master: COMMIT
12:39:59.495805 [debug] [MainThread]: Using postgres connection "master"
12:39:59.496067 [debug] [MainThread]: On master: COMMIT
12:39:59.496465 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:39:59.496760 [debug] [MainThread]: On master: Close
12:39:59.497405 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:39:59.498212 [info ] [MainThread]: 
12:39:59.509853 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:39:59.510337 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
12:39:59.511341 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.511614 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:39:59.511897 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:39:59.517492 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.518129 [debug] [Thread-1  ]: finished collecting timing info
12:39:59.518456 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:39:59.624385 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.625177 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.625496 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:39:59.625758 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:39:59.636690 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:39:59.637008 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.637290 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:39:59.647622 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:39:59.658047 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.658363 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:39:59.659022 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:39:59.662668 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.662997 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:39:59.663615 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:39:59.683004 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:39:59.683396 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.683667 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:39:59.686154 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:39:59.697338 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.697689 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:39:59.700361 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:39:59.702238 [debug] [Thread-1  ]: finished collecting timing info
12:39:59.702600 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:39:59.703381 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05942e40-1c2b-4a64-91aa-03ea1ded67c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c409e190>]}
12:39:59.703998 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.19s]
12:39:59.704829 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:39:59.705817 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:39:59.706377 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
12:39:59.707160 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.707424 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:39:59.707675 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:39:59.712025 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:39:59.712539 [debug] [Thread-1  ]: finished collecting timing info
12:39:59.712817 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:39:59.716951 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:39:59.717495 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.717761 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:39:59.717994 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:39:59.728842 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:39:59.729164 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.729411 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:39:59.736729 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
12:39:59.740724 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.741072 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
12:39:59.741684 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:39:59.745279 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.745591 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
12:39:59.746278 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:39:59.749048 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:39:59.749336 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.749644 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:39:59.750791 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:39:59.753402 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.753732 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
12:39:59.755664 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:39:59.757329 [debug] [Thread-1  ]: finished collecting timing info
12:39:59.757699 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:39:59.758397 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05942e40-1c2b-4a64-91aa-03ea1ded67c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c4e29d00>]}
12:39:59.759063 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.05s]
12:39:59.760148 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:39:59.760632 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week copy
12:39:59.761613 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week copy............................. [RUN]
12:39:59.762724 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week copy"
12:39:59.763041 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week copy
12:39:59.763741 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week copy
12:39:59.767884 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week copy"
12:39:59.768521 [debug] [Thread-1  ]: finished collecting timing info
12:39:59.768871 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week copy
12:39:59.773098 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week copy"
12:39:59.773744 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week copy"
12:39:59.774033 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: BEGIN
12:39:59.774298 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:39:59.785569 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:39:59.785899 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week copy"
12:39:59.786167 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week copy"} */


  create  table "adludio"."public"."email_per_week copy__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:39:59.789609 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
12:39:59.793712 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week copy"
12:39:59.794019 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week copy"} */
alter table "adludio"."public"."email_per_week copy__dbt_tmp" rename to "email_per_week copy"
12:39:59.794611 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:39:59.797184 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: COMMIT
12:39:59.797525 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week copy"
12:39:59.797886 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: COMMIT
12:39:59.799271 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:39:59.803903 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week copy"
12:39:59.804198 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week copy"} */
drop table if exists "adludio"."public"."email_per_week copy__dbt_backup" cascade
12:39:59.804716 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:39:59.806396 [debug] [Thread-1  ]: finished collecting timing info
12:39:59.806740 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: Close
12:39:59.810659 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05942e40-1c2b-4a64-91aa-03ea1ded67c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c06ce040>]}
12:39:59.811244 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week copy........................ [[32mSELECT 53[0m in 0.05s]
12:39:59.811822 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week copy
12:39:59.813552 [debug] [MainThread]: Acquiring new postgres connection "master"
12:39:59.813849 [debug] [MainThread]: Using postgres connection "master"
12:39:59.814105 [debug] [MainThread]: On master: BEGIN
12:39:59.814320 [debug] [MainThread]: Opening a new connection, currently in state closed
12:39:59.826804 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:39:59.827215 [debug] [MainThread]: On master: COMMIT
12:39:59.827476 [debug] [MainThread]: Using postgres connection "master"
12:39:59.827706 [debug] [MainThread]: On master: COMMIT
12:39:59.828042 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:39:59.828312 [debug] [MainThread]: On master: Close
12:39:59.829125 [info ] [MainThread]: 
12:39:59.829863 [info ] [MainThread]: Finished running 3 table models in 0.47s.
12:39:59.830579 [debug] [MainThread]: Connection 'master' was properly closed.
12:39:59.830848 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:39:59.831091 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week copy' was properly closed.
12:39:59.839130 [info ] [MainThread]: 
12:39:59.839685 [info ] [MainThread]: [32mCompleted successfully[0m
12:39:59.840228 [info ] [MainThread]: 
12:39:59.840625 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
12:39:59.841155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c1669730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c1669790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c065a250>]}


============================== 2022-03-06 12:40:16.210991 | 1de40758-4fc9-4ec9-b48a-6c8622a456c0 ==============================
12:40:16.210991 [info ] [MainThread]: Running with dbt=1.0.3
12:40:16.212795 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:40:16.213475 [debug] [MainThread]: Tracking: tracking
12:40:16.223554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b0b8d2c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b0b8d2820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b0b8d2af0>]}
12:40:16.274798 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
12:40:16.275571 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/email_per_week.sql
12:40:16.275950 [debug] [MainThread]: Partial parsing: deleted file: Analytics_dbt://models/Sales Numbers/email_per_week copy.sql
12:40:16.298986 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/email_per_week.sql
12:40:16.321923 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:40:16.331152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1de40758-4fc9-4ec9-b48a-6c8622a456c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b095270d0>]}
12:40:16.340593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1de40758-4fc9-4ec9-b48a-6c8622a456c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b09e46820>]}
12:40:16.341192 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:40:16.343531 [info ] [MainThread]: 
12:40:16.347407 [debug] [MainThread]: Acquiring new postgres connection "master"
12:40:16.348817 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:40:16.370974 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:40:16.371397 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:40:16.371729 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:40:16.385286 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
12:40:16.387967 [debug] [ThreadPool]: On list_adludio: Close
12:40:16.391700 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:40:16.401088 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:40:16.401520 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:40:16.401853 [debug] [ThreadPool]: Opening a new connection, currently in state closed
12:40:16.412882 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:40:16.413335 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:40:16.413712 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:40:16.417022 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.0 seconds
12:40:16.419305 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:40:16.419730 [debug] [ThreadPool]: On list_adludio_public: Close
12:40:16.426588 [debug] [MainThread]: Using postgres connection "master"
12:40:16.426960 [debug] [MainThread]: On master: BEGIN
12:40:16.427312 [debug] [MainThread]: Opening a new connection, currently in state init
12:40:16.438565 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:40:16.438997 [debug] [MainThread]: Using postgres connection "master"
12:40:16.439326 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:40:16.481969 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
12:40:16.484607 [debug] [MainThread]: On master: ROLLBACK
12:40:16.485070 [debug] [MainThread]: Using postgres connection "master"
12:40:16.485398 [debug] [MainThread]: On master: BEGIN
12:40:16.485941 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:40:16.486254 [debug] [MainThread]: On master: COMMIT
12:40:16.486542 [debug] [MainThread]: Using postgres connection "master"
12:40:16.486838 [debug] [MainThread]: On master: COMMIT
12:40:16.487215 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:40:16.487523 [debug] [MainThread]: On master: Close
12:40:16.488230 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:40:16.489005 [info ] [MainThread]: 
12:40:16.499557 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:40:16.500057 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
12:40:16.501065 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.501345 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:40:16.501645 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:40:16.559200 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.560006 [debug] [Thread-1  ]: finished collecting timing info
12:40:16.560385 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:40:16.611958 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.612811 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.613185 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:40:16.613793 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:40:16.630578 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
12:40:16.631062 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.631394 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:40:16.646436 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:40:16.656649 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.657086 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:40:16.657833 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:16.661484 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.661847 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:40:16.662555 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:16.687310 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:40:16.687684 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.687945 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:40:16.690274 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:40:16.701925 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.702265 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:40:16.705423 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:40:16.707292 [debug] [Thread-1  ]: finished collecting timing info
12:40:16.707625 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:40:16.708282 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1de40758-4fc9-4ec9-b48a-6c8622a456c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b09d6e130>]}
12:40:16.708859 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.21s]
12:40:16.709414 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:40:16.710415 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:40:16.710870 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
12:40:16.711610 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.711945 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:40:16.712245 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:40:16.716579 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:40:16.717226 [debug] [Thread-1  ]: finished collecting timing info
12:40:16.717564 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:40:16.721856 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:40:16.722556 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.722899 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:40:16.723186 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:40:16.742195 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
12:40:16.742689 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.743008 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:40:16.746402 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
12:40:16.750410 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.750733 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
12:40:16.751329 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:16.756481 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.756858 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
12:40:16.757513 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:16.760110 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:40:16.760416 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.760687 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:40:16.761991 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:40:16.764648 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.764947 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
12:40:16.767738 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:40:16.769501 [debug] [Thread-1  ]: finished collecting timing info
12:40:16.769848 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:40:16.770567 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1de40758-4fc9-4ec9-b48a-6c8622a456c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b0949ca60>]}
12:40:16.771215 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.06s]
12:40:16.772028 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:40:16.772471 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
12:40:16.773014 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week.................................. [RUN]
12:40:16.773957 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
12:40:16.774271 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
12:40:16.774567 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
12:40:16.779235 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
12:40:16.779841 [debug] [Thread-1  ]: finished collecting timing info
12:40:16.780189 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
12:40:16.784550 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
12:40:16.785210 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:16.785533 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
12:40:16.789172 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:40:16.801196 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:40:16.801618 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:16.801924 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:40:16.805903 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
12:40:16.811960 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:16.812294 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
12:40:16.814375 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:16.816943 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
12:40:16.817237 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:16.817495 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
12:40:16.825021 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
12:40:16.827896 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:16.828439 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
12:40:16.828985 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:40:16.830901 [debug] [Thread-1  ]: finished collecting timing info
12:40:16.831236 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
12:40:16.831884 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1de40758-4fc9-4ec9-b48a-6c8622a456c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b084b2fd0>]}
12:40:16.832439 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week............................. [[32mSELECT 53[0m in 0.06s]
12:40:16.833476 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
12:40:16.835330 [debug] [MainThread]: Acquiring new postgres connection "master"
12:40:16.835620 [debug] [MainThread]: Using postgres connection "master"
12:40:16.835839 [debug] [MainThread]: On master: BEGIN
12:40:16.836088 [debug] [MainThread]: Opening a new connection, currently in state closed
12:40:16.855100 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
12:40:16.855570 [debug] [MainThread]: On master: COMMIT
12:40:16.855856 [debug] [MainThread]: Using postgres connection "master"
12:40:16.856106 [debug] [MainThread]: On master: COMMIT
12:40:16.856460 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:40:16.856751 [debug] [MainThread]: On master: Close
12:40:16.861053 [info ] [MainThread]: 
12:40:16.861594 [info ] [MainThread]: Finished running 3 table models in 0.51s.
12:40:16.862338 [debug] [MainThread]: Connection 'master' was properly closed.
12:40:16.863289 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week' was properly closed.
12:40:16.870887 [info ] [MainThread]: 
12:40:16.871380 [info ] [MainThread]: [32mCompleted successfully[0m
12:40:16.871915 [info ] [MainThread]: 
12:40:16.872377 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
12:40:16.872947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b084b9130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b084b9100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b084fa9d0>]}


============================== 2022-03-06 12:40:54.199328 | 0981a7e3-053a-4869-a61a-4b25b441aef8 ==============================
12:40:54.199328 [info ] [MainThread]: Running with dbt=1.0.3
12:40:54.200797 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:40:54.201257 [debug] [MainThread]: Tracking: tracking
12:40:54.208007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5abb8d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5abb86d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5abb8160>]}
12:40:54.264627 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:40:54.266201 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/email_per_week.sql
12:40:54.300560 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/email_per_week.sql
12:40:54.343895 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:40:54.352399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0981a7e3-053a-4869-a61a-4b25b441aef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5a26eaf0>]}
12:40:54.363824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0981a7e3-053a-4869-a61a-4b25b441aef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5ab0b970>]}
12:40:54.364543 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:40:54.367882 [info ] [MainThread]: 
12:40:54.369271 [debug] [MainThread]: Acquiring new postgres connection "master"
12:40:54.374483 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:40:54.403541 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:40:54.404021 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:40:54.404318 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:40:54.418618 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
12:40:54.421143 [debug] [ThreadPool]: On list_adludio: Close
12:40:54.427293 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:40:54.437615 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:40:54.438095 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:40:54.438411 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:40:54.452848 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:40:54.453291 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:40:54.453612 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:40:54.459917 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.01 seconds
12:40:54.462471 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:40:54.462992 [debug] [ThreadPool]: On list_adludio_public: Close
12:40:54.475547 [debug] [MainThread]: Using postgres connection "master"
12:40:54.476008 [debug] [MainThread]: On master: BEGIN
12:40:54.476309 [debug] [MainThread]: Opening a new connection, currently in state init
12:40:54.491041 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:40:54.491522 [debug] [MainThread]: Using postgres connection "master"
12:40:54.491867 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:40:54.576082 [debug] [MainThread]: SQL status: SELECT 4 in 0.08 seconds
12:40:54.578753 [debug] [MainThread]: On master: ROLLBACK
12:40:54.579387 [debug] [MainThread]: Using postgres connection "master"
12:40:54.579824 [debug] [MainThread]: On master: BEGIN
12:40:54.580369 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:40:54.580752 [debug] [MainThread]: On master: COMMIT
12:40:54.581049 [debug] [MainThread]: Using postgres connection "master"
12:40:54.581383 [debug] [MainThread]: On master: COMMIT
12:40:54.581825 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:40:54.582141 [debug] [MainThread]: On master: Close
12:40:54.582848 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:40:54.583665 [info ] [MainThread]: 
12:40:54.618331 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:40:54.618924 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
12:40:54.619741 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.620018 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:40:54.620264 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:40:54.727846 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.728547 [debug] [Thread-1  ]: finished collecting timing info
12:40:54.728827 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:40:54.776249 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.780302 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.781199 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:40:54.781420 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:40:54.792942 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:40:54.793409 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.793779 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:40:54.814208 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.02 seconds
12:40:54.833005 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.833342 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:40:54.840823 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:54.844213 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.848132 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:40:54.850464 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:54.878049 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:40:54.881437 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.881845 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:40:54.884322 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:40:54.895893 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.896419 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:40:54.903484 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
12:40:54.914833 [debug] [Thread-1  ]: finished collecting timing info
12:40:54.915353 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:40:54.916177 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0981a7e3-053a-4869-a61a-4b25b441aef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5c5f5c40>]}
12:40:54.916838 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.30s]
12:40:54.918155 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:40:54.919555 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:40:54.921993 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
12:40:54.922936 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.923200 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:40:54.923426 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:40:54.927511 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:40:54.928841 [debug] [Thread-1  ]: finished collecting timing info
12:40:54.929178 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:40:54.933773 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:40:54.934442 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.934732 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:40:54.934994 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:40:54.953925 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
12:40:54.954326 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.954691 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:40:54.958684 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
12:40:54.966976 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.967350 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
12:40:54.968000 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:54.971810 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.972118 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
12:40:54.972697 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:54.975217 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:40:54.975521 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.975778 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:40:54.978448 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:40:54.981331 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.981688 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
12:40:54.983649 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:40:54.985521 [debug] [Thread-1  ]: finished collecting timing info
12:40:54.985867 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:40:54.986589 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0981a7e3-053a-4869-a61a-4b25b441aef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5cc59790>]}
12:40:54.987155 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.06s]
12:40:54.987701 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:40:54.988407 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
12:40:54.988930 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week.................................. [RUN]
12:40:54.989731 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
12:40:54.990028 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
12:40:54.990325 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
12:40:54.994589 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
12:40:54.995202 [debug] [Thread-1  ]: finished collecting timing info
12:40:54.995498 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
12:40:54.999829 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
12:40:55.000525 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:55.000809 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
12:40:55.001070 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:40:55.017179 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
12:40:55.017655 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:55.017980 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_email_messages_count) as avg_email_count, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:40:55.024842 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
12:40:55.031293 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:55.031651 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week" rename to "email_per_week__dbt_backup"
12:40:55.032303 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:55.035814 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:55.036184 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
12:40:55.036824 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:55.039400 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
12:40:55.039718 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:55.039975 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
12:40:55.041294 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:40:55.044023 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:55.044307 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
12:40:55.046425 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:40:55.048250 [debug] [Thread-1  ]: finished collecting timing info
12:40:55.048597 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
12:40:55.051322 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0981a7e3-053a-4869-a61a-4b25b441aef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e587f3fd0>]}
12:40:55.051933 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week............................. [[32mSELECT 53[0m in 0.06s]
12:40:55.054106 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
12:40:55.055948 [debug] [MainThread]: Acquiring new postgres connection "master"
12:40:55.056197 [debug] [MainThread]: Using postgres connection "master"
12:40:55.056348 [debug] [MainThread]: On master: BEGIN
12:40:55.056483 [debug] [MainThread]: Opening a new connection, currently in state closed
12:40:55.066848 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:40:55.067333 [debug] [MainThread]: On master: COMMIT
12:40:55.067652 [debug] [MainThread]: Using postgres connection "master"
12:40:55.067944 [debug] [MainThread]: On master: COMMIT
12:40:55.068327 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:40:55.068625 [debug] [MainThread]: On master: Close
12:40:55.071139 [info ] [MainThread]: 
12:40:55.072113 [info ] [MainThread]: Finished running 3 table models in 0.70s.
12:40:55.073386 [debug] [MainThread]: Connection 'master' was properly closed.
12:40:55.073692 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:40:55.090060 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week' was properly closed.
12:40:55.099058 [info ] [MainThread]: 
12:40:55.099672 [info ] [MainThread]: [32mCompleted successfully[0m
12:40:55.100222 [info ] [MainThread]: 
12:40:55.100686 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
12:40:55.101304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5883bcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5883bd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5886a5e0>]}


============================== 2022-03-06 13:22:32.159716 | fbe89c67-7760-4a1a-927e-3e66c01ed02a ==============================
13:22:32.159716 [info ] [MainThread]: Running with dbt=1.0.3
13:22:32.160744 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
13:22:32.161152 [debug] [MainThread]: Tracking: tracking
13:22:32.166787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32dab49190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32dab49370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32dab497c0>]}
13:22:32.246399 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
13:22:32.246813 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
13:22:32.247573 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

13:22:32.258389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fbe89c67-7760-4a1a-927e-3e66c01ed02a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32daa470d0>]}
13:22:32.267759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fbe89c67-7760-4a1a-927e-3e66c01ed02a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32dab18880>]}
13:22:32.268290 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
13:22:32.272659 [info ] [MainThread]: 
13:22:32.273612 [debug] [MainThread]: Acquiring new postgres connection "master"
13:22:32.275341 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
13:22:32.290970 [debug] [ThreadPool]: Using postgres connection "list_adludio"
13:22:32.291356 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
13:22:32.291618 [debug] [ThreadPool]: Opening a new connection, currently in state init
13:22:32.314790 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
13:22:32.317367 [debug] [ThreadPool]: On list_adludio: Close
13:22:32.319292 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
13:22:32.328645 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
13:22:32.329038 [debug] [ThreadPool]: On list_adludio_public: BEGIN
13:22:32.329324 [debug] [ThreadPool]: Opening a new connection, currently in state closed
13:22:32.340033 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
13:22:32.340456 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
13:22:32.340802 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
13:22:32.344104 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.0 seconds
13:22:32.346460 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
13:22:32.346898 [debug] [ThreadPool]: On list_adludio_public: Close
13:22:32.353966 [debug] [MainThread]: Using postgres connection "master"
13:22:32.354402 [debug] [MainThread]: On master: BEGIN
13:22:32.354700 [debug] [MainThread]: Opening a new connection, currently in state init
13:22:32.368449 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
13:22:32.368932 [debug] [MainThread]: Using postgres connection "master"
13:22:32.369239 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
13:22:32.441493 [debug] [MainThread]: SQL status: SELECT 5 in 0.07 seconds
13:22:32.444096 [debug] [MainThread]: On master: ROLLBACK
13:22:32.444716 [debug] [MainThread]: Using postgres connection "master"
13:22:32.445037 [debug] [MainThread]: On master: BEGIN
13:22:32.445603 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
13:22:32.445909 [debug] [MainThread]: On master: COMMIT
13:22:32.446182 [debug] [MainThread]: Using postgres connection "master"
13:22:32.446439 [debug] [MainThread]: On master: COMMIT
13:22:32.446801 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
13:22:32.447082 [debug] [MainThread]: On master: Close
13:22:32.447708 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
13:22:32.448418 [info ] [MainThread]: 
13:22:32.454958 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
13:22:32.455509 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
13:22:32.462436 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.462773 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
13:22:32.463120 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
13:22:32.468810 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.469574 [debug] [Thread-1  ]: finished collecting timing info
13:22:32.469908 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
13:22:32.539614 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.540550 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.540893 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
13:22:32.541165 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:22:32.563251 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
13:22:32.566964 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.567362 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
13:22:32.581922 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
13:22:32.599749 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.605684 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
13:22:32.609551 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
13:22:32.613314 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.613629 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
13:22:32.614745 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
13:22:32.661299 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
13:22:32.661852 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.662219 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
13:22:32.672020 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
13:22:32.680430 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.680756 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
13:22:32.684751 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
13:22:32.686137 [debug] [Thread-1  ]: finished collecting timing info
13:22:32.686401 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
13:22:32.693045 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fbe89c67-7760-4a1a-927e-3e66c01ed02a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32d9a37fa0>]}
13:22:32.693584 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.22s]
13:22:32.693994 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
13:22:32.697587 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
13:22:32.697979 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
13:22:32.698645 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.699109 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
13:22:32.699337 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
13:22:32.813154 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
13:22:32.814054 [debug] [Thread-1  ]: finished collecting timing info
13:22:32.814423 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
13:22:32.818677 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
13:22:32.819442 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.819762 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
13:22:32.820039 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:22:32.837709 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
13:22:32.838289 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.838619 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
13:22:32.841050 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
13:22:32.855711 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.856260 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
13:22:32.856905 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
13:22:32.864224 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.864777 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
13:22:32.865421 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
13:22:32.868486 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
13:22:32.868940 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.869252 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
13:22:32.870550 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
13:22:32.873184 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.873547 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
13:22:32.875767 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
13:22:32.877682 [debug] [Thread-1  ]: finished collecting timing info
13:22:32.881278 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
13:22:32.887571 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fbe89c67-7760-4a1a-927e-3e66c01ed02a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32d85fe760>]}
13:22:32.888343 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.19s]
13:22:32.889499 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
13:22:32.889884 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
13:22:32.890421 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week.................................. [RUN]
13:22:32.892006 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.892310 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
13:22:32.892600 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
13:22:32.897669 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
13:22:32.898370 [debug] [Thread-1  ]: finished collecting timing info
13:22:32.898707 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
13:22:32.906401 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
13:22:32.907336 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.908714 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
13:22:32.909061 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:22:32.925130 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
13:22:32.925647 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.929189 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_email_messages_count) as avg_email_count, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
13:22:32.939482 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
13:22:32.944179 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.944605 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week" rename to "email_per_week__dbt_backup"
13:22:32.947874 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
13:22:32.951795 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.952227 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
13:22:32.957100 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
13:22:32.962707 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
13:22:32.963031 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.963307 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
13:22:32.974400 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
13:22:32.980696 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.981029 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
13:22:32.983484 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
13:22:32.985614 [debug] [Thread-1  ]: finished collecting timing info
13:22:32.985945 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
13:22:32.988662 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fbe89c67-7760-4a1a-927e-3e66c01ed02a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32d9a27790>]}
13:22:32.989259 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week............................. [[32mSELECT 53[0m in 0.10s]
13:22:32.989810 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
13:22:32.991271 [debug] [MainThread]: Acquiring new postgres connection "master"
13:22:32.991594 [debug] [MainThread]: Using postgres connection "master"
13:22:32.991816 [debug] [MainThread]: On master: BEGIN
13:22:32.992030 [debug] [MainThread]: Opening a new connection, currently in state closed
13:22:33.002615 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
13:22:33.003068 [debug] [MainThread]: On master: COMMIT
13:22:33.003364 [debug] [MainThread]: Using postgres connection "master"
13:22:33.003627 [debug] [MainThread]: On master: COMMIT
13:22:33.003996 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
13:22:33.004293 [debug] [MainThread]: On master: Close
13:22:33.004967 [info ] [MainThread]: 
13:22:33.006004 [info ] [MainThread]: Finished running 3 table models in 0.73s.
13:22:33.006682 [debug] [MainThread]: Connection 'master' was properly closed.
13:22:33.006951 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week' was properly closed.
13:22:33.015336 [info ] [MainThread]: 
13:22:33.015934 [info ] [MainThread]: [32mCompleted successfully[0m
13:22:33.019846 [info ] [MainThread]: 
13:22:33.020430 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
13:22:33.024180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32d85dddf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32d9a142b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32d9a02f70>]}


============================== 2022-03-06 18:13:38.509190 | dae62981-2d0e-4552-b3d0-20ba7b253f63 ==============================
18:13:38.509190 [info ] [MainThread]: Running with dbt=1.0.3
18:13:38.510590 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
18:13:38.511004 [debug] [MainThread]: Tracking: tracking
18:13:38.516887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac24d10130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac24d102e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac24d10a00>]}
18:13:38.561211 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
18:13:38.562044 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
18:13:38.581946 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
18:13:38.607155 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

18:13:38.618533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dae62981-2d0e-4552-b3d0-20ba7b253f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac229690d0>]}
18:13:38.627569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dae62981-2d0e-4552-b3d0-20ba7b253f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac23207a30>]}
18:13:38.628096 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
18:13:38.630280 [info ] [MainThread]: 
18:13:38.631114 [debug] [MainThread]: Acquiring new postgres connection "master"
18:13:38.632596 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
18:13:38.651155 [debug] [ThreadPool]: Using postgres connection "list_adludio"
18:13:38.651590 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
18:13:38.651892 [debug] [ThreadPool]: Opening a new connection, currently in state init
18:13:38.668464 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.02 seconds
18:13:38.673608 [debug] [ThreadPool]: On list_adludio: Close
18:13:38.678818 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
18:13:38.688413 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
18:13:38.688784 [debug] [ThreadPool]: On list_adludio_public: BEGIN
18:13:38.689102 [debug] [ThreadPool]: Opening a new connection, currently in state init
18:13:38.699691 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
18:13:38.700073 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
18:13:38.700351 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
18:13:38.703576 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.0 seconds
18:13:38.706072 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
18:13:38.706541 [debug] [ThreadPool]: On list_adludio_public: Close
18:13:38.716252 [debug] [MainThread]: Using postgres connection "master"
18:13:38.716649 [debug] [MainThread]: On master: BEGIN
18:13:38.716930 [debug] [MainThread]: Opening a new connection, currently in state init
18:13:38.730705 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
18:13:38.731059 [debug] [MainThread]: Using postgres connection "master"
18:13:38.731336 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
18:13:38.897257 [debug] [MainThread]: SQL status: SELECT 8 in 0.17 seconds
18:13:38.900030 [debug] [MainThread]: On master: ROLLBACK
18:13:38.900600 [debug] [MainThread]: Using postgres connection "master"
18:13:38.900909 [debug] [MainThread]: On master: BEGIN
18:13:38.901360 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
18:13:38.901682 [debug] [MainThread]: On master: COMMIT
18:13:38.901951 [debug] [MainThread]: Using postgres connection "master"
18:13:38.902226 [debug] [MainThread]: On master: COMMIT
18:13:38.902587 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
18:13:38.902868 [debug] [MainThread]: On master: Close
18:13:38.905591 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
18:13:38.906123 [info ] [MainThread]: 
18:13:38.911521 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
18:13:38.912001 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
18:13:38.912785 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:13:38.913049 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
18:13:38.913328 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
18:13:38.921881 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
18:13:38.922574 [debug] [Thread-1  ]: finished collecting timing info
18:13:38.922899 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
18:13:39.031879 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
18:13:39.032608 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:13:39.032979 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
18:13:39.033256 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:13:39.043454 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
18:13:39.043832 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:13:39.044107 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    -- DATE_PART('month', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarterly from sales_table


)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:13:39.057161 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
18:13:39.067374 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:13:39.067752 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
18:13:39.068468 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:13:39.071993 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:13:39.072311 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
18:13:39.073079 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:13:39.092370 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
18:13:39.092805 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:13:39.093106 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
18:13:39.097155 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
18:13:39.104762 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:13:39.105109 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
18:13:39.110543 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
18:13:39.112554 [debug] [Thread-1  ]: finished collecting timing info
18:13:39.112948 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
18:13:39.118936 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dae62981-2d0e-4552-b3d0-20ba7b253f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac228f0e50>]}
18:13:39.119569 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.21s]
18:13:39.120134 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
18:13:39.121066 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
18:13:39.121642 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
18:13:39.122364 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
18:13:39.122638 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
18:13:39.122886 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
18:13:39.127101 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
18:13:39.127629 [debug] [Thread-1  ]: finished collecting timing info
18:13:39.127887 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
18:13:39.132616 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
18:13:39.133119 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:13:39.133416 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
18:13:39.133709 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:13:39.154519 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
18:13:39.154970 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:13:39.155270 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:13:39.158753 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
18:13:39.163132 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:13:39.163463 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
18:13:39.164024 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:13:39.167343 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:13:39.167654 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
18:13:39.168190 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:13:39.171023 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
18:13:39.171318 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:13:39.171528 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
18:13:39.172956 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
18:13:39.175441 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:13:39.175716 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
18:13:39.177527 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
18:13:39.179298 [debug] [Thread-1  ]: finished collecting timing info
18:13:39.179602 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
18:13:39.180921 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dae62981-2d0e-4552-b3d0-20ba7b253f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac252ea880>]}
18:13:39.181686 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.06s]
18:13:39.182314 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
18:13:39.182644 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
18:13:39.183178 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week.................................. [RUN]
18:13:39.183906 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
18:13:39.184176 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
18:13:39.187536 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
18:13:39.192000 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
18:13:39.192614 [debug] [Thread-1  ]: finished collecting timing info
18:13:39.192889 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
18:13:39.197866 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
18:13:39.198425 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:13:39.198714 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
18:13:39.198927 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:13:39.219558 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
18:13:39.219973 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:13:39.220234 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_email_messages_count) as avg_email_count, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:13:39.230803 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
18:13:39.235673 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:13:39.235992 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week" rename to "email_per_week__dbt_backup"
18:13:39.236598 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:13:39.239997 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:13:39.242428 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
18:13:39.243095 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:13:39.245771 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
18:13:39.246042 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:13:39.246278 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
18:13:39.249830 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
18:13:39.252407 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:13:39.252698 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
18:13:39.260402 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
18:13:39.262642 [debug] [Thread-1  ]: finished collecting timing info
18:13:39.262987 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
18:13:39.263660 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dae62981-2d0e-4552-b3d0-20ba7b253f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac20f67fa0>]}
18:13:39.264253 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week............................. [[32mSELECT 53[0m in 0.08s]
18:13:39.265254 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
18:13:39.267042 [debug] [MainThread]: Acquiring new postgres connection "master"
18:13:39.267360 [debug] [MainThread]: Using postgres connection "master"
18:13:39.267590 [debug] [MainThread]: On master: BEGIN
18:13:39.267803 [debug] [MainThread]: Opening a new connection, currently in state closed
18:13:39.278284 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
18:13:39.278683 [debug] [MainThread]: On master: COMMIT
18:13:39.278949 [debug] [MainThread]: Using postgres connection "master"
18:13:39.279193 [debug] [MainThread]: On master: COMMIT
18:13:39.282927 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
18:13:39.283242 [debug] [MainThread]: On master: Close
18:13:39.283948 [info ] [MainThread]: 
18:13:39.284647 [info ] [MainThread]: Finished running 3 table models in 0.65s.
18:13:39.285081 [debug] [MainThread]: Connection 'master' was properly closed.
18:13:39.285474 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
18:13:39.285689 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week' was properly closed.
18:13:39.293480 [info ] [MainThread]: 
18:13:39.293996 [info ] [MainThread]: [32mCompleted successfully[0m
18:13:39.294511 [info ] [MainThread]: 
18:13:39.294971 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
18:13:39.295539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac2290bf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac2290bf40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac20f69eb0>]}


============================== 2022-03-06 18:16:20.161531 | 1c1e8b4d-a107-4545-9b3e-6a3288a2d72a ==============================
18:16:20.161531 [info ] [MainThread]: Running with dbt=1.0.3
18:16:20.165926 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
18:16:20.166428 [debug] [MainThread]: Tracking: tracking
18:16:20.172837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc77f3f9370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc77f3f9310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc77f3f9100>]}
18:16:20.238770 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
18:16:20.239590 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
18:16:20.261060 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
18:16:20.292706 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

18:16:20.301472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1c1e8b4d-a107-4545-9b3e-6a3288a2d72a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc77d04efa0>]}
18:16:20.312943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1c1e8b4d-a107-4545-9b3e-6a3288a2d72a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc77d8ed970>]}
18:16:20.313556 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
18:16:20.315989 [info ] [MainThread]: 
18:16:20.316830 [debug] [MainThread]: Acquiring new postgres connection "master"
18:16:20.318628 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
18:16:20.334388 [debug] [ThreadPool]: Using postgres connection "list_adludio"
18:16:20.334775 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
18:16:20.335052 [debug] [ThreadPool]: Opening a new connection, currently in state init
18:16:20.348201 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.01 seconds
18:16:20.350752 [debug] [ThreadPool]: On list_adludio: Close
18:16:20.357365 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
18:16:20.367065 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
18:16:20.367411 [debug] [ThreadPool]: On list_adludio_public: BEGIN
18:16:20.367684 [debug] [ThreadPool]: Opening a new connection, currently in state init
18:16:20.378293 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
18:16:20.378622 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
18:16:20.378850 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
18:16:20.382180 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.0 seconds
18:16:20.384807 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
18:16:20.385244 [debug] [ThreadPool]: On list_adludio_public: Close
18:16:20.397594 [debug] [MainThread]: Using postgres connection "master"
18:16:20.397973 [debug] [MainThread]: On master: BEGIN
18:16:20.398264 [debug] [MainThread]: Opening a new connection, currently in state init
18:16:20.408827 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
18:16:20.409208 [debug] [MainThread]: Using postgres connection "master"
18:16:20.409449 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
18:16:20.570047 [debug] [MainThread]: SQL status: SELECT 8 in 0.16 seconds
18:16:20.572771 [debug] [MainThread]: On master: ROLLBACK
18:16:20.573266 [debug] [MainThread]: Using postgres connection "master"
18:16:20.573577 [debug] [MainThread]: On master: BEGIN
18:16:20.574207 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
18:16:20.574472 [debug] [MainThread]: On master: COMMIT
18:16:20.574697 [debug] [MainThread]: Using postgres connection "master"
18:16:20.574910 [debug] [MainThread]: On master: COMMIT
18:16:20.575226 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
18:16:20.575478 [debug] [MainThread]: On master: Close
18:16:20.576225 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
18:16:20.577040 [info ] [MainThread]: 
18:16:20.585645 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
18:16:20.586198 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
18:16:20.587012 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:16:20.587278 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
18:16:20.587542 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
18:16:20.654054 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
18:16:20.654774 [debug] [Thread-1  ]: finished collecting timing info
18:16:20.655088 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
18:16:20.702801 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
18:16:20.703522 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:16:20.704063 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
18:16:20.704316 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:16:20.714782 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
18:16:20.715235 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:16:20.715600 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    -- DATE_PART('month', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarterly from sales_table


)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarterly
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:16:20.728194 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
18:16:20.738904 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:16:20.739266 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
18:16:20.740036 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:16:20.743776 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:16:20.747300 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
18:16:20.748030 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:16:20.765233 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
18:16:20.765624 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:16:20.765879 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
18:16:20.770179 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
18:16:20.778021 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:16:20.778434 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
18:16:20.782198 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
18:16:20.784465 [debug] [Thread-1  ]: finished collecting timing info
18:16:20.784817 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
18:16:20.785528 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c1e8b4d-a107-4545-9b3e-6a3288a2d72a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc77fa3d040>]}
18:16:20.786105 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.20s]
18:16:20.786880 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
18:16:20.788000 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
18:16:20.788567 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
18:16:20.789431 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
18:16:20.789940 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
18:16:20.790200 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
18:16:20.801095 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
18:16:20.801897 [debug] [Thread-1  ]: finished collecting timing info
18:16:20.802235 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
18:16:20.807307 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
18:16:20.807905 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:16:20.808170 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
18:16:20.808401 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:16:20.818982 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
18:16:20.819350 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:16:20.819594 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:16:20.823279 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
18:16:20.827335 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:16:20.827603 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
18:16:20.828141 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:16:20.837438 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:16:20.837854 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
18:16:20.838540 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:16:20.841329 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
18:16:20.841616 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:16:20.841870 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
18:16:20.843101 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
18:16:20.845667 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:16:20.845966 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
18:16:20.847790 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
18:16:20.849633 [debug] [Thread-1  ]: finished collecting timing info
18:16:20.849996 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
18:16:20.852786 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c1e8b4d-a107-4545-9b3e-6a3288a2d72a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc77d9ff280>]}
18:16:20.853355 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.06s]
18:16:20.854367 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
18:16:20.854690 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
18:16:20.855172 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week.................................. [RUN]
18:16:20.856421 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
18:16:20.856673 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
18:16:20.856910 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
18:16:20.861741 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
18:16:20.865295 [debug] [Thread-1  ]: finished collecting timing info
18:16:20.865660 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
18:16:20.870441 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
18:16:20.871026 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:16:20.871275 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
18:16:20.871493 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:16:20.885688 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
18:16:20.886134 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:16:20.886428 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_email_messages_count) as avg_email_count, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:16:20.892133 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
18:16:20.908190 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:16:20.908561 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week" rename to "email_per_week__dbt_backup"
18:16:20.909256 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:16:20.912848 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:16:20.913123 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
18:16:20.913768 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:16:20.916498 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
18:16:20.916791 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:16:20.917054 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
18:16:20.921011 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
18:16:20.923683 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:16:20.924004 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
18:16:20.926940 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
18:16:20.935882 [debug] [Thread-1  ]: finished collecting timing info
18:16:20.936397 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
18:16:20.937209 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c1e8b4d-a107-4545-9b3e-6a3288a2d72a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc77cfd3580>]}
18:16:20.937866 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week............................. [[32mSELECT 53[0m in 0.08s]
18:16:20.938698 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
18:16:20.940522 [debug] [MainThread]: Acquiring new postgres connection "master"
18:16:20.940843 [debug] [MainThread]: Using postgres connection "master"
18:16:20.941079 [debug] [MainThread]: On master: BEGIN
18:16:20.941292 [debug] [MainThread]: Opening a new connection, currently in state closed
18:16:20.957593 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
18:16:20.958023 [debug] [MainThread]: On master: COMMIT
18:16:20.958279 [debug] [MainThread]: Using postgres connection "master"
18:16:20.958501 [debug] [MainThread]: On master: COMMIT
18:16:20.958905 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
18:16:20.959246 [debug] [MainThread]: On master: Close
18:16:20.965968 [info ] [MainThread]: 
18:16:20.966781 [info ] [MainThread]: Finished running 3 table models in 0.65s.
18:16:20.967242 [debug] [MainThread]: Connection 'master' was properly closed.
18:16:20.967465 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
18:16:20.967676 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week' was properly closed.
18:16:20.975362 [info ] [MainThread]: 
18:16:20.975938 [info ] [MainThread]: [32mCompleted successfully[0m
18:16:20.976430 [info ] [MainThread]: 
18:16:20.976821 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
18:16:20.977417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc77cff3f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc77cff3f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc77cffd460>]}


============================== 2022-03-06 18:40:44.084301 | fb319193-6803-41e6-bfbe-321f5a69f7c6 ==============================
18:40:44.084301 [info ] [MainThread]: Running with dbt=1.0.3
18:40:44.089087 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
18:40:44.089595 [debug] [MainThread]: Tracking: tracking
18:40:44.096331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f607ad62b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f607ad62dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f607ad62b50>]}
18:40:44.157502 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
18:40:44.158302 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
18:40:44.179719 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
18:40:44.205332 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

18:40:44.215049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fb319193-6803-41e6-bfbe-321f5a69f7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60789b90d0>]}
18:40:44.233150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fb319193-6803-41e6-bfbe-321f5a69f7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60792d3850>]}
18:40:44.233776 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
18:40:44.237908 [info ] [MainThread]: 
18:40:44.238790 [debug] [MainThread]: Acquiring new postgres connection "master"
18:40:44.240637 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
18:40:44.256673 [debug] [ThreadPool]: Using postgres connection "list_adludio"
18:40:44.257184 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
18:40:44.257495 [debug] [ThreadPool]: Opening a new connection, currently in state init
18:40:44.271030 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.01 seconds
18:40:44.273631 [debug] [ThreadPool]: On list_adludio: Close
18:40:44.275457 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
18:40:44.287959 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
18:40:44.288409 [debug] [ThreadPool]: On list_adludio_public: BEGIN
18:40:44.288705 [debug] [ThreadPool]: Opening a new connection, currently in state closed
18:40:44.299449 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
18:40:44.299889 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
18:40:44.300179 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
18:40:44.303970 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.0 seconds
18:40:44.306572 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
18:40:44.307079 [debug] [ThreadPool]: On list_adludio_public: Close
18:40:44.316093 [debug] [MainThread]: Using postgres connection "master"
18:40:44.316453 [debug] [MainThread]: On master: BEGIN
18:40:44.316724 [debug] [MainThread]: Opening a new connection, currently in state init
18:40:44.327407 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
18:40:44.327777 [debug] [MainThread]: Using postgres connection "master"
18:40:44.328018 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
18:40:44.490437 [debug] [MainThread]: SQL status: SELECT 8 in 0.16 seconds
18:40:44.493087 [debug] [MainThread]: On master: ROLLBACK
18:40:44.493631 [debug] [MainThread]: Using postgres connection "master"
18:40:44.493930 [debug] [MainThread]: On master: BEGIN
18:40:44.494398 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
18:40:44.494670 [debug] [MainThread]: On master: COMMIT
18:40:44.494901 [debug] [MainThread]: Using postgres connection "master"
18:40:44.495109 [debug] [MainThread]: On master: COMMIT
18:40:44.495437 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
18:40:44.495695 [debug] [MainThread]: On master: Close
18:40:44.498366 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
18:40:44.498859 [info ] [MainThread]: 
18:40:44.505112 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
18:40:44.505690 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
18:40:44.506492 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:40:44.506770 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
18:40:44.507035 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
18:40:44.571361 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
18:40:44.572148 [debug] [Thread-1  ]: finished collecting timing info
18:40:44.575779 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
18:40:44.624006 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
18:40:44.624750 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:40:44.625033 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
18:40:44.625305 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:40:44.636066 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
18:40:44.636455 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:40:44.636708 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter from sales_table


)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter,year
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:40:44.649718 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
18:40:44.659914 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:40:44.660316 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
18:40:44.661047 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:40:44.664552 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:40:44.664839 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
18:40:44.665480 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:40:44.682255 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
18:40:44.682647 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:40:44.682899 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
18:40:44.688692 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
18:40:44.696814 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:40:44.697189 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
18:40:44.701213 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
18:40:44.703277 [debug] [Thread-1  ]: finished collecting timing info
18:40:44.706054 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
18:40:44.707803 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb319193-6803-41e6-bfbe-321f5a69f7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f607b39a5e0>]}
18:40:44.708487 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.20s]
18:40:44.709069 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
18:40:44.711626 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
18:40:44.712091 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
18:40:44.713103 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
18:40:44.713363 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
18:40:44.713680 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
18:40:44.722556 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
18:40:44.723328 [debug] [Thread-1  ]: finished collecting timing info
18:40:44.723645 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
18:40:44.728470 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
18:40:44.729085 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:40:44.729344 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
18:40:44.729645 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:40:44.740060 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
18:40:44.740476 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:40:44.740730 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:40:44.744254 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
18:40:44.748456 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:40:44.748791 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
18:40:44.749513 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:40:44.753018 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:40:44.753319 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
18:40:44.756772 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:40:44.759821 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
18:40:44.760119 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:40:44.760359 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
18:40:44.761483 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
18:40:44.764096 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:40:44.764381 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
18:40:44.766774 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
18:40:44.768621 [debug] [Thread-1  ]: finished collecting timing info
18:40:44.768951 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
18:40:44.769660 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb319193-6803-41e6-bfbe-321f5a69f7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f607936d3d0>]}
18:40:44.770213 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.06s]
18:40:44.770948 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
18:40:44.771282 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
18:40:44.771785 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week.................................. [RUN]
18:40:44.772543 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
18:40:44.772804 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
18:40:44.773047 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
18:40:44.777232 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
18:40:44.777797 [debug] [Thread-1  ]: finished collecting timing info
18:40:44.778067 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
18:40:44.782841 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
18:40:44.783420 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:40:44.783682 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
18:40:44.783899 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:40:44.794665 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
18:40:44.795172 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:40:44.795477 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_email_messages_count) as avg_email_count, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:40:44.801627 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
18:40:44.807902 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:40:44.808266 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week" rename to "email_per_week__dbt_backup"
18:40:44.808958 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:40:44.815628 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:40:44.815987 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
18:40:44.816688 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:40:44.819541 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
18:40:44.819841 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:40:44.820118 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
18:40:44.821334 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
18:40:44.823984 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:40:44.824249 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
18:40:44.826624 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
18:40:44.828621 [debug] [Thread-1  ]: finished collecting timing info
18:40:44.828942 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
18:40:44.832957 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb319193-6803-41e6-bfbe-321f5a69f7c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6078347040>]}
18:40:44.833607 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week............................. [[32mSELECT 53[0m in 0.06s]
18:40:44.834146 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
18:40:44.835520 [debug] [MainThread]: Acquiring new postgres connection "master"
18:40:44.835858 [debug] [MainThread]: Using postgres connection "master"
18:40:44.836101 [debug] [MainThread]: On master: BEGIN
18:40:44.836316 [debug] [MainThread]: Opening a new connection, currently in state closed
18:40:44.847015 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
18:40:44.847414 [debug] [MainThread]: On master: COMMIT
18:40:44.847659 [debug] [MainThread]: Using postgres connection "master"
18:40:44.847895 [debug] [MainThread]: On master: COMMIT
18:40:44.848240 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
18:40:44.848511 [debug] [MainThread]: On master: Close
18:40:44.849502 [info ] [MainThread]: 
18:40:44.849953 [info ] [MainThread]: Finished running 3 table models in 0.61s.
18:40:44.850362 [debug] [MainThread]: Connection 'master' was properly closed.
18:40:44.854009 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week' was properly closed.
18:40:44.862423 [info ] [MainThread]: 
18:40:44.862986 [info ] [MainThread]: [32mCompleted successfully[0m
18:40:44.863744 [info ] [MainThread]: 
18:40:44.864475 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
18:40:44.865189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f607b3a2ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f607895df70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60783b41f0>]}


============================== 2022-03-06 18:54:13.205671 | ca70bbe5-6185-4b68-9d27-0b4fa0abce8d ==============================
18:54:13.205671 [info ] [MainThread]: Running with dbt=1.0.3
18:54:13.207055 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
18:54:13.207464 [debug] [MainThread]: Tracking: tracking
18:54:13.232112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb2c59f7b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb2c59f7dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb2c59f7100>]}
18:54:13.319174 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
18:54:13.320031 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
18:54:13.339644 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
18:54:13.375722 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

18:54:13.398498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ca70bbe5-6185-4b68-9d27-0b4fa0abce8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb2c36350d0>]}
18:54:13.412982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ca70bbe5-6185-4b68-9d27-0b4fa0abce8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb2c3f57850>]}
18:54:13.413586 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
18:54:13.420064 [info ] [MainThread]: 
18:54:13.421480 [debug] [MainThread]: Acquiring new postgres connection "master"
18:54:13.423339 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
18:54:13.440356 [debug] [ThreadPool]: Using postgres connection "list_adludio"
18:54:13.440744 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
18:54:13.440996 [debug] [ThreadPool]: Opening a new connection, currently in state init
18:54:13.485523 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.04 seconds
18:54:13.488069 [debug] [ThreadPool]: On list_adludio: Close
18:54:13.494639 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
18:54:13.508092 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
18:54:13.508447 [debug] [ThreadPool]: On list_adludio_public: BEGIN
18:54:13.508694 [debug] [ThreadPool]: Opening a new connection, currently in state init
18:54:13.519670 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
18:54:13.520001 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
18:54:13.520237 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
18:54:13.523706 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.0 seconds
18:54:13.526235 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
18:54:13.527097 [debug] [ThreadPool]: On list_adludio_public: Close
18:54:13.534442 [debug] [MainThread]: Using postgres connection "master"
18:54:13.534744 [debug] [MainThread]: On master: BEGIN
18:54:13.534994 [debug] [MainThread]: Opening a new connection, currently in state init
18:54:13.549761 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
18:54:13.550113 [debug] [MainThread]: Using postgres connection "master"
18:54:13.555136 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
18:54:13.737135 [debug] [MainThread]: SQL status: SELECT 8 in 0.18 seconds
18:54:13.739943 [debug] [MainThread]: On master: ROLLBACK
18:54:13.740399 [debug] [MainThread]: Using postgres connection "master"
18:54:13.740667 [debug] [MainThread]: On master: BEGIN
18:54:13.741150 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
18:54:13.741411 [debug] [MainThread]: On master: COMMIT
18:54:13.741687 [debug] [MainThread]: Using postgres connection "master"
18:54:13.741912 [debug] [MainThread]: On master: COMMIT
18:54:13.742237 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
18:54:13.742498 [debug] [MainThread]: On master: Close
18:54:13.743075 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
18:54:13.751422 [info ] [MainThread]: 
18:54:13.757888 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
18:54:13.758351 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
18:54:13.759358 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:54:13.759634 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
18:54:13.759909 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
18:54:13.821875 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
18:54:13.822629 [debug] [Thread-1  ]: finished collecting timing info
18:54:13.822986 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
18:54:13.877571 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
18:54:13.878849 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:54:13.879136 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
18:54:13.879372 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:54:13.892963 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
18:54:13.893347 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:54:13.893663 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter, 
    CONCAT  (year, '_', week) AS year_week,
    CONCAT  (year, '_', quarter) AS year_quarter
    from sales_table 

)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter, year,year_week, year_quarter
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:54:13.894583 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "year" does not exist
LINE 21:     CONCAT  (year, '_', week) AS year_week,
                      ^

18:54:13.894884 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: ROLLBACK
18:54:13.895450 [debug] [Thread-1  ]: finished collecting timing info
18:54:13.895740 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
18:54:13.896375 [debug] [Thread-1  ]: Database Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)
  column "year" does not exist
  LINE 21:     CONCAT  (year, '_', week) AS year_week,
                        ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
18:54:13.896967 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca70bbe5-6185-4b68-9d27-0b4fa0abce8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb2c3edfca0>]}
18:54:13.897588 [error] [Thread-1  ]: 1 of 3 ERROR creating table model public.transformed_sales_number_data.......... [[31mERROR[0m in 0.14s]
18:54:13.898181 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
18:54:13.899325 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
18:54:13.899725 [info ] [Thread-1  ]: 2 of 3 SKIP relation public.deal_value_per_week................................. [[33mSKIP[0m]
18:54:13.900247 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
18:54:13.900552 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
18:54:13.900904 [info ] [Thread-1  ]: 3 of 3 SKIP relation public.email_per_week...................................... [[33mSKIP[0m]
18:54:13.901479 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
18:54:13.906521 [debug] [MainThread]: Acquiring new postgres connection "master"
18:54:13.906894 [debug] [MainThread]: Using postgres connection "master"
18:54:13.907126 [debug] [MainThread]: On master: BEGIN
18:54:13.907336 [debug] [MainThread]: Opening a new connection, currently in state closed
18:54:13.925738 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
18:54:13.926147 [debug] [MainThread]: On master: COMMIT
18:54:13.926412 [debug] [MainThread]: Using postgres connection "master"
18:54:13.929953 [debug] [MainThread]: On master: COMMIT
18:54:13.930392 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
18:54:13.930707 [debug] [MainThread]: On master: Close
18:54:13.931393 [info ] [MainThread]: 
18:54:13.932073 [info ] [MainThread]: Finished running 3 table models in 0.51s.
18:54:13.932651 [debug] [MainThread]: Connection 'master' was properly closed.
18:54:13.932878 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
18:54:13.933083 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
18:54:13.940789 [info ] [MainThread]: 
18:54:13.941307 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
18:54:13.941840 [info ] [MainThread]: 
18:54:13.942228 [error] [MainThread]: [33mDatabase Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)[0m
18:54:13.942598 [error] [MainThread]:   column "year" does not exist
18:54:13.942968 [error] [MainThread]:   LINE 21:     CONCAT  (year, '_', week) AS year_week,
18:54:13.943317 [error] [MainThread]:                         ^
18:54:13.943672 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
18:54:13.944053 [info ] [MainThread]: 
18:54:13.944452 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=2 TOTAL=3
18:54:13.944999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb2c35d8f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb2c35d8f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb2c59efc10>]}


============================== 2022-03-06 18:57:14.513700 | 9d14c9fe-7925-456c-b34e-087facbe68e5 ==============================
18:57:14.513700 [info ] [MainThread]: Running with dbt=1.0.3
18:57:14.514935 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
18:57:14.515360 [debug] [MainThread]: Tracking: tracking
18:57:14.520816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6976f66130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6976f662e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6976f66850>]}
18:57:14.582998 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
18:57:14.587337 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
18:57:14.608567 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
18:57:14.636990 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

18:57:14.649782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9d14c9fe-7925-456c-b34e-087facbe68e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6974bbe0d0>]}
18:57:14.659217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9d14c9fe-7925-456c-b34e-087facbe68e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f697545ba30>]}
18:57:14.659745 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
18:57:14.661915 [info ] [MainThread]: 
18:57:14.662708 [debug] [MainThread]: Acquiring new postgres connection "master"
18:57:14.664217 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
18:57:14.679438 [debug] [ThreadPool]: Using postgres connection "list_adludio"
18:57:14.679825 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
18:57:14.680103 [debug] [ThreadPool]: Opening a new connection, currently in state init
18:57:14.706004 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.03 seconds
18:57:14.708523 [debug] [ThreadPool]: On list_adludio: Close
18:57:14.710488 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
18:57:14.723364 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
18:57:14.723750 [debug] [ThreadPool]: On list_adludio_public: BEGIN
18:57:14.724021 [debug] [ThreadPool]: Opening a new connection, currently in state closed
18:57:14.734350 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
18:57:14.734788 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
18:57:14.735076 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
18:57:14.737444 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.0 seconds
18:57:14.739968 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
18:57:14.740361 [debug] [ThreadPool]: On list_adludio_public: Close
18:57:14.747835 [debug] [MainThread]: Using postgres connection "master"
18:57:14.748148 [debug] [MainThread]: On master: BEGIN
18:57:14.748383 [debug] [MainThread]: Opening a new connection, currently in state init
18:57:14.762642 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
18:57:14.762995 [debug] [MainThread]: Using postgres connection "master"
18:57:14.763245 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
18:57:14.928471 [debug] [MainThread]: SQL status: SELECT 8 in 0.16 seconds
18:57:14.931149 [debug] [MainThread]: On master: ROLLBACK
18:57:14.931623 [debug] [MainThread]: Using postgres connection "master"
18:57:14.931883 [debug] [MainThread]: On master: BEGIN
18:57:14.932319 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
18:57:14.932596 [debug] [MainThread]: On master: COMMIT
18:57:14.932823 [debug] [MainThread]: Using postgres connection "master"
18:57:14.933033 [debug] [MainThread]: On master: COMMIT
18:57:14.933326 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
18:57:14.933646 [debug] [MainThread]: On master: Close
18:57:14.936261 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
18:57:14.937064 [info ] [MainThread]: 
18:57:14.947708 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
18:57:14.948244 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
18:57:14.949057 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:57:14.949365 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
18:57:14.949678 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
18:57:14.955175 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
18:57:14.955822 [debug] [Thread-1  ]: finished collecting timing info
18:57:14.956169 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
18:57:15.147855 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
18:57:15.148582 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:57:15.148887 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
18:57:15.149152 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:57:15.166676 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
18:57:15.167058 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:57:15.167347 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter, 

    from sales_table 

)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter, year,
CONCAT  (year, '_', week) AS year_week,
CONCAT  (year, '_', quarter) AS year_quarter
from source_data

ORDER BY week

 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:57:15.167868 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "from"
LINE 22:     from sales_table 
             ^

18:57:15.168127 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: ROLLBACK
18:57:15.168643 [debug] [Thread-1  ]: finished collecting timing info
18:57:15.168928 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
18:57:15.169939 [debug] [Thread-1  ]: Database Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)
  syntax error at or near "from"
  LINE 22:     from sales_table 
               ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
18:57:15.190361 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9d14c9fe-7925-456c-b34e-087facbe68e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6974bc8610>]}
18:57:15.191106 [error] [Thread-1  ]: 1 of 3 ERROR creating table model public.transformed_sales_number_data.......... [[31mERROR[0m in 0.24s]
18:57:15.191991 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
18:57:15.193097 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
18:57:15.193565 [info ] [Thread-1  ]: 2 of 3 SKIP relation public.deal_value_per_week................................. [[33mSKIP[0m]
18:57:15.194286 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
18:57:15.194582 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
18:57:15.195001 [info ] [Thread-1  ]: 3 of 3 SKIP relation public.email_per_week...................................... [[33mSKIP[0m]
18:57:15.195659 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
18:57:15.197077 [debug] [MainThread]: Acquiring new postgres connection "master"
18:57:15.197385 [debug] [MainThread]: Using postgres connection "master"
18:57:15.197640 [debug] [MainThread]: On master: BEGIN
18:57:15.197852 [debug] [MainThread]: Opening a new connection, currently in state closed
18:57:15.211873 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
18:57:15.212303 [debug] [MainThread]: On master: COMMIT
18:57:15.212593 [debug] [MainThread]: Using postgres connection "master"
18:57:15.212842 [debug] [MainThread]: On master: COMMIT
18:57:15.213204 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
18:57:15.213512 [debug] [MainThread]: On master: Close
18:57:15.214188 [info ] [MainThread]: 
18:57:15.215088 [info ] [MainThread]: Finished running 3 table models in 0.55s.
18:57:15.216118 [debug] [MainThread]: Connection 'master' was properly closed.
18:57:15.216389 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
18:57:15.228417 [info ] [MainThread]: 
18:57:15.232596 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
18:57:15.233821 [info ] [MainThread]: 
18:57:15.234906 [error] [MainThread]: [33mDatabase Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)[0m
18:57:15.235881 [error] [MainThread]:   syntax error at or near "from"
18:57:15.236533 [error] [MainThread]:   LINE 22:     from sales_table 
18:57:15.237173 [error] [MainThread]:                ^
18:57:15.237998 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
18:57:15.238668 [info ] [MainThread]: 
18:57:15.239337 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=2 TOTAL=3
18:57:15.244127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6974b60040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6974b60400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6975467b80>]}


============================== 2022-03-06 18:57:36.643997 | d403b580-34c8-453b-88a8-7cd5eb5d4a34 ==============================
18:57:36.643997 [info ] [MainThread]: Running with dbt=1.0.3
18:57:36.644986 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
18:57:36.645400 [debug] [MainThread]: Tracking: tracking
18:57:36.653036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39febb53a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39febb5160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39febb5520>]}
18:57:36.712091 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
18:57:36.712903 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
18:57:36.732433 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
18:57:36.762180 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

18:57:36.770332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd403b580-34c8-453b-88a8-7cd5eb5d4a34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39fc80bfd0>]}
18:57:36.790531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd403b580-34c8-453b-88a8-7cd5eb5d4a34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39fd12e820>]}
18:57:36.791053 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
18:57:36.793234 [info ] [MainThread]: 
18:57:36.796203 [debug] [MainThread]: Acquiring new postgres connection "master"
18:57:36.805067 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
18:57:36.820599 [debug] [ThreadPool]: Using postgres connection "list_adludio"
18:57:36.821009 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
18:57:36.821298 [debug] [ThreadPool]: Opening a new connection, currently in state init
18:57:36.843056 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.02 seconds
18:57:36.845543 [debug] [ThreadPool]: On list_adludio: Close
18:57:36.849888 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
18:57:36.864138 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
18:57:36.864472 [debug] [ThreadPool]: On list_adludio_public: BEGIN
18:57:36.864923 [debug] [ThreadPool]: Opening a new connection, currently in state init
18:57:36.883565 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
18:57:36.883977 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
18:57:36.884277 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
18:57:36.888843 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.0 seconds
18:57:36.891562 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
18:57:36.891969 [debug] [ThreadPool]: On list_adludio_public: Close
18:57:36.898542 [debug] [MainThread]: Using postgres connection "master"
18:57:36.898858 [debug] [MainThread]: On master: BEGIN
18:57:36.899128 [debug] [MainThread]: Opening a new connection, currently in state init
18:57:36.912493 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
18:57:36.912860 [debug] [MainThread]: Using postgres connection "master"
18:57:36.913145 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
18:57:37.092502 [debug] [MainThread]: SQL status: SELECT 8 in 0.18 seconds
18:57:37.095352 [debug] [MainThread]: On master: ROLLBACK
18:57:37.095857 [debug] [MainThread]: Using postgres connection "master"
18:57:37.096145 [debug] [MainThread]: On master: BEGIN
18:57:37.096633 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
18:57:37.096941 [debug] [MainThread]: On master: COMMIT
18:57:37.097235 [debug] [MainThread]: Using postgres connection "master"
18:57:37.097521 [debug] [MainThread]: On master: COMMIT
18:57:37.097877 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
18:57:37.098162 [debug] [MainThread]: On master: Close
18:57:37.100865 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
18:57:37.101361 [info ] [MainThread]: 
18:57:37.107086 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
18:57:37.107588 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
18:57:37.108354 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:57:37.108618 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
18:57:37.108917 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
18:57:37.167825 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
18:57:37.168448 [debug] [Thread-1  ]: finished collecting timing info
18:57:37.168799 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
18:57:37.227016 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
18:57:37.227836 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:57:37.228157 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
18:57:37.228419 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:57:37.250178 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
18:57:37.250542 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:57:37.250794 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter

    from sales_table 

)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter, year,
CONCAT  (year, '_', week) AS year_week,
CONCAT  (year, '_', quarter) AS year_quarter
from source_data

ORDER BY week

 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:57:37.270189 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.02 seconds
18:57:37.280382 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:57:37.280739 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
18:57:37.287262 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.01 seconds
18:57:37.291194 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:57:37.291533 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
18:57:37.292178 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:57:37.308949 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
18:57:37.309341 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:57:37.309659 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
18:57:37.318143 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
18:57:37.325723 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
18:57:37.326042 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
18:57:37.330109 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
18:57:37.331953 [debug] [Thread-1  ]: finished collecting timing info
18:57:37.332258 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
18:57:37.332927 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd403b580-34c8-453b-88a8-7cd5eb5d4a34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39ff1ec190>]}
18:57:37.333486 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.22s]
18:57:37.334013 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
18:57:37.335412 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
18:57:37.335800 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
18:57:37.336534 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
18:57:37.336796 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
18:57:37.337070 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
18:57:37.343045 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
18:57:37.343589 [debug] [Thread-1  ]: finished collecting timing info
18:57:37.343854 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
18:57:37.348698 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
18:57:37.349229 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:57:37.349519 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
18:57:37.349753 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:57:37.369597 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
18:57:37.369940 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:57:37.370350 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:57:37.374015 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
18:57:37.378207 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:57:37.378496 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
18:57:37.379078 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:57:37.382430 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:57:37.382701 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
18:57:37.383221 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:57:37.385968 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
18:57:37.386234 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:57:37.386458 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
18:57:37.388649 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
18:57:37.391177 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
18:57:37.391440 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
18:57:37.395881 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
18:57:37.397611 [debug] [Thread-1  ]: finished collecting timing info
18:57:37.397923 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
18:57:37.398563 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd403b580-34c8-453b-88a8-7cd5eb5d4a34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39fd1bf520>]}
18:57:37.399352 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.06s]
18:57:37.400190 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
18:57:37.400596 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
18:57:37.401123 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week.................................. [RUN]
18:57:37.401980 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
18:57:37.402251 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
18:57:37.402523 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
18:57:37.410155 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
18:57:37.410663 [debug] [Thread-1  ]: finished collecting timing info
18:57:37.410932 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
18:57:37.415614 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
18:57:37.416130 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:57:37.416424 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
18:57:37.416658 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
18:57:37.427395 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
18:57:37.427701 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:57:37.427984 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_email_messages_count) as avg_email_count, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
18:57:37.434120 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
18:57:37.440169 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:57:37.440441 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week" rename to "email_per_week__dbt_backup"
18:57:37.443994 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:57:37.447383 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:57:37.447684 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
18:57:37.448253 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
18:57:37.450966 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
18:57:37.451251 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:57:37.451499 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
18:57:37.455996 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
18:57:37.458500 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
18:57:37.458798 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
18:57:37.463927 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
18:57:37.465984 [debug] [Thread-1  ]: finished collecting timing info
18:57:37.466336 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
18:57:37.467062 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd403b580-34c8-453b-88a8-7cd5eb5d4a34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39fc7987c0>]}
18:57:37.467703 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week............................. [[32mSELECT 53[0m in 0.07s]
18:57:37.468309 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
18:57:37.469746 [debug] [MainThread]: Acquiring new postgres connection "master"
18:57:37.470040 [debug] [MainThread]: Using postgres connection "master"
18:57:37.470266 [debug] [MainThread]: On master: BEGIN
18:57:37.470483 [debug] [MainThread]: Opening a new connection, currently in state closed
18:57:37.490128 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
18:57:37.490592 [debug] [MainThread]: On master: COMMIT
18:57:37.490891 [debug] [MainThread]: Using postgres connection "master"
18:57:37.491145 [debug] [MainThread]: On master: COMMIT
18:57:37.491494 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
18:57:37.491783 [debug] [MainThread]: On master: Close
18:57:37.492499 [info ] [MainThread]: 
18:57:37.492983 [info ] [MainThread]: Finished running 3 table models in 0.70s.
18:57:37.493380 [debug] [MainThread]: Connection 'master' was properly closed.
18:57:37.493682 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
18:57:37.493942 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week' was properly closed.
18:57:37.502305 [info ] [MainThread]: 
18:57:37.503162 [info ] [MainThread]: [32mCompleted successfully[0m
18:57:37.503996 [info ] [MainThread]: 
18:57:37.504599 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
18:57:37.505137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39fc7b99d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39fc7b9130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39fc7aa370>]}


============================== 2022-03-06 19:05:35.837005 | 7b13b234-d8e8-48d7-83cd-f4951b259ea5 ==============================
19:05:35.837005 [info ] [MainThread]: Running with dbt=1.0.3
19:05:35.838367 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
19:05:35.838780 [debug] [MainThread]: Tracking: tracking
19:05:35.844218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dcbeba370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dcbebac70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dcbeba880>]}
19:05:35.884842 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
19:05:35.885713 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
19:05:35.905314 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
19:05:35.930776 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

19:05:35.939516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7b13b234-d8e8-48d7-83cd-f4951b259ea5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dc9b12fa0>]}
19:05:35.948908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7b13b234-d8e8-48d7-83cd-f4951b259ea5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dca3b1970>]}
19:05:35.949442 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
19:05:35.951986 [info ] [MainThread]: 
19:05:35.952860 [debug] [MainThread]: Acquiring new postgres connection "master"
19:05:35.954756 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
19:05:35.970292 [debug] [ThreadPool]: Using postgres connection "list_adludio"
19:05:35.970704 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
19:05:35.970993 [debug] [ThreadPool]: Opening a new connection, currently in state init
19:05:35.986001 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.01 seconds
19:05:35.988502 [debug] [ThreadPool]: On list_adludio: Close
19:05:35.990542 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
19:05:35.999996 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
19:05:36.000322 [debug] [ThreadPool]: On list_adludio_public: BEGIN
19:05:36.000588 [debug] [ThreadPool]: Opening a new connection, currently in state init
19:05:36.011105 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
19:05:36.011517 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
19:05:36.011812 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
19:05:36.015074 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.0 seconds
19:05:36.017583 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
19:05:36.017999 [debug] [ThreadPool]: On list_adludio_public: Close
19:05:36.026349 [debug] [MainThread]: Using postgres connection "master"
19:05:36.026678 [debug] [MainThread]: On master: BEGIN
19:05:36.026917 [debug] [MainThread]: Opening a new connection, currently in state init
19:05:36.037706 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
19:05:36.038063 [debug] [MainThread]: Using postgres connection "master"
19:05:36.038339 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
19:05:36.217268 [debug] [MainThread]: SQL status: SELECT 8 in 0.18 seconds
19:05:36.220277 [debug] [MainThread]: On master: ROLLBACK
19:05:36.220846 [debug] [MainThread]: Using postgres connection "master"
19:05:36.221156 [debug] [MainThread]: On master: BEGIN
19:05:36.221671 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
19:05:36.221974 [debug] [MainThread]: On master: COMMIT
19:05:36.222238 [debug] [MainThread]: Using postgres connection "master"
19:05:36.222517 [debug] [MainThread]: On master: COMMIT
19:05:36.222857 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
19:05:36.223144 [debug] [MainThread]: On master: Close
19:05:36.228815 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
19:05:36.229816 [info ] [MainThread]: 
19:05:36.237266 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
19:05:36.237796 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
19:05:36.238953 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:05:36.239220 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
19:05:36.239483 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
19:05:36.327149 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
19:05:36.327796 [debug] [Thread-1  ]: finished collecting timing info
19:05:36.328134 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
19:05:36.392056 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
19:05:36.392661 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:05:36.392895 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
19:05:36.393071 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
19:05:36.406012 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
19:05:36.406447 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:05:36.406736 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter

    from sales_table 

)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter, year,
CONCAT  (year, '_', week) AS year_week,
CONCAT  (year, '_', quarter) AS year_quarter
from source_data

ORDER BY year_week

 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
19:05:36.434963 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.03 seconds
19:05:36.456442 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:05:36.456899 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
19:05:36.457699 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
19:05:36.461484 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:05:36.461755 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
19:05:36.462380 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
19:05:36.483250 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
19:05:36.492606 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:05:36.493048 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
19:05:36.496258 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
19:05:36.508605 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:05:36.508963 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
19:05:36.513878 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
19:05:36.516005 [debug] [Thread-1  ]: finished collecting timing info
19:05:36.516337 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
19:05:36.517236 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b13b234-d8e8-48d7-83cd-f4951b259ea5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dcc4ff040>]}
19:05:36.520959 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.28s]
19:05:36.521556 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
19:05:36.523070 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
19:05:36.523488 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
19:05:36.524271 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
19:05:36.524530 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
19:05:36.524765 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
19:05:36.532299 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
19:05:36.532863 [debug] [Thread-1  ]: finished collecting timing info
19:05:36.545103 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
19:05:36.560366 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
19:05:36.561121 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
19:05:36.561435 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
19:05:36.561757 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
19:05:36.576992 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
19:05:36.577383 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
19:05:36.577715 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
19:05:36.582815 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
19:05:36.590662 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
19:05:36.590974 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
19:05:36.591663 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
19:05:36.602247 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
19:05:36.602589 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
19:05:36.603254 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
19:05:36.606237 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
19:05:36.606523 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
19:05:36.606761 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
19:05:36.611323 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
19:05:36.617410 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
19:05:36.617820 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
19:05:36.628754 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
19:05:36.643963 [debug] [Thread-1  ]: finished collecting timing info
19:05:36.644399 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
19:05:36.645107 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b13b234-d8e8-48d7-83cd-f4951b259ea5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dca4c13d0>]}
19:05:36.645671 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.12s]
19:05:36.646629 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
19:05:36.647054 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
19:05:36.655298 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week.................................. [RUN]
19:05:36.656095 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
19:05:36.656364 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
19:05:36.656612 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
19:05:36.672049 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
19:05:36.672695 [debug] [Thread-1  ]: finished collecting timing info
19:05:36.672994 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
19:05:36.677214 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
19:05:36.677867 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
19:05:36.678139 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
19:05:36.678348 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
19:05:36.704481 [debug] [Thread-1  ]: SQL status: BEGIN in 0.03 seconds
19:05:36.704967 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
19:05:36.705274 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_email_messages_count) as avg_email_count, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
19:05:36.717710 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
19:05:36.730553 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
19:05:36.730991 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week" rename to "email_per_week__dbt_backup"
19:05:36.734926 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
19:05:36.738846 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
19:05:36.739199 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
19:05:36.739894 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
19:05:36.749440 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
19:05:36.749859 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
19:05:36.750201 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
19:05:36.752195 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
19:05:36.754834 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
19:05:36.755120 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
19:05:36.757580 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
19:05:36.759923 [debug] [Thread-1  ]: finished collecting timing info
19:05:36.763686 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
19:05:36.764505 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b13b234-d8e8-48d7-83cd-f4951b259ea5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dc9ab0190>]}
19:05:36.765103 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week............................. [[32mSELECT 53[0m in 0.11s]
19:05:36.765830 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
19:05:36.768640 [debug] [MainThread]: Acquiring new postgres connection "master"
19:05:36.768965 [debug] [MainThread]: Using postgres connection "master"
19:05:36.769208 [debug] [MainThread]: On master: BEGIN
19:05:36.769435 [debug] [MainThread]: Opening a new connection, currently in state closed
19:05:36.784840 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
19:05:36.785530 [debug] [MainThread]: On master: COMMIT
19:05:36.785887 [debug] [MainThread]: Using postgres connection "master"
19:05:36.786153 [debug] [MainThread]: On master: COMMIT
19:05:36.786526 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
19:05:36.786827 [debug] [MainThread]: On master: Close
19:05:36.787518 [info ] [MainThread]: 
19:05:36.788377 [info ] [MainThread]: Finished running 3 table models in 0.83s.
19:05:36.789527 [debug] [MainThread]: Connection 'master' was properly closed.
19:05:36.792932 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
19:05:36.793292 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week' was properly closed.
19:05:36.825582 [info ] [MainThread]: 
19:05:36.826308 [info ] [MainThread]: [32mCompleted successfully[0m
19:05:36.826870 [info ] [MainThread]: 
19:05:36.827304 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
19:05:36.835043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dca4b3970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dca4b36a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6dc80a3160>]}


============================== 2022-03-06 19:28:13.320081 | ef9da4d5-aa51-4953-a68b-86d88a37e789 ==============================
19:28:13.320081 [info ] [MainThread]: Running with dbt=1.0.3
19:28:13.321515 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
19:28:13.322024 [debug] [MainThread]: Tracking: tracking
19:28:13.327705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba2150f3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba2150f5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba2150f520>]}
19:28:13.386469 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
19:28:13.387428 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
19:28:13.412702 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
19:28:13.437792 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

19:28:13.446753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ef9da4d5-aa51-4953-a68b-86d88a37e789', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba1f1680d0>]}
19:28:13.462375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ef9da4d5-aa51-4953-a68b-86d88a37e789', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba1fa83820>]}
19:28:13.462968 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
19:28:13.465338 [info ] [MainThread]: 
19:28:13.466171 [debug] [MainThread]: Acquiring new postgres connection "master"
19:28:13.467790 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
19:28:13.486521 [debug] [ThreadPool]: Using postgres connection "list_adludio"
19:28:13.486903 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
19:28:13.487161 [debug] [ThreadPool]: Opening a new connection, currently in state init
19:28:13.500246 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.01 seconds
19:28:13.502745 [debug] [ThreadPool]: On list_adludio: Close
19:28:13.506500 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
19:28:13.515984 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
19:28:13.516356 [debug] [ThreadPool]: On list_adludio_public: BEGIN
19:28:13.516608 [debug] [ThreadPool]: Opening a new connection, currently in state closed
19:28:13.527332 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
19:28:13.527755 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
19:28:13.528151 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
19:28:13.534649 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.01 seconds
19:28:13.537615 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
19:28:13.538099 [debug] [ThreadPool]: On list_adludio_public: Close
19:28:13.549201 [debug] [MainThread]: Using postgres connection "master"
19:28:13.549573 [debug] [MainThread]: On master: BEGIN
19:28:13.549872 [debug] [MainThread]: Opening a new connection, currently in state init
19:28:13.562981 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
19:28:13.563373 [debug] [MainThread]: Using postgres connection "master"
19:28:13.563643 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
19:28:13.731091 [debug] [MainThread]: SQL status: SELECT 8 in 0.17 seconds
19:28:13.733803 [debug] [MainThread]: On master: ROLLBACK
19:28:13.734310 [debug] [MainThread]: Using postgres connection "master"
19:28:13.734588 [debug] [MainThread]: On master: BEGIN
19:28:13.735022 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
19:28:13.735297 [debug] [MainThread]: On master: COMMIT
19:28:13.735530 [debug] [MainThread]: Using postgres connection "master"
19:28:13.735734 [debug] [MainThread]: On master: COMMIT
19:28:13.736030 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
19:28:13.736281 [debug] [MainThread]: On master: Close
19:28:13.737888 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
19:28:13.738865 [info ] [MainThread]: 
19:28:13.747368 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
19:28:13.747929 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
19:28:13.748767 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:28:13.749069 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
19:28:13.749331 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
19:28:13.755256 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
19:28:13.755823 [debug] [Thread-1  ]: finished collecting timing info
19:28:13.756141 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
19:28:13.871994 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
19:28:13.872876 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:28:13.873174 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
19:28:13.873598 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
19:28:13.885939 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
19:28:13.886424 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:28:13.886765 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter

    from sales_table 

)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter, year,
CONCAT  (year, '_', week) AS year_week,
CONCAT  (year, '_', quarter) AS year_quarter
from source_data

ORDER BY year_week

 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
19:28:13.905974 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.02 seconds
19:28:13.916163 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:28:13.916741 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
19:28:13.917425 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
19:28:13.921011 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:28:13.921303 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
19:28:13.921936 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
19:28:13.939103 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
19:28:13.939507 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:28:13.939766 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
19:28:13.944461 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
19:28:13.952537 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:28:13.952859 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
19:28:13.963467 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
19:28:13.965593 [debug] [Thread-1  ]: finished collecting timing info
19:28:13.965961 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
19:28:13.966674 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ef9da4d5-aa51-4953-a68b-86d88a37e789', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba1f0e6f70>]}
19:28:13.967297 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.22s]
19:28:13.968085 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
19:28:13.969128 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
19:28:13.969649 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
19:28:13.970677 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
19:28:13.970965 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
19:28:13.971231 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
19:28:13.975773 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
19:28:13.976419 [debug] [Thread-1  ]: finished collecting timing info
19:28:13.976753 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
19:28:13.981627 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
19:28:13.982330 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
19:28:13.982615 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
19:28:13.982859 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
19:28:13.993386 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
19:28:13.993843 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
19:28:13.994127 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value)*7 as avg_deal_value, year_quarter
from source_data
group by year_quarter
ORDER BY year_quarter 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
19:28:14.000589 [debug] [Thread-1  ]: SQL status: SELECT 8 in 0.01 seconds
19:28:14.005053 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
19:28:14.005398 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
19:28:14.006107 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
19:28:14.009756 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
19:28:14.010044 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
19:28:14.010872 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
19:28:14.013711 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
19:28:14.014023 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
19:28:14.014252 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
19:28:14.018507 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
19:28:14.021258 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
19:28:14.021560 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
19:28:14.023789 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
19:28:14.025618 [debug] [Thread-1  ]: finished collecting timing info
19:28:14.025933 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
19:28:14.026578 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ef9da4d5-aa51-4953-a68b-86d88a37e789', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba21b54bb0>]}
19:28:14.027142 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 8[0m in 0.06s]
19:28:14.028222 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
19:28:14.028549 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
19:28:14.029098 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week.................................. [RUN]
19:28:14.030038 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
19:28:14.030309 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
19:28:14.030557 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
19:28:14.034929 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
19:28:14.035460 [debug] [Thread-1  ]: finished collecting timing info
19:28:14.035725 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
19:28:14.040323 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
19:28:14.040875 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
19:28:14.041129 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
19:28:14.041336 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
19:28:14.052469 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
19:28:14.052893 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
19:28:14.053190 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_email_messages_count) as avg_email_count, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
19:28:14.059101 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
19:28:14.064060 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
19:28:14.064450 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week" rename to "email_per_week__dbt_backup"
19:28:14.067269 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
19:28:14.074109 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
19:28:14.076924 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
19:28:14.077648 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
19:28:14.080343 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
19:28:14.080799 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
19:28:14.081053 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
19:28:14.082406 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
19:28:14.084919 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
19:28:14.085214 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
19:28:14.089566 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
19:28:14.096264 [debug] [Thread-1  ]: finished collecting timing info
19:28:14.096688 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
19:28:14.097544 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ef9da4d5-aa51-4953-a68b-86d88a37e789', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba1e165fd0>]}
19:28:14.098135 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week............................. [[32mSELECT 53[0m in 0.07s]
19:28:14.098945 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
19:28:14.105301 [debug] [MainThread]: Acquiring new postgres connection "master"
19:28:14.105680 [debug] [MainThread]: Using postgres connection "master"
19:28:14.105919 [debug] [MainThread]: On master: BEGIN
19:28:14.106154 [debug] [MainThread]: Opening a new connection, currently in state closed
19:28:14.118015 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
19:28:14.118413 [debug] [MainThread]: On master: COMMIT
19:28:14.118679 [debug] [MainThread]: Using postgres connection "master"
19:28:14.118895 [debug] [MainThread]: On master: COMMIT
19:28:14.119253 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
19:28:14.119533 [debug] [MainThread]: On master: Close
19:28:14.120195 [info ] [MainThread]: 
19:28:14.122793 [info ] [MainThread]: Finished running 3 table models in 0.65s.
19:28:14.123816 [debug] [MainThread]: Connection 'master' was properly closed.
19:28:14.124060 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week' was properly closed.
19:28:14.132039 [info ] [MainThread]: 
19:28:14.132968 [info ] [MainThread]: [32mCompleted successfully[0m
19:28:14.134020 [info ] [MainThread]: 
19:28:14.134576 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
19:28:14.135127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba1f108f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba1f108fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba1e147a60>]}


============================== 2022-03-06 19:58:40.286453 | a7b27c24-d805-4ca3-acde-5c8b36ee8cc5 ==============================
19:58:40.286453 [info ] [MainThread]: Running with dbt=1.0.3
19:58:40.287283 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
19:58:40.287777 [debug] [MainThread]: Tracking: tracking
19:58:40.293061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf16f93190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf16f93370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf16f937c0>]}
19:58:40.333662 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 3 files added, 2 files changed.
19:58:40.334351 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/ meeting_per_week.sql
19:58:40.336804 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/rfps_per_week.sql
19:58:40.337226 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/ios_per_week.sql
19:58:40.337723 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
19:58:40.339382 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/email_per_week.sql
19:58:40.359954 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/ meeting_per_week.sql
19:58:40.381384 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/rfps_per_week.sql
19:58:40.385938 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/ios_per_week.sql
19:58:40.389987 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
19:58:40.394487 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/email_per_week.sql
19:58:40.403741 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

19:58:40.412323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a7b27c24-d805-4ca3-acde-5c8b36ee8cc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf166530d0>]}
19:58:40.422192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a7b27c24-d805-4ca3-acde-5c8b36ee8cc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf16f63d60>]}
19:58:40.422718 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
19:58:40.424974 [info ] [MainThread]: 
19:58:40.425812 [debug] [MainThread]: Acquiring new postgres connection "master"
19:58:40.427618 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
19:58:40.442819 [debug] [ThreadPool]: Using postgres connection "list_adludio"
19:58:40.443165 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
19:58:40.443417 [debug] [ThreadPool]: Opening a new connection, currently in state init
19:58:40.456472 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.01 seconds
19:58:40.458844 [debug] [ThreadPool]: On list_adludio: Close
19:58:40.461841 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
19:58:40.471142 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
19:58:40.471471 [debug] [ThreadPool]: On list_adludio_public: BEGIN
19:58:40.471707 [debug] [ThreadPool]: Opening a new connection, currently in state closed
19:58:40.482313 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
19:58:40.482711 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
19:58:40.482965 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
19:58:40.486218 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.0 seconds
19:58:40.488695 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
19:58:40.489109 [debug] [ThreadPool]: On list_adludio_public: Close
19:58:40.497242 [debug] [MainThread]: Using postgres connection "master"
19:58:40.497585 [debug] [MainThread]: On master: BEGIN
19:58:40.497824 [debug] [MainThread]: Opening a new connection, currently in state init
19:58:40.508915 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
19:58:40.509277 [debug] [MainThread]: Using postgres connection "master"
19:58:40.509629 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
19:58:40.671586 [debug] [MainThread]: SQL status: SELECT 8 in 0.16 seconds
19:58:40.674332 [debug] [MainThread]: On master: ROLLBACK
19:58:40.674820 [debug] [MainThread]: Using postgres connection "master"
19:58:40.675090 [debug] [MainThread]: On master: BEGIN
19:58:40.675537 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
19:58:40.675802 [debug] [MainThread]: On master: COMMIT
19:58:40.676045 [debug] [MainThread]: Using postgres connection "master"
19:58:40.676269 [debug] [MainThread]: On master: COMMIT
19:58:40.676593 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
19:58:40.676865 [debug] [MainThread]: On master: Close
19:58:40.677449 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
19:58:40.678413 [info ] [MainThread]: 
19:58:40.736970 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
19:58:40.737641 [info ] [Thread-1  ]: 1 of 6 START table model public.transformed_sales_number_data................... [RUN]
19:58:40.738529 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:58:40.738829 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
19:58:40.739139 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
19:58:40.743036 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
19:58:40.743628 [debug] [Thread-1  ]: finished collecting timing info
19:58:40.743942 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
19:58:40.787644 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
19:58:40.788483 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:58:40.788795 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
19:58:40.789051 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
19:58:40.800180 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
19:58:40.800563 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
19:58:40.800848 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter

    from sales_table 

)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,

count(case when deal_Stage = 'Meeting' then 1 else 0 end) as deal_meeting_count,
count(case when deal_Stage = 'IO Sent' then 1 else 0 end) as deal_iosent_count,
count(case when deal_Stage = 'RFP' then 1 else 0 end) as deal_rfp_count,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter, year,
CONCAT  (year, '_', week) AS year_week,
CONCAT  (year, '_', quarter) AS year_quarter
from source_data

ORDER BY year_week

 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
19:58:40.801902 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal_stage" does not exist
LINE 29: count(case when deal_Stage = 'Meeting' then 1 else 0 end) as...
                         ^
HINT:  Perhaps you meant to reference the column "source_data.Deal_Stage".

19:58:40.802204 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: ROLLBACK
19:58:40.802739 [debug] [Thread-1  ]: finished collecting timing info
19:58:40.803065 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
19:58:40.803685 [debug] [Thread-1  ]: Database Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)
  column "deal_stage" does not exist
  LINE 29: count(case when deal_Stage = 'Meeting' then 1 else 0 end) as...
                           ^
  HINT:  Perhaps you meant to reference the column "source_data.Deal_Stage".
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
19:58:40.804185 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7b27c24-d805-4ca3-acde-5c8b36ee8cc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf165f1e20>]}
19:58:40.804753 [error] [Thread-1  ]: 1 of 6 ERROR creating table model public.transformed_sales_number_data.......... [[31mERROR[0m in 0.07s]
19:58:40.805341 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
19:58:40.806589 [debug] [Thread-1  ]: Began running node model.Analytics_dbt. meeting_per_week
19:58:40.806888 [info ] [Thread-1  ]: 2 of 6 SKIP relation public. meeting_per_week................................... [[33mSKIP[0m]
19:58:40.807371 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt. meeting_per_week
19:58:40.807628 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
19:58:40.808017 [info ] [Thread-1  ]: 3 of 6 SKIP relation public.deal_value_per_week................................. [[33mSKIP[0m]
19:58:40.808466 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
19:58:40.808720 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
19:58:40.809081 [info ] [Thread-1  ]: 4 of 6 SKIP relation public.email_per_week...................................... [[33mSKIP[0m]
19:58:40.809590 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
19:58:40.809871 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.ios_per_week
19:58:40.810202 [info ] [Thread-1  ]: 5 of 6 SKIP relation public.ios_per_week........................................ [[33mSKIP[0m]
19:58:40.810748 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.ios_per_week
19:58:40.811006 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.rfps_per_week
19:58:40.811231 [info ] [Thread-1  ]: 6 of 6 SKIP relation public.rfps_per_week....................................... [[33mSKIP[0m]
19:58:40.811821 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.rfps_per_week
19:58:40.813123 [debug] [MainThread]: Acquiring new postgres connection "master"
19:58:40.813430 [debug] [MainThread]: Using postgres connection "master"
19:58:40.813691 [debug] [MainThread]: On master: BEGIN
19:58:40.813904 [debug] [MainThread]: Opening a new connection, currently in state closed
19:58:40.824736 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
19:58:40.825090 [debug] [MainThread]: On master: COMMIT
19:58:40.825335 [debug] [MainThread]: Using postgres connection "master"
19:58:40.825646 [debug] [MainThread]: On master: COMMIT
19:58:40.826009 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
19:58:40.826270 [debug] [MainThread]: On master: Close
19:58:40.826974 [info ] [MainThread]: 
19:58:40.827829 [info ] [MainThread]: Finished running 6 table models in 0.40s.
19:58:40.828337 [debug] [MainThread]: Connection 'master' was properly closed.
19:58:40.828587 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
19:58:40.836226 [info ] [MainThread]: 
19:58:40.836738 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
19:58:40.837213 [info ] [MainThread]: 
19:58:40.837655 [error] [MainThread]: [33mDatabase Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)[0m
19:58:40.838055 [error] [MainThread]:   column "deal_stage" does not exist
19:58:40.838438 [error] [MainThread]:   LINE 29: count(case when deal_Stage = 'Meeting' then 1 else 0 end) as...
19:58:40.838804 [error] [MainThread]:                            ^
19:58:40.839167 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "source_data.Deal_Stage".
19:58:40.839533 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
19:58:40.839912 [info ] [MainThread]: 
19:58:40.840296 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=5 TOTAL=6
19:58:40.840845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf192fc2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf192fc100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf16ee4250>]}


============================== 2022-03-06 20:00:14.028905 | 720f87e0-4d32-45d1-8d73-41206b07a5cd ==============================
20:00:14.028905 [info ] [MainThread]: Running with dbt=1.0.3
20:00:14.029880 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
20:00:14.030302 [debug] [MainThread]: Tracking: tracking
20:00:14.035889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08c456e190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08c456e370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08c456e7c0>]}
20:00:14.088057 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
20:00:14.088874 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
20:00:14.118400 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
20:00:14.144309 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

20:00:14.153998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '720f87e0-4d32-45d1-8d73-41206b07a5cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08c40280d0>]}
20:00:14.164297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '720f87e0-4d32-45d1-8d73-41206b07a5cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08c456e220>]}
20:00:14.164899 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
20:00:14.167853 [info ] [MainThread]: 
20:00:14.168718 [debug] [MainThread]: Acquiring new postgres connection "master"
20:00:14.170782 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
20:00:14.186353 [debug] [ThreadPool]: Using postgres connection "list_adludio"
20:00:14.186792 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
20:00:14.187109 [debug] [ThreadPool]: Opening a new connection, currently in state init
20:00:14.206094 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.02 seconds
20:00:14.208585 [debug] [ThreadPool]: On list_adludio: Close
20:00:14.214043 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
20:00:14.223716 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
20:00:14.224077 [debug] [ThreadPool]: On list_adludio_public: BEGIN
20:00:14.224411 [debug] [ThreadPool]: Opening a new connection, currently in state closed
20:00:14.235281 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
20:00:14.235735 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
20:00:14.236011 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
20:00:14.241497 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.01 seconds
20:00:14.251708 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
20:00:14.252301 [debug] [ThreadPool]: On list_adludio_public: Close
20:00:14.265557 [debug] [MainThread]: Using postgres connection "master"
20:00:14.266017 [debug] [MainThread]: On master: BEGIN
20:00:14.266311 [debug] [MainThread]: Opening a new connection, currently in state init
20:00:14.288489 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
20:00:14.289029 [debug] [MainThread]: Using postgres connection "master"
20:00:14.289396 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
20:00:14.464584 [debug] [MainThread]: SQL status: SELECT 8 in 0.17 seconds
20:00:14.467384 [debug] [MainThread]: On master: ROLLBACK
20:00:14.467933 [debug] [MainThread]: Using postgres connection "master"
20:00:14.468204 [debug] [MainThread]: On master: BEGIN
20:00:14.468681 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
20:00:14.471034 [debug] [MainThread]: On master: COMMIT
20:00:14.471270 [debug] [MainThread]: Using postgres connection "master"
20:00:14.471476 [debug] [MainThread]: On master: COMMIT
20:00:14.473550 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
20:00:14.473853 [debug] [MainThread]: On master: Close
20:00:14.474616 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
20:00:14.475356 [info ] [MainThread]: 
20:00:14.481181 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
20:00:14.481703 [info ] [Thread-1  ]: 1 of 6 START table model public.transformed_sales_number_data................... [RUN]
20:00:14.482489 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:00:14.482773 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
20:00:14.483036 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
20:00:14.548793 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
20:00:14.549484 [debug] [Thread-1  ]: finished collecting timing info
20:00:14.549810 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
20:00:14.595148 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
20:00:14.595853 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:00:14.596148 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
20:00:14.596385 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
20:00:14.609436 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
20:00:14.609863 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:00:14.610134 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter

    from sales_table 

)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_stage,

count(case when deal_stage = 'Meeting' then 1 else 0 end) as deal_meeting_count,
count(case when deal_stage = 'IO Sent' then 1 else 0 end) as deal_iosent_count,
count(case when deal_stage = 'RFP' then 1 else 0 end) as deal_rfp_count,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter, year,
CONCAT  (year, '_', week) AS year_week,
CONCAT  (year, '_', quarter) AS year_quarter
from source_data

ORDER BY year_week

 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
20:00:14.611183 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal_stage" does not exist
LINE 29: count(case when deal_stage = 'Meeting' then 1 else 0 end) as...
                         ^
HINT:  Perhaps you meant to reference the column "source_data.Deal_Stage".

20:00:14.611466 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: ROLLBACK
20:00:14.612031 [debug] [Thread-1  ]: finished collecting timing info
20:00:14.612329 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
20:00:14.612898 [debug] [Thread-1  ]: Database Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)
  column "deal_stage" does not exist
  LINE 29: count(case when deal_stage = 'Meeting' then 1 else 0 end) as...
                           ^
  HINT:  Perhaps you meant to reference the column "source_data.Deal_Stage".
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
20:00:14.613364 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '720f87e0-4d32-45d1-8d73-41206b07a5cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08c4465c70>]}
20:00:14.613973 [error] [Thread-1  ]: 1 of 6 ERROR creating table model public.transformed_sales_number_data.......... [[31mERROR[0m in 0.13s]
20:00:14.614789 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
20:00:14.617875 [debug] [Thread-1  ]: Began running node model.Analytics_dbt. meeting_per_week
20:00:14.618218 [info ] [Thread-1  ]: 2 of 6 SKIP relation public. meeting_per_week................................... [[33mSKIP[0m]
20:00:14.618712 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt. meeting_per_week
20:00:14.621649 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
20:00:14.622005 [info ] [Thread-1  ]: 3 of 6 SKIP relation public.deal_value_per_week................................. [[33mSKIP[0m]
20:00:14.628738 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
20:00:14.629173 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
20:00:14.629561 [info ] [Thread-1  ]: 4 of 6 SKIP relation public.email_per_week...................................... [[33mSKIP[0m]
20:00:14.630045 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
20:00:14.630305 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.ios_per_week
20:00:14.630606 [info ] [Thread-1  ]: 5 of 6 SKIP relation public.ios_per_week........................................ [[33mSKIP[0m]
20:00:14.631091 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.ios_per_week
20:00:14.631379 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.rfps_per_week
20:00:14.631668 [info ] [Thread-1  ]: 6 of 6 SKIP relation public.rfps_per_week....................................... [[33mSKIP[0m]
20:00:14.632105 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.rfps_per_week
20:00:14.634531 [debug] [MainThread]: Acquiring new postgres connection "master"
20:00:14.634884 [debug] [MainThread]: Using postgres connection "master"
20:00:14.635112 [debug] [MainThread]: On master: BEGIN
20:00:14.635325 [debug] [MainThread]: Opening a new connection, currently in state closed
20:00:14.648695 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
20:00:14.649105 [debug] [MainThread]: On master: COMMIT
20:00:14.649378 [debug] [MainThread]: Using postgres connection "master"
20:00:14.649668 [debug] [MainThread]: On master: COMMIT
20:00:14.650009 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
20:00:14.650266 [debug] [MainThread]: On master: Close
20:00:14.650920 [info ] [MainThread]: 
20:00:14.651683 [info ] [MainThread]: Finished running 6 table models in 0.48s.
20:00:14.652335 [debug] [MainThread]: Connection 'master' was properly closed.
20:00:14.652601 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
20:00:14.664265 [info ] [MainThread]: 
20:00:14.664867 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
20:00:14.665402 [info ] [MainThread]: 
20:00:14.665982 [error] [MainThread]: [33mDatabase Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)[0m
20:00:14.666416 [error] [MainThread]:   column "deal_stage" does not exist
20:00:14.666927 [error] [MainThread]:   LINE 29: count(case when deal_stage = 'Meeting' then 1 else 0 end) as...
20:00:14.667491 [error] [MainThread]:                            ^
20:00:14.668002 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "source_data.Deal_Stage".
20:00:14.668533 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
20:00:14.669083 [info ] [MainThread]: 
20:00:14.669691 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=5 TOTAL=6
20:00:14.670428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08c44c0af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08bedece20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08bedfcaf0>]}


============================== 2022-03-06 20:02:42.439582 | bc5a01a3-4769-4951-abb1-c12035ee8434 ==============================
20:02:42.439582 [info ] [MainThread]: Running with dbt=1.0.3
20:02:42.457331 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
20:02:42.457918 [debug] [MainThread]: Tracking: tracking
20:02:42.463932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6a85fe6190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6a85fe6370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6a85fe67c0>]}
20:02:42.509223 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
20:02:42.510048 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
20:02:42.536633 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
20:02:42.562319 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

20:02:42.574839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bc5a01a3-4769-4951-abb1-c12035ee8434', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6a856a50d0>]}
20:02:42.584995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bc5a01a3-4769-4951-abb1-c12035ee8434', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6a85fe6130>]}
20:02:42.585594 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
20:02:42.588740 [info ] [MainThread]: 
20:02:42.589895 [debug] [MainThread]: Acquiring new postgres connection "master"
20:02:42.592242 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
20:02:42.607530 [debug] [ThreadPool]: Using postgres connection "list_adludio"
20:02:42.607913 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
20:02:42.608210 [debug] [ThreadPool]: Opening a new connection, currently in state init
20:02:42.625072 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.02 seconds
20:02:42.627615 [debug] [ThreadPool]: On list_adludio: Close
20:02:42.632924 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
20:02:42.642543 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
20:02:42.642892 [debug] [ThreadPool]: On list_adludio_public: BEGIN
20:02:42.643134 [debug] [ThreadPool]: Opening a new connection, currently in state closed
20:02:42.653730 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
20:02:42.654093 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
20:02:42.657590 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
20:02:42.662267 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.0 seconds
20:02:42.664765 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
20:02:42.665169 [debug] [ThreadPool]: On list_adludio_public: Close
20:02:42.672037 [debug] [MainThread]: Using postgres connection "master"
20:02:42.672396 [debug] [MainThread]: On master: BEGIN
20:02:42.672640 [debug] [MainThread]: Opening a new connection, currently in state init
20:02:42.683038 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
20:02:42.683377 [debug] [MainThread]: Using postgres connection "master"
20:02:42.684447 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
20:02:42.852331 [debug] [MainThread]: SQL status: SELECT 8 in 0.17 seconds
20:02:42.855148 [debug] [MainThread]: On master: ROLLBACK
20:02:42.855683 [debug] [MainThread]: Using postgres connection "master"
20:02:42.856008 [debug] [MainThread]: On master: BEGIN
20:02:42.856489 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
20:02:42.856775 [debug] [MainThread]: On master: COMMIT
20:02:42.857025 [debug] [MainThread]: Using postgres connection "master"
20:02:42.857255 [debug] [MainThread]: On master: COMMIT
20:02:42.857623 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
20:02:42.857903 [debug] [MainThread]: On master: Close
20:02:42.858534 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
20:02:42.859265 [info ] [MainThread]: 
20:02:42.867912 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
20:02:42.868467 [info ] [Thread-1  ]: 1 of 6 START table model public.transformed_sales_number_data................... [RUN]
20:02:42.869531 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:02:42.869804 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
20:02:42.870063 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
20:02:42.930605 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
20:02:42.931341 [debug] [Thread-1  ]: finished collecting timing info
20:02:42.931659 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
20:02:42.984985 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
20:02:42.988997 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:02:42.989373 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
20:02:42.989682 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
20:02:43.011839 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
20:02:43.012236 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:02:43.012596 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter

    from sales_table 

)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_stage,

    case when Deal_Stage = 'Meeting' then 1 else 0 end as deal_meeting_count,
    case when Deal_Stage = 'IO Sent' then 1 else 0 end as deal_iosent_count,
    case when Deal_Stage = 'RFP' then 1 else 0 end  as deal_rfp_count,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter, year,
CONCAT  (year, '_', week) AS year_week,
CONCAT  (year, '_', quarter) AS year_quarter
from source_data

ORDER BY year_week

 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
20:02:43.013668 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal_stage" does not exist
LINE 29:     case when Deal_Stage = 'Meeting' then 1 else 0 end as de...
                       ^
HINT:  Perhaps you meant to reference the column "source_data.Deal_Stage".

20:02:43.013968 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: ROLLBACK
20:02:43.014510 [debug] [Thread-1  ]: finished collecting timing info
20:02:43.014812 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
20:02:43.022270 [debug] [Thread-1  ]: Database Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)
  column "deal_stage" does not exist
  LINE 29:     case when Deal_Stage = 'Meeting' then 1 else 0 end as de...
                         ^
  HINT:  Perhaps you meant to reference the column "source_data.Deal_Stage".
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
20:02:43.022858 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc5a01a3-4769-4951-abb1-c12035ee8434', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6a85ee1af0>]}
20:02:43.023429 [error] [Thread-1  ]: 1 of 6 ERROR creating table model public.transformed_sales_number_data.......... [[31mERROR[0m in 0.15s]
20:02:43.024041 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
20:02:43.027889 [debug] [Thread-1  ]: Began running node model.Analytics_dbt. meeting_per_week
20:02:43.028276 [info ] [Thread-1  ]: 2 of 6 SKIP relation public. meeting_per_week................................... [[33mSKIP[0m]
20:02:43.028843 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt. meeting_per_week
20:02:43.029360 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
20:02:43.029776 [info ] [Thread-1  ]: 3 of 6 SKIP relation public.deal_value_per_week................................. [[33mSKIP[0m]
20:02:43.030416 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
20:02:43.030733 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
20:02:43.031110 [info ] [Thread-1  ]: 4 of 6 SKIP relation public.email_per_week...................................... [[33mSKIP[0m]
20:02:43.031613 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
20:02:43.032142 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.ios_per_week
20:02:43.032514 [info ] [Thread-1  ]: 5 of 6 SKIP relation public.ios_per_week........................................ [[33mSKIP[0m]
20:02:43.033025 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.ios_per_week
20:02:43.033526 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.rfps_per_week
20:02:43.033909 [info ] [Thread-1  ]: 6 of 6 SKIP relation public.rfps_per_week....................................... [[33mSKIP[0m]
20:02:43.034677 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.rfps_per_week
20:02:43.036896 [debug] [MainThread]: Acquiring new postgres connection "master"
20:02:43.037239 [debug] [MainThread]: Using postgres connection "master"
20:02:43.037520 [debug] [MainThread]: On master: BEGIN
20:02:43.037748 [debug] [MainThread]: Opening a new connection, currently in state closed
20:02:43.053551 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
20:02:43.053998 [debug] [MainThread]: On master: COMMIT
20:02:43.054261 [debug] [MainThread]: Using postgres connection "master"
20:02:43.054491 [debug] [MainThread]: On master: COMMIT
20:02:43.054835 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
20:02:43.055130 [debug] [MainThread]: On master: Close
20:02:43.055801 [info ] [MainThread]: 
20:02:43.056698 [info ] [MainThread]: Finished running 6 table models in 0.47s.
20:02:43.057337 [debug] [MainThread]: Connection 'master' was properly closed.
20:02:43.057625 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
20:02:43.065566 [info ] [MainThread]: 
20:02:43.066280 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
20:02:43.066888 [info ] [MainThread]: 
20:02:43.067438 [error] [MainThread]: [33mDatabase Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)[0m
20:02:43.068143 [error] [MainThread]:   column "deal_stage" does not exist
20:02:43.068629 [error] [MainThread]:   LINE 29:     case when Deal_Stage = 'Meeting' then 1 else 0 end as de...
20:02:43.069205 [error] [MainThread]:                          ^
20:02:43.069756 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "source_data.Deal_Stage".
20:02:43.070186 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
20:02:43.070651 [info ] [MainThread]: 
20:02:43.071048 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=5 TOTAL=6
20:02:43.071585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6a85f3cb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6a85f3cdf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6a85ee19d0>]}


============================== 2022-03-06 20:04:02.665194 | 210d126c-da42-44a9-b506-0abf5cee65c6 ==============================
20:04:02.665194 [info ] [MainThread]: Running with dbt=1.0.3
20:04:02.669980 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
20:04:02.670450 [debug] [MainThread]: Tracking: tracking
20:04:02.680311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf5c09d3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf5c09db80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf5c09d520>]}
20:04:02.760101 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
20:04:02.760612 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
20:04:02.761582 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

20:04:02.773504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '210d126c-da42-44a9-b506-0abf5cee65c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf5a54b0d0>]}
20:04:02.785340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '210d126c-da42-44a9-b506-0abf5cee65c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf5a5c6280>]}
20:04:02.785950 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
20:04:02.789156 [info ] [MainThread]: 
20:04:02.793987 [debug] [MainThread]: Acquiring new postgres connection "master"
20:04:02.795943 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
20:04:02.811312 [debug] [ThreadPool]: Using postgres connection "list_adludio"
20:04:02.811668 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
20:04:02.811924 [debug] [ThreadPool]: Opening a new connection, currently in state init
20:04:02.825007 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.01 seconds
20:04:02.827381 [debug] [ThreadPool]: On list_adludio: Close
20:04:02.829931 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
20:04:02.839350 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
20:04:02.839690 [debug] [ThreadPool]: On list_adludio_public: BEGIN
20:04:02.839950 [debug] [ThreadPool]: Opening a new connection, currently in state init
20:04:02.850859 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
20:04:02.851229 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
20:04:02.851483 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
20:04:02.854819 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.0 seconds
20:04:02.857358 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
20:04:02.857840 [debug] [ThreadPool]: On list_adludio_public: Close
20:04:02.865259 [debug] [MainThread]: Using postgres connection "master"
20:04:02.865706 [debug] [MainThread]: On master: BEGIN
20:04:02.866072 [debug] [MainThread]: Opening a new connection, currently in state init
20:04:02.877878 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
20:04:02.881441 [debug] [MainThread]: Using postgres connection "master"
20:04:02.881796 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
20:04:03.077887 [debug] [MainThread]: SQL status: SELECT 8 in 0.2 seconds
20:04:03.081031 [debug] [MainThread]: On master: ROLLBACK
20:04:03.081559 [debug] [MainThread]: Using postgres connection "master"
20:04:03.081817 [debug] [MainThread]: On master: BEGIN
20:04:03.084575 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
20:04:03.084920 [debug] [MainThread]: On master: COMMIT
20:04:03.085203 [debug] [MainThread]: Using postgres connection "master"
20:04:03.085498 [debug] [MainThread]: On master: COMMIT
20:04:03.085876 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
20:04:03.086164 [debug] [MainThread]: On master: Close
20:04:03.089161 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
20:04:03.089728 [info ] [MainThread]: 
20:04:03.110324 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
20:04:03.110907 [info ] [Thread-1  ]: 1 of 6 START table model public.transformed_sales_number_data................... [RUN]
20:04:03.111769 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:04:03.112048 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
20:04:03.112313 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
20:04:03.120165 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
20:04:03.120888 [debug] [Thread-1  ]: finished collecting timing info
20:04:03.121213 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
20:04:03.172305 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
20:04:03.172938 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:04:03.173218 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
20:04:03.173708 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
20:04:03.187439 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
20:04:03.187878 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:04:03.188155 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter

    from sales_table 

)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_stage,

    case when Deal_Stage = 'Meeting' then 1 else 0 end as deal_meeting_count,
    case when Deal_Stage = 'IO Sent' then 1 else 0 end as deal_iosent_count,
    case when Deal_Stage = 'RFP' then 1 else 0 end  as deal_rfp_count,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter, year,
CONCAT  (year, '_', week) AS year_week,
CONCAT  (year, '_', quarter) AS year_quarter
from source_data

ORDER BY year_week

 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
20:04:03.189117 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal_stage" does not exist
LINE 29:     case when Deal_Stage = 'Meeting' then 1 else 0 end as de...
                       ^
HINT:  Perhaps you meant to reference the column "source_data.Deal_Stage".

20:04:03.189430 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: ROLLBACK
20:04:03.190053 [debug] [Thread-1  ]: finished collecting timing info
20:04:03.190360 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
20:04:03.191057 [debug] [Thread-1  ]: Database Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)
  column "deal_stage" does not exist
  LINE 29:     case when Deal_Stage = 'Meeting' then 1 else 0 end as de...
                         ^
  HINT:  Perhaps you meant to reference the column "source_data.Deal_Stage".
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
20:04:03.191580 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '210d126c-da42-44a9-b506-0abf5cee65c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf580f8a00>]}
20:04:03.192167 [error] [Thread-1  ]: 1 of 6 ERROR creating table model public.transformed_sales_number_data.......... [[31mERROR[0m in 0.08s]
20:04:03.193249 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
20:04:03.205943 [debug] [Thread-1  ]: Began running node model.Analytics_dbt. meeting_per_week
20:04:03.212613 [info ] [Thread-1  ]: 2 of 6 SKIP relation public. meeting_per_week................................... [[33mSKIP[0m]
20:04:03.213859 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt. meeting_per_week
20:04:03.214184 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
20:04:03.214489 [info ] [Thread-1  ]: 3 of 6 SKIP relation public.deal_value_per_week................................. [[33mSKIP[0m]
20:04:03.219628 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
20:04:03.219974 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
20:04:03.220566 [info ] [Thread-1  ]: 4 of 6 SKIP relation public.email_per_week...................................... [[33mSKIP[0m]
20:04:03.221662 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
20:04:03.222015 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.ios_per_week
20:04:03.222358 [info ] [Thread-1  ]: 5 of 6 SKIP relation public.ios_per_week........................................ [[33mSKIP[0m]
20:04:03.240320 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.ios_per_week
20:04:03.242900 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.rfps_per_week
20:04:03.243237 [info ] [Thread-1  ]: 6 of 6 SKIP relation public.rfps_per_week....................................... [[33mSKIP[0m]
20:04:03.243830 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.rfps_per_week
20:04:03.245607 [debug] [MainThread]: Acquiring new postgres connection "master"
20:04:03.245941 [debug] [MainThread]: Using postgres connection "master"
20:04:03.246198 [debug] [MainThread]: On master: BEGIN
20:04:03.246417 [debug] [MainThread]: Opening a new connection, currently in state closed
20:04:03.260448 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
20:04:03.260871 [debug] [MainThread]: On master: COMMIT
20:04:03.261087 [debug] [MainThread]: Using postgres connection "master"
20:04:03.269636 [debug] [MainThread]: On master: COMMIT
20:04:03.270156 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
20:04:03.270488 [debug] [MainThread]: On master: Close
20:04:03.271192 [info ] [MainThread]: 
20:04:03.272323 [info ] [MainThread]: Finished running 6 table models in 0.48s.
20:04:03.273846 [debug] [MainThread]: Connection 'master' was properly closed.
20:04:03.274096 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
20:04:03.274335 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
20:04:03.282508 [info ] [MainThread]: 
20:04:03.283567 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
20:04:03.284653 [info ] [MainThread]: 
20:04:03.285368 [error] [MainThread]: [33mDatabase Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)[0m
20:04:03.286293 [error] [MainThread]:   column "deal_stage" does not exist
20:04:03.287160 [error] [MainThread]:   LINE 29:     case when Deal_Stage = 'Meeting' then 1 else 0 end as de...
20:04:03.288072 [error] [MainThread]:                          ^
20:04:03.288494 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "source_data.Deal_Stage".
20:04:03.289065 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
20:04:03.289685 [info ] [MainThread]: 
20:04:03.290441 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=5 TOTAL=6
20:04:03.291257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf58b01e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf58b0da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf580f88b0>]}


============================== 2022-03-06 20:05:41.553373 | 0a73a330-c36b-4466-b1cb-4e9ca76ac53f ==============================
20:05:41.553373 [info ] [MainThread]: Running with dbt=1.0.3
20:05:41.554547 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
20:05:41.554955 [debug] [MainThread]: Tracking: tracking
20:05:41.560276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3379990190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3379990370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33799907c0>]}
20:05:41.608837 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
20:05:41.609683 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
20:05:41.632539 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
20:05:41.664860 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

20:05:41.674357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0a73a330-c36b-4466-b1cb-4e9ca76ac53f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f337904a0d0>]}
20:05:41.683607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0a73a330-c36b-4466-b1cb-4e9ca76ac53f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3379990130>]}
20:05:41.684106 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
20:05:41.686904 [info ] [MainThread]: 
20:05:41.689724 [debug] [MainThread]: Acquiring new postgres connection "master"
20:05:41.694684 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
20:05:41.712068 [debug] [ThreadPool]: Using postgres connection "list_adludio"
20:05:41.712492 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
20:05:41.712825 [debug] [ThreadPool]: Opening a new connection, currently in state init
20:05:41.728318 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.02 seconds
20:05:41.730786 [debug] [ThreadPool]: On list_adludio: Close
20:05:41.732985 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
20:05:41.742418 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
20:05:41.742819 [debug] [ThreadPool]: On list_adludio_public: BEGIN
20:05:41.743099 [debug] [ThreadPool]: Opening a new connection, currently in state init
20:05:41.756720 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
20:05:41.757117 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
20:05:41.757362 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
20:05:41.760663 [debug] [ThreadPool]: SQL status: SELECT 17 in 0.0 seconds
20:05:41.763175 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
20:05:41.763580 [debug] [ThreadPool]: On list_adludio_public: Close
20:05:41.774756 [debug] [MainThread]: Using postgres connection "master"
20:05:41.775120 [debug] [MainThread]: On master: BEGIN
20:05:41.775410 [debug] [MainThread]: Opening a new connection, currently in state init
20:05:41.798699 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
20:05:41.799156 [debug] [MainThread]: Using postgres connection "master"
20:05:41.799448 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
20:05:41.961878 [debug] [MainThread]: SQL status: SELECT 8 in 0.16 seconds
20:05:41.964734 [debug] [MainThread]: On master: ROLLBACK
20:05:41.965209 [debug] [MainThread]: Using postgres connection "master"
20:05:41.965536 [debug] [MainThread]: On master: BEGIN
20:05:41.965998 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
20:05:41.966259 [debug] [MainThread]: On master: COMMIT
20:05:41.966498 [debug] [MainThread]: Using postgres connection "master"
20:05:41.966725 [debug] [MainThread]: On master: COMMIT
20:05:41.967059 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
20:05:41.967358 [debug] [MainThread]: On master: Close
20:05:41.967981 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
20:05:41.969100 [info ] [MainThread]: 
20:05:41.977879 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
20:05:41.978417 [info ] [Thread-1  ]: 1 of 6 START table model public.transformed_sales_number_data................... [RUN]
20:05:41.979211 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:05:41.979489 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
20:05:41.979753 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
20:05:42.041669 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
20:05:42.042386 [debug] [Thread-1  ]: finished collecting timing info
20:05:42.042724 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
20:05:42.088269 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
20:05:42.089198 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:05:42.089537 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
20:05:42.089812 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
20:05:42.103346 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
20:05:42.103774 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:05:42.104106 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter

    from sales_table 

)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_stage,

    case when "Deal_Stage" = 'Meeting' then 1 else 0 end as deal_meeting_count,
    case when "Deal_Stage" = 'IO Sent' then 1 else 0 end as deal_iosent_count,
    case when "Deal_Stage" = 'RFP' then 1 else 0 end  as deal_rfp_count,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter, year,
CONCAT  (year, '_', week) AS year_week,
CONCAT  (year, '_', quarter) AS year_quarter
from source_data

ORDER BY year_week

 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
20:05:42.122997 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.02 seconds
20:05:42.133197 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:05:42.133621 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
20:05:42.137262 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
20:05:42.141105 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:05:42.141390 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
20:05:42.142084 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
20:05:42.170228 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
20:05:42.170675 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:05:42.170960 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
20:05:42.173728 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
20:05:42.181611 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
20:05:42.181920 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
20:05:42.184605 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
20:05:42.186605 [debug] [Thread-1  ]: finished collecting timing info
20:05:42.186911 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
20:05:42.188786 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a73a330-c36b-4466-b1cb-4e9ca76ac53f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f337bc7b790>]}
20:05:42.189673 [info ] [Thread-1  ]: 1 of 6 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.21s]
20:05:42.190272 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
20:05:42.191771 [debug] [Thread-1  ]: Began running node model.Analytics_dbt. meeting_per_week
20:05:42.192187 [info ] [Thread-1  ]: 2 of 6 START table model public.meeting_per_week................................ [RUN]
20:05:42.193176 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt. meeting_per_week"
20:05:42.193504 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt. meeting_per_week
20:05:42.193802 [debug] [Thread-1  ]: Compiling model.Analytics_dbt. meeting_per_week
20:05:42.198334 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt. meeting_per_week"
20:05:42.198905 [debug] [Thread-1  ]: finished collecting timing info
20:05:42.199223 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt. meeting_per_week
20:05:42.203989 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt. meeting_per_week"
20:05:42.204516 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt. meeting_per_week"
20:05:42.204792 [debug] [Thread-1  ]: On model.Analytics_dbt. meeting_per_week: BEGIN
20:05:42.205011 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
20:05:42.221302 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
20:05:42.221721 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt. meeting_per_week"
20:05:42.221973 [debug] [Thread-1  ]: On model.Analytics_dbt. meeting_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt. meeting_per_week"} */


  create  table "adludio"."public"." meeting_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_meeting_count)*7 as avg_meeting_count, year_quarter
from source_data
group by year_quarter
ORDER BY year_quarter

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
20:05:42.227838 [debug] [Thread-1  ]: SQL status: SELECT 8 in 0.01 seconds
20:05:42.231620 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt. meeting_per_week"
20:05:42.231929 [debug] [Thread-1  ]: On model.Analytics_dbt. meeting_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt. meeting_per_week"} */
alter table "adludio"."public"." meeting_per_week__dbt_tmp" rename to "meeting_per_week"
20:05:42.232493 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
20:05:42.235902 [debug] [Thread-1  ]: On model.Analytics_dbt. meeting_per_week: COMMIT
20:05:42.236167 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt. meeting_per_week"
20:05:42.236387 [debug] [Thread-1  ]: On model.Analytics_dbt. meeting_per_week: COMMIT
20:05:42.244277 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
20:05:42.247041 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt. meeting_per_week"
20:05:42.247342 [debug] [Thread-1  ]: On model.Analytics_dbt. meeting_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt. meeting_per_week"} */
drop table if exists "adludio"."public"." meeting_per_week__dbt_backup" cascade
20:05:42.247841 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
20:05:42.249970 [debug] [Thread-1  ]: finished collecting timing info
20:05:42.250265 [debug] [Thread-1  ]: On model.Analytics_dbt. meeting_per_week: Close
20:05:42.250879 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a73a330-c36b-4466-b1cb-4e9ca76ac53f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3378035d00>]}
20:05:42.251414 [info ] [Thread-1  ]: 2 of 6 OK created table model public.meeting_per_week........................... [[32mSELECT 8[0m in 0.06s]
20:05:42.252137 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt. meeting_per_week
20:05:42.252441 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
20:05:42.252902 [info ] [Thread-1  ]: 3 of 6 START table model public.deal_value_per_week............................. [RUN]
20:05:42.253761 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
20:05:42.254014 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
20:05:42.254251 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
20:05:42.265643 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
20:05:42.266225 [debug] [Thread-1  ]: finished collecting timing info
20:05:42.266509 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
20:05:42.271374 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
20:05:42.271936 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
20:05:42.272198 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
20:05:42.272421 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
20:05:42.286611 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
20:05:42.286981 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
20:05:42.287223 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value)*7 as avg_deal_value, year_quarter
from source_data
group by year_quarter
ORDER BY year_quarter 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
20:05:42.293535 [debug] [Thread-1  ]: SQL status: SELECT 8 in 0.01 seconds
20:05:42.299760 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
20:05:42.300068 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
20:05:42.300704 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
20:05:42.304246 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
20:05:42.304515 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
20:05:42.305093 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
20:05:42.308712 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
20:05:42.309015 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
20:05:42.309284 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
20:05:42.313814 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
20:05:42.316623 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
20:05:42.316902 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
20:05:42.320574 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
20:05:42.322473 [debug] [Thread-1  ]: finished collecting timing info
20:05:42.322783 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
20:05:42.323415 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a73a330-c36b-4466-b1cb-4e9ca76ac53f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3378041ca0>]}
20:05:42.323942 [info ] [Thread-1  ]: 3 of 6 OK created table model public.deal_value_per_week........................ [[32mSELECT 8[0m in 0.07s]
20:05:42.324759 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
20:05:42.325066 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
20:05:42.325563 [info ] [Thread-1  ]: 4 of 6 START table model public.email_per_week.................................. [RUN]
20:05:42.330090 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
20:05:42.330416 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
20:05:42.330781 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
20:05:42.338179 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
20:05:42.338713 [debug] [Thread-1  ]: finished collecting timing info
20:05:42.338982 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
20:05:42.351020 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
20:05:42.351640 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
20:05:42.351910 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
20:05:42.352127 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
20:05:42.362584 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
20:05:42.363009 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
20:05:42.363289 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_email_messages_count)*7 as avg_email_count, year_quarter
from source_data
group by year_quarter
ORDER BY year_quarter

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
20:05:42.372017 [debug] [Thread-1  ]: SQL status: SELECT 8 in 0.01 seconds
20:05:42.376277 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
20:05:42.376574 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week" rename to "email_per_week__dbt_backup"
20:05:42.377172 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
20:05:42.387487 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
20:05:42.387817 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
20:05:42.388431 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
20:05:42.391116 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
20:05:42.391385 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
20:05:42.391605 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
20:05:42.392783 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
20:05:42.395595 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
20:05:42.395856 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
20:05:42.398636 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
20:05:42.400440 [debug] [Thread-1  ]: finished collecting timing info
20:05:42.400744 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
20:05:42.401426 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a73a330-c36b-4466-b1cb-4e9ca76ac53f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f337b3cc5b0>]}
20:05:42.401990 [info ] [Thread-1  ]: 4 of 6 OK created table model public.email_per_week............................. [[32mSELECT 8[0m in 0.07s]
20:05:42.402788 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
20:05:42.403124 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.ios_per_week
20:05:42.403625 [info ] [Thread-1  ]: 5 of 6 START table model public.ios_per_week.................................... [RUN]
20:05:42.404738 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.ios_per_week"
20:05:42.404990 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.ios_per_week
20:05:42.405227 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.ios_per_week
20:05:42.409407 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.ios_per_week"
20:05:42.410078 [debug] [Thread-1  ]: finished collecting timing info
20:05:42.410348 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.ios_per_week
20:05:42.418525 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.ios_per_week"
20:05:42.419171 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.ios_per_week"
20:05:42.419486 [debug] [Thread-1  ]: On model.Analytics_dbt.ios_per_week: BEGIN
20:05:42.419735 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
20:05:42.431339 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
20:05:42.431710 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.ios_per_week"
20:05:42.432012 [debug] [Thread-1  ]: On model.Analytics_dbt.ios_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.ios_per_week"} */


  create  table "adludio"."public"."ios_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_iosent_count)*7 as avg_iosent_count, year_quarter
from source_data
group by year_quarter
ORDER BY year_quarter

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
20:05:42.438292 [debug] [Thread-1  ]: SQL status: SELECT 8 in 0.01 seconds
20:05:42.442461 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.ios_per_week"
20:05:42.442748 [debug] [Thread-1  ]: On model.Analytics_dbt.ios_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.ios_per_week"} */
alter table "adludio"."public"."ios_per_week__dbt_tmp" rename to "ios_per_week"
20:05:42.443376 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
20:05:42.445918 [debug] [Thread-1  ]: On model.Analytics_dbt.ios_per_week: COMMIT
20:05:42.446233 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.ios_per_week"
20:05:42.446465 [debug] [Thread-1  ]: On model.Analytics_dbt.ios_per_week: COMMIT
20:05:42.450275 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
20:05:42.453171 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.ios_per_week"
20:05:42.453538 [debug] [Thread-1  ]: On model.Analytics_dbt.ios_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.ios_per_week"} */
drop table if exists "adludio"."public"."ios_per_week__dbt_backup" cascade
20:05:42.454077 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
20:05:42.455907 [debug] [Thread-1  ]: finished collecting timing info
20:05:42.456197 [debug] [Thread-1  ]: On model.Analytics_dbt.ios_per_week: Close
20:05:42.456818 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a73a330-c36b-4466-b1cb-4e9ca76ac53f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3378febd60>]}
20:05:42.457349 [info ] [Thread-1  ]: 5 of 6 OK created table model public.ios_per_week............................... [[32mSELECT 8[0m in 0.05s]
20:05:42.458090 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.ios_per_week
20:05:42.458349 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.rfps_per_week
20:05:42.458752 [info ] [Thread-1  ]: 6 of 6 START table model public.rfps_per_week................................... [RUN]
20:05:42.459351 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.rfps_per_week"
20:05:42.459561 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.rfps_per_week
20:05:42.459757 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.rfps_per_week
20:05:42.463635 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.rfps_per_week"
20:05:42.467256 [debug] [Thread-1  ]: finished collecting timing info
20:05:42.467569 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.rfps_per_week
20:05:42.472474 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.rfps_per_week"
20:05:42.473013 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.rfps_per_week"
20:05:42.473270 [debug] [Thread-1  ]: On model.Analytics_dbt.rfps_per_week: BEGIN
20:05:42.473504 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
20:05:42.487666 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
20:05:42.488099 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.rfps_per_week"
20:05:42.488373 [debug] [Thread-1  ]: On model.Analytics_dbt.rfps_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.rfps_per_week"} */


  create  table "adludio"."public"."rfps_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_rfp_count)*7 as avg_rfps_count, year_quarter
from source_data
group by year_quarter
ORDER BY year_quarter

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
20:05:42.495178 [debug] [Thread-1  ]: SQL status: SELECT 8 in 0.01 seconds
20:05:42.501445 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.rfps_per_week"
20:05:42.501815 [debug] [Thread-1  ]: On model.Analytics_dbt.rfps_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.rfps_per_week"} */
alter table "adludio"."public"."rfps_per_week__dbt_tmp" rename to "rfps_per_week"
20:05:42.502440 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
20:05:42.509195 [debug] [Thread-1  ]: On model.Analytics_dbt.rfps_per_week: COMMIT
20:05:42.509526 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.rfps_per_week"
20:05:42.509770 [debug] [Thread-1  ]: On model.Analytics_dbt.rfps_per_week: COMMIT
20:05:42.513316 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
20:05:42.515869 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.rfps_per_week"
20:05:42.516138 [debug] [Thread-1  ]: On model.Analytics_dbt.rfps_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.rfps_per_week"} */
drop table if exists "adludio"."public"."rfps_per_week__dbt_backup" cascade
20:05:42.516622 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
20:05:42.518671 [debug] [Thread-1  ]: finished collecting timing info
20:05:42.518970 [debug] [Thread-1  ]: On model.Analytics_dbt.rfps_per_week: Close
20:05:42.519635 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a73a330-c36b-4466-b1cb-4e9ca76ac53f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3362187dc0>]}
20:05:42.520157 [info ] [Thread-1  ]: 6 of 6 OK created table model public.rfps_per_week.............................. [[32mSELECT 8[0m in 0.06s]
20:05:42.521140 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.rfps_per_week
20:05:42.525164 [debug] [MainThread]: Acquiring new postgres connection "master"
20:05:42.525523 [debug] [MainThread]: Using postgres connection "master"
20:05:42.525772 [debug] [MainThread]: On master: BEGIN
20:05:42.526010 [debug] [MainThread]: Opening a new connection, currently in state closed
20:05:42.536740 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
20:05:42.537116 [debug] [MainThread]: On master: COMMIT
20:05:42.537359 [debug] [MainThread]: Using postgres connection "master"
20:05:42.537624 [debug] [MainThread]: On master: COMMIT
20:05:42.537964 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
20:05:42.538257 [debug] [MainThread]: On master: Close
20:05:42.538936 [info ] [MainThread]: 
20:05:42.539655 [info ] [MainThread]: Finished running 6 table models in 0.85s.
20:05:42.543589 [debug] [MainThread]: Connection 'master' was properly closed.
20:05:42.543872 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
20:05:42.544094 [debug] [MainThread]: Connection 'model.Analytics_dbt.rfps_per_week' was properly closed.
20:05:42.551989 [info ] [MainThread]: 
20:05:42.552475 [info ] [MainThread]: [32mCompleted successfully[0m
20:05:42.552976 [info ] [MainThread]: 
20:05:42.553411 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
20:05:42.553968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33621f42b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33621f98b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3378012e20>]}
2022-03-07 11:06:49.967215 (MainThread): Running with dbt=0.20.2
2022-03-07 11:06:50.446214 (MainThread): running dbt with arguments Namespace(record_timing_info=None, debug=False, log_format='default', write_json=True, use_colors=None, strict=False, warn_error=False, partial_parse=None, single_threaded=False, test_new_parser=False, use_experimental_parser=False, project_dir=None, profiles_dir='C:\\Users\\Abreham\\.dbt', profile=None, target=None, vars='{}', log_cache_events=False, use_cache=True, fail_fast=False, threads=None, version_check=True, models=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
2022-03-07 11:06:50.450215 (MainThread): Tracking: tracking
2022-03-07 11:06:50.520210 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014D2C01F280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014D3B9BDE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014D3B9BDD90>]}
2022-03-07 11:06:50.600213 (MainThread): Partial parsing not enabled
2022-03-07 11:06:51.389211 (MainThread): Parsing macros\adapters.sql
2022-03-07 11:06:51.542209 (MainThread): Parsing macros\catalog.sql
2022-03-07 11:06:51.552212 (MainThread): Parsing macros\relations.sql
2022-03-07 11:06:51.559212 (MainThread): Parsing macros\materializations\snapshot_merge.sql
2022-03-07 11:06:51.580223 (MainThread): Parsing macros\core.sql
2022-03-07 11:06:51.603212 (MainThread): Parsing macros\adapters\common.sql
2022-03-07 11:06:51.963218 (MainThread): Parsing macros\etc\datetime.sql
2022-03-07 11:06:52.003213 (MainThread): Parsing macros\etc\get_custom_alias.sql
2022-03-07 11:06:52.010215 (MainThread): Parsing macros\etc\get_custom_database.sql
2022-03-07 11:06:52.019215 (MainThread): Parsing macros\etc\get_custom_schema.sql
2022-03-07 11:06:52.036225 (MainThread): Parsing macros\etc\is_incremental.sql
2022-03-07 11:06:52.054216 (MainThread): Parsing macros\etc\query.sql
2022-03-07 11:06:52.059215 (MainThread): Parsing macros\materializations\helpers.sql
2022-03-07 11:06:52.128217 (MainThread): Parsing macros\materializations\test.sql
2022-03-07 11:06:52.160212 (MainThread): Parsing macros\materializations\common\merge.sql
2022-03-07 11:06:52.242210 (MainThread): Parsing macros\materializations\incremental\helpers.sql
2022-03-07 11:06:52.252213 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2022-03-07 11:06:52.309215 (MainThread): Parsing macros\materializations\seed\seed.sql
2022-03-07 11:06:52.441219 (MainThread): Parsing macros\materializations\snapshot\snapshot.sql
2022-03-07 11:06:52.615216 (MainThread): Parsing macros\materializations\snapshot\snapshot_merge.sql
2022-03-07 11:06:52.626212 (MainThread): Parsing macros\materializations\snapshot\strategies.sql
2022-03-07 11:06:52.753210 (MainThread): Parsing macros\materializations\table\table.sql
2022-03-07 11:06:52.784208 (MainThread): Parsing macros\materializations\view\create_or_replace_view.sql
2022-03-07 11:06:52.830213 (MainThread): Parsing macros\materializations\view\view.sql
2022-03-07 11:06:52.859210 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2022-03-07 11:06:52.870215 (MainThread): Parsing macros\schema_tests\not_null.sql
2022-03-07 11:06:52.882240 (MainThread): Parsing macros\schema_tests\relationships.sql
2022-03-07 11:06:52.901213 (MainThread): Parsing macros\schema_tests\unique.sql
2022-03-07 11:06:53.923214 (MainThread): Acquiring new postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 11:06:53.988211 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 11:06:54.005215 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 11:06:54.035215 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 11:06:54.050216 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 11:06:54.070213 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:06:54.293217 (MainThread): [WARNING]: Test 'test.Analytics_dbt.unique_my_first_dbt_model_id.5ef3c774bb' (models\Sales Numbers\schema.yml) depends on a node named 'my_first_dbt_model' which was not found
2022-03-07 11:06:54.303220 (MainThread): [WARNING]: Test 'test.Analytics_dbt.not_null_my_first_dbt_model_id.f479667b44' (models\Sales Numbers\schema.yml) depends on a node named 'my_first_dbt_model' which was not found
2022-03-07 11:06:54.319217 (MainThread): [WARNING]: Test 'test.Analytics_dbt.unique_my_second_dbt_model_id.bc96125570' (models\Sales Numbers\schema.yml) depends on a node named 'my_second_dbt_model' which was not found
2022-03-07 11:06:54.332214 (MainThread): [WARNING]: Test 'test.Analytics_dbt.not_null_my_second_dbt_model_id.dc1d13aa6d' (models\Sales Numbers\schema.yml) depends on a node named 'my_second_dbt_model' which was not found
2022-03-07 11:06:54.494219 (MainThread): [WARNING]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

2022-03-07 11:06:54.605224 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dfe29ba5-616e-4edc-9cd8-bd79aa8afb19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014D3BC01F10>]}
2022-03-07 11:06:54.673217 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dfe29ba5-616e-4edc-9cd8-bd79aa8afb19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014D3BB33E50>]}
2022-03-07 11:06:54.674213 (MainThread): Found 6 models, 0 tests, 0 snapshots, 0 analyses, 147 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2022-03-07 11:06:54.689214 (MainThread): 
2022-03-07 11:06:54.691217 (MainThread): Acquiring new postgres connection "master".
2022-03-07 11:06:54.725219 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_adludio".
2022-03-07 11:06:54.819213 (ThreadPoolExecutor-0_0): Using postgres connection "list_adludio".
2022-03-07 11:06:54.820211 (ThreadPoolExecutor-0_0): On list_adludio: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
2022-03-07 11:06:54.820211 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-03-07 11:06:55.201219 (ThreadPoolExecutor-0_0): Got an error when attempting to open a postgres connection: 'FATAL:  database "adludio" does not exist
'
2022-03-07 11:06:55.202216 (ThreadPoolExecutor-0_0): Error running SQL: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
2022-03-07 11:06:55.202216 (ThreadPoolExecutor-0_0): Rolling back transaction.
2022-03-07 11:06:55.203216 (ThreadPoolExecutor-0_0): Error running SQL: macro list_schemas
2022-03-07 11:06:55.203216 (ThreadPoolExecutor-0_0): Rolling back transaction.
2022-03-07 11:06:55.204214 (ThreadPoolExecutor-0_0): On list_adludio: No close available on handle
2022-03-07 11:06:55.206216 (MainThread): Connection 'master' was properly closed.
2022-03-07 11:06:55.207216 (MainThread): Connection 'list_adludio' was properly closed.
2022-03-07 11:06:55.207216 (MainThread): ERROR: Database Error
  FATAL:  database "adludio" does not exist
  
2022-03-07 11:06:55.216217 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014D3BB03670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014D3BBD3FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014D3BB546A0>]}
2022-03-07 11:06:55.217216 (MainThread): Flushing usage events
2022-03-07 11:11:52.751213 (MainThread): Running with dbt=0.20.2
2022-03-07 11:11:53.296753 (MainThread): running dbt with arguments Namespace(record_timing_info=None, debug=False, log_format='default', write_json=True, use_colors=None, strict=False, warn_error=False, partial_parse=None, single_threaded=False, test_new_parser=False, use_experimental_parser=False, project_dir=None, profiles_dir='C:\\Users\\Abreham\\.dbt', profile=None, target=None, vars='{}', log_cache_events=False, use_cache=True, fail_fast=False, threads=None, version_check=True, models=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
2022-03-07 11:11:53.298753 (MainThread): Tracking: tracking
2022-03-07 11:11:53.343753 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B579AF2DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B579AFDE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B579AFDCA0>]}
2022-03-07 11:11:53.418325 (MainThread): Partial parsing not enabled
2022-03-07 11:11:54.108319 (MainThread): Parsing macros\adapters.sql
2022-03-07 11:11:54.242321 (MainThread): Parsing macros\catalog.sql
2022-03-07 11:11:54.252323 (MainThread): Parsing macros\relations.sql
2022-03-07 11:11:54.261330 (MainThread): Parsing macros\materializations\snapshot_merge.sql
2022-03-07 11:11:54.276329 (MainThread): Parsing macros\core.sql
2022-03-07 11:11:54.300322 (MainThread): Parsing macros\adapters\common.sql
2022-03-07 11:11:54.573325 (MainThread): Parsing macros\etc\datetime.sql
2022-03-07 11:11:54.618321 (MainThread): Parsing macros\etc\get_custom_alias.sql
2022-03-07 11:11:54.623325 (MainThread): Parsing macros\etc\get_custom_database.sql
2022-03-07 11:11:54.630321 (MainThread): Parsing macros\etc\get_custom_schema.sql
2022-03-07 11:11:54.637319 (MainThread): Parsing macros\etc\is_incremental.sql
2022-03-07 11:11:54.642323 (MainThread): Parsing macros\etc\query.sql
2022-03-07 11:11:54.649321 (MainThread): Parsing macros\materializations\helpers.sql
2022-03-07 11:11:54.690333 (MainThread): Parsing macros\materializations\test.sql
2022-03-07 11:11:54.725335 (MainThread): Parsing macros\materializations\common\merge.sql
2022-03-07 11:11:54.783327 (MainThread): Parsing macros\materializations\incremental\helpers.sql
2022-03-07 11:11:54.794327 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2022-03-07 11:11:54.830320 (MainThread): Parsing macros\materializations\seed\seed.sql
2022-03-07 11:11:54.932373 (MainThread): Parsing macros\materializations\snapshot\snapshot.sql
2022-03-07 11:11:55.126325 (MainThread): Parsing macros\materializations\snapshot\snapshot_merge.sql
2022-03-07 11:11:55.141324 (MainThread): Parsing macros\materializations\snapshot\strategies.sql
2022-03-07 11:11:55.238324 (MainThread): Parsing macros\materializations\table\table.sql
2022-03-07 11:11:55.279324 (MainThread): Parsing macros\materializations\view\create_or_replace_view.sql
2022-03-07 11:11:55.312324 (MainThread): Parsing macros\materializations\view\view.sql
2022-03-07 11:11:55.353324 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2022-03-07 11:11:55.379324 (MainThread): Parsing macros\schema_tests\not_null.sql
2022-03-07 11:11:55.385322 (MainThread): Parsing macros\schema_tests\relationships.sql
2022-03-07 11:11:55.394323 (MainThread): Parsing macros\schema_tests\unique.sql
2022-03-07 11:11:56.327325 (MainThread): Acquiring new postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 11:11:56.403870 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 11:11:56.436867 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 11:11:56.464869 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 11:11:56.504871 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 11:11:56.526871 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:11:56.763867 (MainThread): [WARNING]: Test 'test.Analytics_dbt.unique_my_first_dbt_model_id.5ef3c774bb' (models\Sales Numbers\schema.yml) depends on a node named 'my_first_dbt_model' which was not found
2022-03-07 11:11:56.771873 (MainThread): [WARNING]: Test 'test.Analytics_dbt.not_null_my_first_dbt_model_id.f479667b44' (models\Sales Numbers\schema.yml) depends on a node named 'my_first_dbt_model' which was not found
2022-03-07 11:11:56.781871 (MainThread): [WARNING]: Test 'test.Analytics_dbt.unique_my_second_dbt_model_id.bc96125570' (models\Sales Numbers\schema.yml) depends on a node named 'my_second_dbt_model' which was not found
2022-03-07 11:11:56.801873 (MainThread): [WARNING]: Test 'test.Analytics_dbt.not_null_my_second_dbt_model_id.dc1d13aa6d' (models\Sales Numbers\schema.yml) depends on a node named 'my_second_dbt_model' which was not found
2022-03-07 11:11:56.929874 (MainThread): [WARNING]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

2022-03-07 11:11:57.027867 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b2cb4825-eb0f-4bfe-8261-d70f068aec61', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B579C7A0D0>]}
2022-03-07 11:11:57.121871 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b2cb4825-eb0f-4bfe-8261-d70f068aec61', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B579C7A2E0>]}
2022-03-07 11:11:57.126883 (MainThread): Found 6 models, 0 tests, 0 snapshots, 0 analyses, 147 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2022-03-07 11:11:57.184875 (MainThread): 
2022-03-07 11:11:57.201868 (MainThread): Acquiring new postgres connection "master".
2022-03-07 11:11:57.266176 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_adludio".
2022-03-07 11:11:57.381697 (ThreadPoolExecutor-0_0): Using postgres connection "list_adludio".
2022-03-07 11:11:57.381697 (ThreadPoolExecutor-0_0): On list_adludio: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
2022-03-07 11:11:57.382703 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-03-07 11:11:57.671913 (ThreadPoolExecutor-0_0): Got an error when attempting to open a postgres connection: 'FATAL:  database "adludio" does not exist
'
2022-03-07 11:11:57.671913 (ThreadPoolExecutor-0_0): Error running SQL: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
2022-03-07 11:11:57.672915 (ThreadPoolExecutor-0_0): Rolling back transaction.
2022-03-07 11:11:57.672915 (ThreadPoolExecutor-0_0): Error running SQL: macro list_schemas
2022-03-07 11:11:57.673915 (ThreadPoolExecutor-0_0): Rolling back transaction.
2022-03-07 11:11:57.673915 (ThreadPoolExecutor-0_0): On list_adludio: No close available on handle
2022-03-07 11:11:57.675917 (MainThread): Connection 'master' was properly closed.
2022-03-07 11:11:57.676913 (MainThread): Connection 'list_adludio' was properly closed.
2022-03-07 11:11:57.676913 (MainThread): ERROR: Database Error
  FATAL:  database "adludio" does not exist
  
2022-03-07 11:11:57.689917 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B579BB9100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B579AFDE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B579D14550>]}
2022-03-07 11:11:57.690932 (MainThread): Flushing usage events
2022-03-07 11:18:20.077108 (MainThread): Running with dbt=0.20.2
2022-03-07 11:18:20.513097 (MainThread): running dbt with arguments Namespace(record_timing_info=None, debug=False, log_format='default', write_json=True, use_colors=None, strict=False, warn_error=False, partial_parse=None, single_threaded=False, test_new_parser=False, use_experimental_parser=False, project_dir=None, profiles_dir='C:\\Users\\Abreham\\.dbt', profile=None, target=None, vars='{}', log_cache_events=False, use_cache=True, fail_fast=False, threads=None, version_check=True, models=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
2022-03-07 11:18:20.518099 (MainThread): Tracking: tracking
2022-03-07 11:18:20.711100 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA6E1F280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BB67BDE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BB67BDD90>]}
2022-03-07 11:18:20.771104 (MainThread): Partial parsing not enabled
2022-03-07 11:18:20.930100 (MainThread): Parsing macros\adapters.sql
2022-03-07 11:18:21.045104 (MainThread): Parsing macros\catalog.sql
2022-03-07 11:18:21.054104 (MainThread): Parsing macros\relations.sql
2022-03-07 11:18:21.062097 (MainThread): Parsing macros\materializations\snapshot_merge.sql
2022-03-07 11:18:21.074111 (MainThread): Parsing macros\core.sql
2022-03-07 11:18:21.093109 (MainThread): Parsing macros\adapters\common.sql
2022-03-07 11:18:21.525115 (MainThread): Parsing macros\etc\datetime.sql
2022-03-07 11:18:21.636099 (MainThread): Parsing macros\etc\get_custom_alias.sql
2022-03-07 11:18:21.641100 (MainThread): Parsing macros\etc\get_custom_database.sql
2022-03-07 11:18:21.656097 (MainThread): Parsing macros\etc\get_custom_schema.sql
2022-03-07 11:18:21.706097 (MainThread): Parsing macros\etc\is_incremental.sql
2022-03-07 11:18:21.719100 (MainThread): Parsing macros\etc\query.sql
2022-03-07 11:18:21.725104 (MainThread): Parsing macros\materializations\helpers.sql
2022-03-07 11:18:21.833642 (MainThread): Parsing macros\materializations\test.sql
2022-03-07 11:18:21.896642 (MainThread): Parsing macros\materializations\common\merge.sql
2022-03-07 11:18:21.980642 (MainThread): Parsing macros\materializations\incremental\helpers.sql
2022-03-07 11:18:21.989647 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2022-03-07 11:18:22.032643 (MainThread): Parsing macros\materializations\seed\seed.sql
2022-03-07 11:18:22.129640 (MainThread): Parsing macros\materializations\snapshot\snapshot.sql
2022-03-07 11:18:22.340641 (MainThread): Parsing macros\materializations\snapshot\snapshot_merge.sql
2022-03-07 11:18:22.352643 (MainThread): Parsing macros\materializations\snapshot\strategies.sql
2022-03-07 11:18:22.586642 (MainThread): Parsing macros\materializations\table\table.sql
2022-03-07 11:18:22.657643 (MainThread): Parsing macros\materializations\view\create_or_replace_view.sql
2022-03-07 11:18:22.692668 (MainThread): Parsing macros\materializations\view\view.sql
2022-03-07 11:18:22.735326 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2022-03-07 11:18:22.748324 (MainThread): Parsing macros\schema_tests\not_null.sql
2022-03-07 11:18:22.754325 (MainThread): Parsing macros\schema_tests\relationships.sql
2022-03-07 11:18:22.769898 (MainThread): Parsing macros\schema_tests\unique.sql
2022-03-07 11:18:23.813878 (MainThread): Acquiring new postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 11:18:23.852876 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 11:18:23.871876 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 11:18:23.890873 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 11:18:23.905874 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 11:18:23.924875 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:18:24.094887 (MainThread): [WARNING]: Test 'test.Analytics_dbt.unique_my_first_dbt_model_id.5ef3c774bb' (models\Sales Numbers\schema.yml) depends on a node named 'my_first_dbt_model' which was not found
2022-03-07 11:18:24.101879 (MainThread): [WARNING]: Test 'test.Analytics_dbt.not_null_my_first_dbt_model_id.f479667b44' (models\Sales Numbers\schema.yml) depends on a node named 'my_first_dbt_model' which was not found
2022-03-07 11:18:24.106879 (MainThread): [WARNING]: Test 'test.Analytics_dbt.unique_my_second_dbt_model_id.bc96125570' (models\Sales Numbers\schema.yml) depends on a node named 'my_second_dbt_model' which was not found
2022-03-07 11:18:24.113878 (MainThread): [WARNING]: Test 'test.Analytics_dbt.not_null_my_second_dbt_model_id.dc1d13aa6d' (models\Sales Numbers\schema.yml) depends on a node named 'my_second_dbt_model' which was not found
2022-03-07 11:18:24.178877 (MainThread): [WARNING]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

2022-03-07 11:18:24.264882 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '39d565d4-6374-48d7-9a33-efc6f54d294e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BB6A01F10>]}
2022-03-07 11:18:24.378887 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '39d565d4-6374-48d7-9a33-efc6f54d294e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BB6936280>]}
2022-03-07 11:18:24.380883 (MainThread): Found 6 models, 0 tests, 0 snapshots, 0 analyses, 147 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2022-03-07 11:18:24.441880 (MainThread): 
2022-03-07 11:18:24.460883 (MainThread): Acquiring new postgres connection "master".
2022-03-07 11:18:24.548883 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_adludio".
2022-03-07 11:18:24.660882 (ThreadPoolExecutor-0_0): Using postgres connection "list_adludio".
2022-03-07 11:18:24.661883 (ThreadPoolExecutor-0_0): On list_adludio: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
2022-03-07 11:18:24.661883 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-03-07 11:18:25.058874 (ThreadPoolExecutor-0_0): SQL status: SELECT 4 in 0.40 seconds
2022-03-07 11:18:25.071873 (ThreadPoolExecutor-0_0): On list_adludio: Close
2022-03-07 11:18:25.113878 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_adludio_public".
2022-03-07 11:18:25.163879 (ThreadPoolExecutor-1_0): Using postgres connection "list_adludio_public".
2022-03-07 11:18:25.164882 (ThreadPoolExecutor-1_0): On list_adludio_public: BEGIN
2022-03-07 11:18:25.165881 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-03-07 11:18:25.452882 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.29 seconds
2022-03-07 11:18:25.452882 (ThreadPoolExecutor-1_0): Using postgres connection "list_adludio_public".
2022-03-07 11:18:25.453881 (ThreadPoolExecutor-1_0): On list_adludio_public: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
2022-03-07 11:18:25.495874 (ThreadPoolExecutor-1_0): SQL status: SELECT 1 in 0.04 seconds
2022-03-07 11:18:25.498876 (ThreadPoolExecutor-1_0): On list_adludio_public: ROLLBACK
2022-03-07 11:18:25.499888 (ThreadPoolExecutor-1_0): On list_adludio_public: Close
2022-03-07 11:18:25.522875 (MainThread): Using postgres connection "master".
2022-03-07 11:18:25.522875 (MainThread): On master: BEGIN
2022-03-07 11:18:25.523878 (MainThread): Opening a new connection, currently in state init
2022-03-07 11:18:25.717425 (MainThread): SQL status: BEGIN in 0.19 seconds
2022-03-07 11:18:25.718426 (MainThread): Using postgres connection "master".
2022-03-07 11:18:25.718426 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-03-07 11:18:25.777427 (MainThread): SQL status: SELECT 0 in 0.06 seconds
2022-03-07 11:18:25.779426 (MainThread): On master: ROLLBACK
2022-03-07 11:18:25.780427 (MainThread): Using postgres connection "master".
2022-03-07 11:18:25.780427 (MainThread): On master: BEGIN
2022-03-07 11:18:25.781439 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-03-07 11:18:25.782425 (MainThread): On master: COMMIT
2022-03-07 11:18:25.782425 (MainThread): Using postgres connection "master".
2022-03-07 11:18:25.783426 (MainThread): On master: COMMIT
2022-03-07 11:18:25.784428 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-03-07 11:18:25.785430 (MainThread): On master: Close
2022-03-07 11:18:25.786427 (MainThread): 08:18:25 | Concurrency: 1 threads (target='dev')
2022-03-07 11:18:25.791425 (MainThread): 08:18:25 | 
2022-03-07 11:18:25.947990 (Thread-1): Began running node model.Analytics_dbt.transformed_sales_number_data
2022-03-07 11:18:25.948989 (Thread-1): 08:18:25 | 1 of 6 START table model public.transformed_sales_number_data........ [RUN]
2022-03-07 11:18:25.952992 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:18:25.952992 (Thread-1): Compiling model.Analytics_dbt.transformed_sales_number_data
2022-03-07 11:18:25.965991 (Thread-1): Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
2022-03-07 11:18:25.970991 (Thread-1): finished collecting timing info
2022-03-07 11:18:26.298986 (Thread-1): Using postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:18:26.299986 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_tmp" cascade
2022-03-07 11:18:26.305000 (Thread-1): Opening a new connection, currently in state init
2022-03-07 11:18:26.678984 (Thread-1): SQL status: DROP TABLE in 0.37 seconds
2022-03-07 11:18:26.688984 (Thread-1): Using postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:18:26.688984 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
2022-03-07 11:18:26.689987 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:18:26.772983 (Thread-1): Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
2022-03-07 11:18:26.792986 (Thread-1): Using postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:18:26.793986 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: BEGIN
2022-03-07 11:18:26.794983 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-03-07 11:18:26.795985 (Thread-1): Using postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:18:26.796984 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter

    from sales_table 

)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_stage,

    case when "Deal_Stage" = 'Meeting' then 1 else 0 end as deal_meeting_count,
    case when "Deal_Stage" = 'IO Sent' then 1 else 0 end as deal_iosent_count,
    case when "Deal_Stage" = 'RFP' then 1 else 0 end  as deal_rfp_count,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter, year,
CONCAT  (year, '_', week) AS year_week,
CONCAT  (year, '_', quarter) AS year_quarter
from source_data

ORDER BY year_week

 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
2022-03-07 11:18:26.803989 (Thread-1): Postgres error: relation "sales_table" does not exist
LINE 22:     from sales_table 
                  ^

2022-03-07 11:18:26.804992 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: ROLLBACK
2022-03-07 11:18:26.804992 (Thread-1): finished collecting timing info
2022-03-07 11:18:26.805983 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: Close
2022-03-07 11:18:26.806983 (Thread-1): Database Error in model transformed_sales_number_data (models\Sales Numbers\transformed_sales_number_data.sql)
  relation "sales_table" does not exist
  LINE 22:     from sales_table 
                    ^
  compiled SQL at target\run\Analytics_dbt\models\Sales Numbers\transformed_sales_number_data.sql
Traceback (most recent call last):
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\adapters\postgres\connections.py", line 48, in exception_handler
    yield
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\adapters\sql\connections.py", line 80, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: relation "sales_table" does not exist
LINE 22:     from sales_table 
                  ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\task\base.py", line 348, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\task\base.py", line 291, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\task\base.py", line 393, in run
    return self.execute(compiled_node, manifest)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\task\run.py", line 249, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\clients\jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\clients\jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\jinja2\runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\jinja2\runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 64, in macro
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\jinja2\sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\jinja2\runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\clients\jinja.py", line 333, in __call__
    return self.call_macro(*args, **kwargs)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\clients\jinja.py", line 260, in call_macro
    return macro(*args, **kwargs)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\jinja2\runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\jinja2\runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\jinja2\sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\jinja2\runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\adapters\base\impl.py", line 226, in execute
    return self.connections.execute(
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\adapters\sql\connections.py", line 131, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\adapters\sql\connections.py", line 87, in add_query
    return connection, cursor
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\contextlib.py", line 135, in __exit__
    self.gen.throw(type, value, traceback)
  File "c:\users\abreham\appdata\local\programs\python\python39\lib\site-packages\dbt\adapters\postgres\connections.py", line 59, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model transformed_sales_number_data (models\Sales Numbers\transformed_sales_number_data.sql)
  relation "sales_table" does not exist
  LINE 22:     from sales_table 
                    ^
  compiled SQL at target\run\Analytics_dbt\models\Sales Numbers\transformed_sales_number_data.sql
2022-03-07 11:18:27.225985 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '39d565d4-6374-48d7-9a33-efc6f54d294e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BB6A103D0>]}
2022-03-07 11:18:27.229985 (Thread-1): 08:18:27 | 1 of 6 ERROR creating table model public.transformed_sales_number_data [ERROR in 1.27s]
2022-03-07 11:18:27.274986 (Thread-1): Finished running node model.Analytics_dbt.transformed_sales_number_data
2022-03-07 11:18:27.284989 (Thread-1): Began running node model.Analytics_dbt. meeting_per_week
2022-03-07 11:18:27.285988 (Thread-1): 08:18:27 | 2 of 6 SKIP relation public. meeting_per_week........................ [SKIP]
2022-03-07 11:18:27.290989 (Thread-1): Finished running node model.Analytics_dbt. meeting_per_week
2022-03-07 11:18:27.291988 (Thread-1): Began running node model.Analytics_dbt.deal_value_per_week
2022-03-07 11:18:27.292987 (Thread-1): 08:18:27 | 3 of 6 SKIP relation public.deal_value_per_week...................... [SKIP]
2022-03-07 11:18:27.294989 (Thread-1): Finished running node model.Analytics_dbt.deal_value_per_week
2022-03-07 11:18:27.295992 (Thread-1): Began running node model.Analytics_dbt.email_per_week
2022-03-07 11:18:27.296990 (Thread-1): 08:18:27 | 4 of 6 SKIP relation public.email_per_week........................... [SKIP]
2022-03-07 11:18:27.300000 (Thread-1): Finished running node model.Analytics_dbt.email_per_week
2022-03-07 11:18:27.301992 (Thread-1): Began running node model.Analytics_dbt.ios_per_week
2022-03-07 11:18:27.302992 (Thread-1): 08:18:27 | 5 of 6 SKIP relation public.ios_per_week............................. [SKIP]
2022-03-07 11:18:27.305985 (Thread-1): Finished running node model.Analytics_dbt.ios_per_week
2022-03-07 11:18:27.307988 (Thread-1): Began running node model.Analytics_dbt.rfps_per_week
2022-03-07 11:18:27.308988 (Thread-1): 08:18:27 | 6 of 6 SKIP relation public.rfps_per_week............................ [SKIP]
2022-03-07 11:18:27.311996 (Thread-1): Finished running node model.Analytics_dbt.rfps_per_week
2022-03-07 11:18:27.316987 (MainThread): Acquiring new postgres connection "master".
2022-03-07 11:18:27.317987 (MainThread): Using postgres connection "master".
2022-03-07 11:18:27.318989 (MainThread): On master: BEGIN
2022-03-07 11:18:27.318989 (MainThread): Opening a new connection, currently in state closed
2022-03-07 11:18:27.762478 (MainThread): SQL status: BEGIN in 0.44 seconds
2022-03-07 11:18:27.763479 (MainThread): On master: COMMIT
2022-03-07 11:18:27.763479 (MainThread): Using postgres connection "master".
2022-03-07 11:18:27.764480 (MainThread): On master: COMMIT
2022-03-07 11:18:27.764480 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-03-07 11:18:27.765478 (MainThread): On master: Close
2022-03-07 11:18:27.766478 (MainThread): 08:18:27 | 
2022-03-07 11:18:27.769481 (MainThread): 08:18:27 | Finished running 6 table models in 3.31s.
2022-03-07 11:18:27.770483 (MainThread): Connection 'master' was properly closed.
2022-03-07 11:18:27.771481 (MainThread): Connection 'list_adludio' was properly closed.
2022-03-07 11:18:27.771481 (MainThread): Connection 'list_adludio_public' was properly closed.
2022-03-07 11:18:27.771481 (MainThread): Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
2022-03-07 11:18:27.793484 (MainThread): 
2022-03-07 11:18:27.805481 (MainThread): Completed with 1 error and 0 warnings:
2022-03-07 11:18:27.814499 (MainThread): 
2022-03-07 11:18:27.825495 (MainThread): Database Error in model transformed_sales_number_data (models\Sales Numbers\transformed_sales_number_data.sql)
2022-03-07 11:18:27.879480 (MainThread):   relation "sales_table" does not exist
2022-03-07 11:18:27.914481 (MainThread):   LINE 22:     from sales_table 
2022-03-07 11:18:27.961482 (MainThread):                     ^
2022-03-07 11:18:27.975477 (MainThread):   compiled SQL at target\run\Analytics_dbt\models\Sales Numbers\transformed_sales_number_data.sql
2022-03-07 11:18:28.018482 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=5 TOTAL=6
2022-03-07 11:18:28.104479 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BB69506A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BB6950670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BB691FEE0>]}
2022-03-07 11:18:28.105477 (MainThread): Flushing usage events
2022-03-07 11:24:08.683692 (MainThread): Running with dbt=0.20.2
2022-03-07 11:24:08.967690 (MainThread): running dbt with arguments Namespace(record_timing_info=None, debug=False, log_format='default', write_json=True, use_colors=None, strict=False, warn_error=False, partial_parse=None, single_threaded=False, test_new_parser=False, use_experimental_parser=False, project_dir=None, profiles_dir='C:\\Users\\Abreham\\.dbt', profile=None, target=None, vars='{}', log_cache_events=False, use_cache=True, fail_fast=False, threads=None, version_check=True, models=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
2022-03-07 11:24:08.970687 (MainThread): Tracking: tracking
2022-03-07 11:24:08.996693 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C617DCF280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C62776DE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C62776DD90>]}
2022-03-07 11:24:09.029686 (MainThread): Partial parsing not enabled
2022-03-07 11:24:09.093530 (MainThread): Parsing macros\adapters.sql
2022-03-07 11:24:09.159560 (MainThread): Parsing macros\catalog.sql
2022-03-07 11:24:09.165531 (MainThread): Parsing macros\relations.sql
2022-03-07 11:24:09.168529 (MainThread): Parsing macros\materializations\snapshot_merge.sql
2022-03-07 11:24:09.172529 (MainThread): Parsing macros\core.sql
2022-03-07 11:24:09.189531 (MainThread): Parsing macros\adapters\common.sql
2022-03-07 11:24:09.408535 (MainThread): Parsing macros\etc\datetime.sql
2022-03-07 11:24:09.452531 (MainThread): Parsing macros\etc\get_custom_alias.sql
2022-03-07 11:24:09.455531 (MainThread): Parsing macros\etc\get_custom_database.sql
2022-03-07 11:24:09.461534 (MainThread): Parsing macros\etc\get_custom_schema.sql
2022-03-07 11:24:09.468534 (MainThread): Parsing macros\etc\is_incremental.sql
2022-03-07 11:24:09.475539 (MainThread): Parsing macros\etc\query.sql
2022-03-07 11:24:09.485533 (MainThread): Parsing macros\materializations\helpers.sql
2022-03-07 11:24:09.527531 (MainThread): Parsing macros\materializations\test.sql
2022-03-07 11:24:09.546534 (MainThread): Parsing macros\materializations\common\merge.sql
2022-03-07 11:24:09.590529 (MainThread): Parsing macros\materializations\incremental\helpers.sql
2022-03-07 11:24:09.595530 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2022-03-07 11:24:09.615534 (MainThread): Parsing macros\materializations\seed\seed.sql
2022-03-07 11:24:09.680539 (MainThread): Parsing macros\materializations\snapshot\snapshot.sql
2022-03-07 11:24:09.780534 (MainThread): Parsing macros\materializations\snapshot\snapshot_merge.sql
2022-03-07 11:24:09.784534 (MainThread): Parsing macros\materializations\snapshot\strategies.sql
2022-03-07 11:24:09.838541 (MainThread): Parsing macros\materializations\table\table.sql
2022-03-07 11:24:09.859528 (MainThread): Parsing macros\materializations\view\create_or_replace_view.sql
2022-03-07 11:24:09.873538 (MainThread): Parsing macros\materializations\view\view.sql
2022-03-07 11:24:09.893537 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2022-03-07 11:24:09.899530 (MainThread): Parsing macros\schema_tests\not_null.sql
2022-03-07 11:24:09.902534 (MainThread): Parsing macros\schema_tests\relationships.sql
2022-03-07 11:24:09.906532 (MainThread): Parsing macros\schema_tests\unique.sql
2022-03-07 11:24:10.559532 (MainThread): Acquiring new postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 11:24:10.631531 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 11:24:10.653530 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 11:24:10.672533 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 11:24:10.699531 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 11:24:10.713534 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:24:10.822538 (MainThread): [WARNING]: Test 'test.Analytics_dbt.unique_my_first_dbt_model_id.5ef3c774bb' (models\Sales Numbers\schema.yml) depends on a node named 'my_first_dbt_model' which was not found
2022-03-07 11:24:10.826535 (MainThread): [WARNING]: Test 'test.Analytics_dbt.not_null_my_first_dbt_model_id.f479667b44' (models\Sales Numbers\schema.yml) depends on a node named 'my_first_dbt_model' which was not found
2022-03-07 11:24:10.830534 (MainThread): [WARNING]: Test 'test.Analytics_dbt.unique_my_second_dbt_model_id.bc96125570' (models\Sales Numbers\schema.yml) depends on a node named 'my_second_dbt_model' which was not found
2022-03-07 11:24:10.835537 (MainThread): [WARNING]: Test 'test.Analytics_dbt.not_null_my_second_dbt_model_id.dc1d13aa6d' (models\Sales Numbers\schema.yml) depends on a node named 'my_second_dbt_model' which was not found
2022-03-07 11:24:10.949539 (MainThread): [WARNING]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

2022-03-07 11:24:10.980576 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '530c8bb1-3087-4481-baf4-b447a0d53c0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6279B1F10>]}
2022-03-07 11:24:11.032542 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '530c8bb1-3087-4481-baf4-b447a0d53c0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6278E5D00>]}
2022-03-07 11:24:11.035551 (MainThread): Found 6 models, 0 tests, 0 snapshots, 0 analyses, 147 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2022-03-07 11:24:11.053543 (MainThread): 
2022-03-07 11:24:11.055544 (MainThread): Acquiring new postgres connection "master".
2022-03-07 11:24:11.070541 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_adludio".
2022-03-07 11:24:11.185538 (ThreadPoolExecutor-0_0): Using postgres connection "list_adludio".
2022-03-07 11:24:11.187540 (ThreadPoolExecutor-0_0): On list_adludio: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
2022-03-07 11:24:11.188544 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-03-07 11:24:11.408627 (ThreadPoolExecutor-0_0): SQL status: SELECT 4 in 0.22 seconds
2022-03-07 11:24:11.424627 (ThreadPoolExecutor-0_0): On list_adludio: Close
2022-03-07 11:24:11.434625 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_adludio_public".
2022-03-07 11:24:11.464629 (ThreadPoolExecutor-1_0): Using postgres connection "list_adludio_public".
2022-03-07 11:24:11.465628 (ThreadPoolExecutor-1_0): On list_adludio_public: BEGIN
2022-03-07 11:24:11.466203 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-03-07 11:24:11.597108 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.13 seconds
2022-03-07 11:24:11.598114 (ThreadPoolExecutor-1_0): Using postgres connection "list_adludio_public".
2022-03-07 11:24:11.598114 (ThreadPoolExecutor-1_0): On list_adludio_public: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
2022-03-07 11:24:11.625113 (ThreadPoolExecutor-1_0): SQL status: SELECT 1 in 0.03 seconds
2022-03-07 11:24:11.627112 (ThreadPoolExecutor-1_0): On list_adludio_public: ROLLBACK
2022-03-07 11:24:11.628109 (ThreadPoolExecutor-1_0): On list_adludio_public: Close
2022-03-07 11:24:11.643109 (MainThread): Using postgres connection "master".
2022-03-07 11:24:11.643109 (MainThread): On master: BEGIN
2022-03-07 11:24:11.643109 (MainThread): Opening a new connection, currently in state init
2022-03-07 11:24:11.747108 (MainThread): SQL status: BEGIN in 0.10 seconds
2022-03-07 11:24:11.747108 (MainThread): Using postgres connection "master".
2022-03-07 11:24:11.748111 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-03-07 11:24:11.775932 (MainThread): SQL status: SELECT 0 in 0.03 seconds
2022-03-07 11:24:11.776505 (MainThread): On master: ROLLBACK
2022-03-07 11:24:11.776505 (MainThread): Using postgres connection "master".
2022-03-07 11:24:11.776505 (MainThread): On master: BEGIN
2022-03-07 11:24:11.777476 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-03-07 11:24:11.777476 (MainThread): On master: COMMIT
2022-03-07 11:24:11.778476 (MainThread): Using postgres connection "master".
2022-03-07 11:24:11.778476 (MainThread): On master: COMMIT
2022-03-07 11:24:11.778476 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-03-07 11:24:11.778476 (MainThread): On master: Close
2022-03-07 11:24:11.779476 (MainThread): 08:24:11 | Concurrency: 1 threads (target='dev')
2022-03-07 11:24:11.781478 (MainThread): 08:24:11 | 
2022-03-07 11:24:11.792487 (Thread-1): Began running node model.Analytics_dbt.transformed_sales_number_data
2022-03-07 11:24:11.793480 (Thread-1): 08:24:11 | 1 of 6 START table model public.transformed_sales_number_data........ [RUN]
2022-03-07 11:24:11.795480 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:24:11.796482 (Thread-1): Compiling model.Analytics_dbt.transformed_sales_number_data
2022-03-07 11:24:11.812485 (Thread-1): Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
2022-03-07 11:24:11.822485 (Thread-1): finished collecting timing info
2022-03-07 11:24:11.942479 (Thread-1): Using postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:24:11.943479 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_tmp" cascade
2022-03-07 11:24:11.943479 (Thread-1): Opening a new connection, currently in state init
2022-03-07 11:24:12.075479 (Thread-1): SQL status: DROP TABLE in 0.13 seconds
2022-03-07 11:24:12.081481 (Thread-1): Using postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:24:12.081481 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
2022-03-07 11:24:12.082477 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:24:12.110478 (Thread-1): Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
2022-03-07 11:24:12.112478 (Thread-1): Using postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:24:12.113480 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: BEGIN
2022-03-07 11:24:12.113480 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-03-07 11:24:12.114481 (Thread-1): Using postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:24:12.114481 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week,
    DATE_PART('year', to_date("Deal_created_at", 'DD/MM/YYYY')) as year,
    EXTRACT (QUARTER FROM to_date("Deal_created_at", 'DD/MM/YYYY')) as quarter

    from sales_table 

)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_stage,

    case when "Deal_Stage" = 'Meeting' then 1 else 0 end as deal_meeting_count,
    case when "Deal_Stage" = 'IO Sent' then 1 else 0 end as deal_iosent_count,
    case when "Deal_Stage" = 'RFP' then 1 else 0 end  as deal_rfp_count,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week,quarter, year,
CONCAT  (year, '_', week) AS year_week,
CONCAT  (year, '_', quarter) AS year_quarter
from source_data

ORDER BY year_week

 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
2022-03-07 11:24:12.286482 (Thread-1): SQL status: SELECT 2032 in 0.17 seconds
2022-03-07 11:24:12.308481 (Thread-1): Using postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:24:12.309481 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
2022-03-07 11:24:12.312480 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-03-07 11:24:12.362478 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: COMMIT
2022-03-07 11:24:12.362478 (Thread-1): Using postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:24:12.363479 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: COMMIT
2022-03-07 11:24:12.367478 (Thread-1): SQL status: COMMIT in 0.00 seconds
2022-03-07 11:24:12.380480 (Thread-1): Using postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 11:24:12.381480 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
2022-03-07 11:24:12.382476 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:24:12.385478 (Thread-1): finished collecting timing info
2022-03-07 11:24:12.386480 (Thread-1): On model.Analytics_dbt.transformed_sales_number_data: Close
2022-03-07 11:24:12.387479 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '530c8bb1-3087-4481-baf4-b447a0d53c0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6278B95E0>]}
2022-03-07 11:24:12.389487 (Thread-1): 08:24:12 | 1 of 6 OK created table model public.transformed_sales_number_data... [SELECT 2032 in 0.59s]
2022-03-07 11:24:12.391484 (Thread-1): Finished running node model.Analytics_dbt.transformed_sales_number_data
2022-03-07 11:24:12.393483 (Thread-1): Began running node model.Analytics_dbt. meeting_per_week
2022-03-07 11:24:12.394483 (Thread-1): 08:24:12 | 2 of 6 START table model public.meeting_per_week..................... [RUN]
2022-03-07 11:24:12.398484 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 11:24:12.398484 (Thread-1): Compiling model.Analytics_dbt. meeting_per_week
2022-03-07 11:24:12.412483 (Thread-1): Writing injected SQL for node "model.Analytics_dbt. meeting_per_week"
2022-03-07 11:24:12.414479 (Thread-1): finished collecting timing info
2022-03-07 11:24:12.437483 (Thread-1): Using postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 11:24:12.437483 (Thread-1): On model.Analytics_dbt. meeting_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt. meeting_per_week"} */
drop table if exists "adludio"."public"." meeting_per_week__dbt_tmp" cascade
2022-03-07 11:24:12.438484 (Thread-1): Opening a new connection, currently in state closed
2022-03-07 11:24:12.664226 (Thread-1): SQL status: DROP TABLE in 0.23 seconds
2022-03-07 11:24:12.673218 (Thread-1): Using postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 11:24:12.673218 (Thread-1): On model.Analytics_dbt. meeting_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt. meeting_per_week"} */
drop table if exists "adludio"."public"." meeting_per_week__dbt_backup" cascade
2022-03-07 11:24:12.675221 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:24:12.681218 (Thread-1): Writing runtime SQL for node "model.Analytics_dbt. meeting_per_week"
2022-03-07 11:24:12.683220 (Thread-1): Using postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 11:24:12.684218 (Thread-1): On model.Analytics_dbt. meeting_per_week: BEGIN
2022-03-07 11:24:12.685220 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-03-07 11:24:12.685220 (Thread-1): Using postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 11:24:12.686220 (Thread-1): On model.Analytics_dbt. meeting_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt. meeting_per_week"} */


  create  table "adludio"."public"." meeting_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_meeting_count)*7 as avg_meeting_count, year_quarter
from source_data
group by year_quarter
ORDER BY year_quarter

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
2022-03-07 11:24:12.728218 (Thread-1): SQL status: SELECT 8 in 0.04 seconds
2022-03-07 11:24:12.734216 (Thread-1): Using postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 11:24:12.734216 (Thread-1): On model.Analytics_dbt. meeting_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt. meeting_per_week"} */
alter table "adludio"."public"." meeting_per_week__dbt_tmp" rename to "meeting_per_week"
2022-03-07 11:24:12.736219 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-03-07 11:24:12.741219 (Thread-1): On model.Analytics_dbt. meeting_per_week: COMMIT
2022-03-07 11:24:12.741219 (Thread-1): Using postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 11:24:12.741219 (Thread-1): On model.Analytics_dbt. meeting_per_week: COMMIT
2022-03-07 11:24:12.744218 (Thread-1): SQL status: COMMIT in 0.00 seconds
2022-03-07 11:24:12.751220 (Thread-1): Using postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 11:24:12.751220 (Thread-1): On model.Analytics_dbt. meeting_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt. meeting_per_week"} */
drop table if exists "adludio"."public"." meeting_per_week__dbt_backup" cascade
2022-03-07 11:24:12.752218 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:24:12.757218 (Thread-1): finished collecting timing info
2022-03-07 11:24:12.757218 (Thread-1): On model.Analytics_dbt. meeting_per_week: Close
2022-03-07 11:24:12.759220 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '530c8bb1-3087-4481-baf4-b447a0d53c0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6278B95E0>]}
2022-03-07 11:24:12.760220 (Thread-1): 08:24:12 | 2 of 6 OK created table model public.meeting_per_week................ [SELECT 8 in 0.36s]
2022-03-07 11:24:12.763222 (Thread-1): Finished running node model.Analytics_dbt. meeting_per_week
2022-03-07 11:24:12.763222 (Thread-1): Began running node model.Analytics_dbt.deal_value_per_week
2022-03-07 11:24:12.764220 (Thread-1): 08:24:12 | 3 of 6 START table model public.deal_value_per_week.................. [RUN]
2022-03-07 11:24:12.767219 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 11:24:12.768222 (Thread-1): Compiling model.Analytics_dbt.deal_value_per_week
2022-03-07 11:24:12.787221 (Thread-1): Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
2022-03-07 11:24:12.790222 (Thread-1): finished collecting timing info
2022-03-07 11:24:12.810221 (Thread-1): Using postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 11:24:12.811220 (Thread-1): On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_tmp" cascade
2022-03-07 11:24:12.811220 (Thread-1): Opening a new connection, currently in state closed
2022-03-07 11:24:12.975217 (Thread-1): SQL status: DROP TABLE in 0.16 seconds
2022-03-07 11:24:12.983215 (Thread-1): Using postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 11:24:12.983215 (Thread-1): On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
2022-03-07 11:24:12.984218 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:24:12.989224 (Thread-1): Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
2022-03-07 11:24:12.991220 (Thread-1): Using postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 11:24:12.991220 (Thread-1): On model.Analytics_dbt.deal_value_per_week: BEGIN
2022-03-07 11:24:12.992220 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-03-07 11:24:12.993221 (Thread-1): Using postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 11:24:12.993221 (Thread-1): On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value)*7 as avg_deal_value, year_quarter
from source_data
group by year_quarter
ORDER BY year_quarter 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
2022-03-07 11:24:13.028222 (Thread-1): SQL status: SELECT 8 in 0.03 seconds
2022-03-07 11:24:13.038220 (Thread-1): Using postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 11:24:13.038220 (Thread-1): On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
2022-03-07 11:24:13.040220 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-03-07 11:24:13.045220 (Thread-1): On model.Analytics_dbt.deal_value_per_week: COMMIT
2022-03-07 11:24:13.046220 (Thread-1): Using postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 11:24:13.047220 (Thread-1): On model.Analytics_dbt.deal_value_per_week: COMMIT
2022-03-07 11:24:13.049220 (Thread-1): SQL status: COMMIT in 0.00 seconds
2022-03-07 11:24:13.055218 (Thread-1): Using postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 11:24:13.055218 (Thread-1): On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
2022-03-07 11:24:13.056216 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:24:13.061221 (Thread-1): finished collecting timing info
2022-03-07 11:24:13.061221 (Thread-1): On model.Analytics_dbt.deal_value_per_week: Close
2022-03-07 11:24:13.063221 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '530c8bb1-3087-4481-baf4-b447a0d53c0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C627A761C0>]}
2022-03-07 11:24:13.064218 (Thread-1): 08:24:13 | 3 of 6 OK created table model public.deal_value_per_week............. [SELECT 8 in 0.30s]
2022-03-07 11:24:13.071220 (Thread-1): Finished running node model.Analytics_dbt.deal_value_per_week
2022-03-07 11:24:13.073221 (Thread-1): Began running node model.Analytics_dbt.email_per_week
2022-03-07 11:24:13.074221 (Thread-1): 08:24:13 | 4 of 6 START table model public.email_per_week....................... [RUN]
2022-03-07 11:24:13.083219 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 11:24:13.083219 (Thread-1): Compiling model.Analytics_dbt.email_per_week
2022-03-07 11:24:13.100223 (Thread-1): Writing injected SQL for node "model.Analytics_dbt.email_per_week"
2022-03-07 11:24:13.102222 (Thread-1): finished collecting timing info
2022-03-07 11:24:13.131222 (Thread-1): Using postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 11:24:13.138227 (Thread-1): On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_tmp" cascade
2022-03-07 11:24:13.138227 (Thread-1): Opening a new connection, currently in state closed
2022-03-07 11:24:13.332219 (Thread-1): SQL status: DROP TABLE in 0.19 seconds
2022-03-07 11:24:13.340220 (Thread-1): Using postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 11:24:13.341221 (Thread-1): On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
2022-03-07 11:24:13.342220 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:24:13.347221 (Thread-1): Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
2022-03-07 11:24:13.350217 (Thread-1): Using postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 11:24:13.350217 (Thread-1): On model.Analytics_dbt.email_per_week: BEGIN
2022-03-07 11:24:13.351222 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-03-07 11:24:13.352220 (Thread-1): Using postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 11:24:13.352220 (Thread-1): On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_email_messages_count)*7 as avg_email_count, year_quarter
from source_data
group by year_quarter
ORDER BY year_quarter

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
2022-03-07 11:24:13.391220 (Thread-1): SQL status: SELECT 8 in 0.04 seconds
2022-03-07 11:24:13.399232 (Thread-1): Using postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 11:24:13.400230 (Thread-1): On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
2022-03-07 11:24:13.402221 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-03-07 11:24:13.408223 (Thread-1): On model.Analytics_dbt.email_per_week: COMMIT
2022-03-07 11:24:13.408223 (Thread-1): Using postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 11:24:13.409220 (Thread-1): On model.Analytics_dbt.email_per_week: COMMIT
2022-03-07 11:24:13.411222 (Thread-1): SQL status: COMMIT in 0.00 seconds
2022-03-07 11:24:13.417220 (Thread-1): Using postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 11:24:13.417220 (Thread-1): On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
2022-03-07 11:24:13.418220 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:24:13.422220 (Thread-1): finished collecting timing info
2022-03-07 11:24:13.423216 (Thread-1): On model.Analytics_dbt.email_per_week: Close
2022-03-07 11:24:13.425218 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '530c8bb1-3087-4481-baf4-b447a0d53c0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C627A09FA0>]}
2022-03-07 11:24:13.426221 (Thread-1): 08:24:13 | 4 of 6 OK created table model public.email_per_week.................. [SELECT 8 in 0.34s]
2022-03-07 11:24:13.428220 (Thread-1): Finished running node model.Analytics_dbt.email_per_week
2022-03-07 11:24:13.429223 (Thread-1): Began running node model.Analytics_dbt.ios_per_week
2022-03-07 11:24:13.430223 (Thread-1): 08:24:13 | 5 of 6 START table model public.ios_per_week......................... [RUN]
2022-03-07 11:24:13.432221 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 11:24:13.433227 (Thread-1): Compiling model.Analytics_dbt.ios_per_week
2022-03-07 11:24:13.445223 (Thread-1): Writing injected SQL for node "model.Analytics_dbt.ios_per_week"
2022-03-07 11:24:13.450258 (Thread-1): finished collecting timing info
2022-03-07 11:24:13.462217 (Thread-1): Using postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 11:24:13.479220 (Thread-1): On model.Analytics_dbt.ios_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.ios_per_week"} */
drop table if exists "adludio"."public"."ios_per_week__dbt_tmp" cascade
2022-03-07 11:24:13.485272 (Thread-1): Opening a new connection, currently in state closed
2022-03-07 11:24:13.678820 (Thread-1): SQL status: DROP TABLE in 0.19 seconds
2022-03-07 11:24:13.686819 (Thread-1): Using postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 11:24:13.687819 (Thread-1): On model.Analytics_dbt.ios_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.ios_per_week"} */
drop table if exists "adludio"."public"."ios_per_week__dbt_backup" cascade
2022-03-07 11:24:13.688819 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:24:13.694817 (Thread-1): Writing runtime SQL for node "model.Analytics_dbt.ios_per_week"
2022-03-07 11:24:13.697820 (Thread-1): Using postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 11:24:13.698821 (Thread-1): On model.Analytics_dbt.ios_per_week: BEGIN
2022-03-07 11:24:13.699818 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-03-07 11:24:13.699818 (Thread-1): Using postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 11:24:13.700822 (Thread-1): On model.Analytics_dbt.ios_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.ios_per_week"} */


  create  table "adludio"."public"."ios_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_iosent_count)*7 as avg_iosent_count, year_quarter
from source_data
group by year_quarter
ORDER BY year_quarter

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
2022-03-07 11:24:13.736820 (Thread-1): SQL status: SELECT 8 in 0.04 seconds
2022-03-07 11:24:13.744818 (Thread-1): Using postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 11:24:13.744818 (Thread-1): On model.Analytics_dbt.ios_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.ios_per_week"} */
alter table "adludio"."public"."ios_per_week__dbt_tmp" rename to "ios_per_week"
2022-03-07 11:24:13.746818 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-03-07 11:24:13.750817 (Thread-1): On model.Analytics_dbt.ios_per_week: COMMIT
2022-03-07 11:24:13.750817 (Thread-1): Using postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 11:24:13.750817 (Thread-1): On model.Analytics_dbt.ios_per_week: COMMIT
2022-03-07 11:24:13.753828 (Thread-1): SQL status: COMMIT in 0.00 seconds
2022-03-07 11:24:13.763822 (Thread-1): Using postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 11:24:13.764821 (Thread-1): On model.Analytics_dbt.ios_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.ios_per_week"} */
drop table if exists "adludio"."public"."ios_per_week__dbt_backup" cascade
2022-03-07 11:24:13.765822 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:24:13.769814 (Thread-1): finished collecting timing info
2022-03-07 11:24:13.770821 (Thread-1): On model.Analytics_dbt.ios_per_week: Close
2022-03-07 11:24:13.771820 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '530c8bb1-3087-4481-baf4-b447a0d53c0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C627A79B50>]}
2022-03-07 11:24:13.773820 (Thread-1): 08:24:13 | 5 of 6 OK created table model public.ios_per_week.................... [SELECT 8 in 0.34s]
2022-03-07 11:24:13.776818 (Thread-1): Finished running node model.Analytics_dbt.ios_per_week
2022-03-07 11:24:13.777820 (Thread-1): Began running node model.Analytics_dbt.rfps_per_week
2022-03-07 11:24:13.778822 (Thread-1): 08:24:13 | 6 of 6 START table model public.rfps_per_week........................ [RUN]
2022-03-07 11:24:13.781817 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 11:24:13.782822 (Thread-1): Compiling model.Analytics_dbt.rfps_per_week
2022-03-07 11:24:13.795819 (Thread-1): Writing injected SQL for node "model.Analytics_dbt.rfps_per_week"
2022-03-07 11:24:13.800821 (Thread-1): finished collecting timing info
2022-03-07 11:24:13.811819 (Thread-1): Using postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 11:24:13.811819 (Thread-1): On model.Analytics_dbt.rfps_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.rfps_per_week"} */
drop table if exists "adludio"."public"."rfps_per_week__dbt_tmp" cascade
2022-03-07 11:24:13.812818 (Thread-1): Opening a new connection, currently in state closed
2022-03-07 11:24:13.992817 (Thread-1): SQL status: DROP TABLE in 0.18 seconds
2022-03-07 11:24:14.000818 (Thread-1): Using postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 11:24:14.000818 (Thread-1): On model.Analytics_dbt.rfps_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.rfps_per_week"} */
drop table if exists "adludio"."public"."rfps_per_week__dbt_backup" cascade
2022-03-07 11:24:14.001820 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:24:14.007817 (Thread-1): Writing runtime SQL for node "model.Analytics_dbt.rfps_per_week"
2022-03-07 11:24:14.009818 (Thread-1): Using postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 11:24:14.010821 (Thread-1): On model.Analytics_dbt.rfps_per_week: BEGIN
2022-03-07 11:24:14.011818 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-03-07 11:24:14.011818 (Thread-1): Using postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 11:24:14.011818 (Thread-1): On model.Analytics_dbt.rfps_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.rfps_per_week"} */


  create  table "adludio"."public"."rfps_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_rfp_count)*7 as avg_rfps_count, year_quarter
from source_data
group by year_quarter
ORDER BY year_quarter

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
2022-03-07 11:24:14.042822 (Thread-1): SQL status: SELECT 8 in 0.03 seconds
2022-03-07 11:24:14.050829 (Thread-1): Using postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 11:24:14.050829 (Thread-1): On model.Analytics_dbt.rfps_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.rfps_per_week"} */
alter table "adludio"."public"."rfps_per_week__dbt_tmp" rename to "rfps_per_week"
2022-03-07 11:24:14.051820 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-03-07 11:24:14.056821 (Thread-1): On model.Analytics_dbt.rfps_per_week: COMMIT
2022-03-07 11:24:14.057818 (Thread-1): Using postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 11:24:14.057818 (Thread-1): On model.Analytics_dbt.rfps_per_week: COMMIT
2022-03-07 11:24:14.059816 (Thread-1): SQL status: COMMIT in 0.00 seconds
2022-03-07 11:24:14.064822 (Thread-1): Using postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 11:24:14.064822 (Thread-1): On model.Analytics_dbt.rfps_per_week: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.rfps_per_week"} */
drop table if exists "adludio"."public"."rfps_per_week__dbt_backup" cascade
2022-03-07 11:24:14.065819 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-03-07 11:24:14.070820 (Thread-1): finished collecting timing info
2022-03-07 11:24:14.071819 (Thread-1): On model.Analytics_dbt.rfps_per_week: Close
2022-03-07 11:24:14.072820 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '530c8bb1-3087-4481-baf4-b447a0d53c0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C627A794F0>]}
2022-03-07 11:24:14.073823 (Thread-1): 08:24:14 | 6 of 6 OK created table model public.rfps_per_week................... [SELECT 8 in 0.29s]
2022-03-07 11:24:14.076819 (Thread-1): Finished running node model.Analytics_dbt.rfps_per_week
2022-03-07 11:24:14.081825 (MainThread): Acquiring new postgres connection "master".
2022-03-07 11:24:14.082822 (MainThread): Using postgres connection "master".
2022-03-07 11:24:14.082822 (MainThread): On master: BEGIN
2022-03-07 11:24:14.082822 (MainThread): Opening a new connection, currently in state closed
2022-03-07 11:24:14.324819 (MainThread): SQL status: BEGIN in 0.24 seconds
2022-03-07 11:24:14.325818 (MainThread): On master: COMMIT
2022-03-07 11:24:14.325818 (MainThread): Using postgres connection "master".
2022-03-07 11:24:14.325818 (MainThread): On master: COMMIT
2022-03-07 11:24:14.326814 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-03-07 11:24:14.327816 (MainThread): On master: Close
2022-03-07 11:24:14.328819 (MainThread): 08:24:14 | 
2022-03-07 11:24:14.330821 (MainThread): 08:24:14 | Finished running 6 table models in 3.27s.
2022-03-07 11:24:14.332815 (MainThread): Connection 'master' was properly closed.
2022-03-07 11:24:14.332815 (MainThread): Connection 'list_adludio' was properly closed.
2022-03-07 11:24:14.333824 (MainThread): Connection 'list_adludio_public' was properly closed.
2022-03-07 11:24:14.333824 (MainThread): Connection 'model.Analytics_dbt.rfps_per_week' was properly closed.
2022-03-07 11:24:14.352820 (MainThread): 
2022-03-07 11:24:14.355826 (MainThread): Completed successfully
2022-03-07 11:24:14.363860 (MainThread): 
Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
2022-03-07 11:24:14.367838 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6278FF670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6278FF2E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C62776DE80>]}
2022-03-07 11:24:14.369822 (MainThread): Flushing usage events
2022-03-07 14:15:16.515976 (MainThread): Running with dbt=0.20.2
2022-03-07 14:15:17.156974 (MainThread): running dbt with arguments Namespace(record_timing_info=None, debug=False, log_format='default', write_json=True, use_colors=None, strict=False, warn_error=False, partial_parse=None, single_threaded=False, test_new_parser=False, use_experimental_parser=False, project_dir=None, profiles_dir='C:\\Users\\Abreham\\.dbt', profile=None, target=None, vars='{}', log_cache_events=False, use_cache=True, compile=True, threads=None, version_check=True, models=None, exclude=None, selector_name=None, state=None, defer=None, cls=<class 'dbt.task.generate.GenerateTask'>, which='generate', rpc_method='docs.generate')
2022-03-07 14:15:17.163978 (MainThread): Tracking: tracking
2022-03-07 14:15:17.249979 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000213FBB2F2B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002138B4FD1C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002138B4FDE20>]}
2022-03-07 14:15:17.307982 (MainThread): Partial parsing not enabled
2022-03-07 14:15:17.473976 (MainThread): Parsing macros\adapters.sql
2022-03-07 14:15:17.723979 (MainThread): Parsing macros\catalog.sql
2022-03-07 14:15:17.741975 (MainThread): Parsing macros\relations.sql
2022-03-07 14:15:17.750984 (MainThread): Parsing macros\materializations\snapshot_merge.sql
2022-03-07 14:15:17.759977 (MainThread): Parsing macros\core.sql
2022-03-07 14:15:17.784976 (MainThread): Parsing macros\adapters\common.sql
2022-03-07 14:15:17.993971 (MainThread): Parsing macros\etc\datetime.sql
2022-03-07 14:15:18.056976 (MainThread): Parsing macros\etc\get_custom_alias.sql
2022-03-07 14:15:18.065975 (MainThread): Parsing macros\etc\get_custom_database.sql
2022-03-07 14:15:18.088976 (MainThread): Parsing macros\etc\get_custom_schema.sql
2022-03-07 14:15:18.112976 (MainThread): Parsing macros\etc\is_incremental.sql
2022-03-07 14:15:18.125972 (MainThread): Parsing macros\etc\query.sql
2022-03-07 14:15:18.131974 (MainThread): Parsing macros\materializations\helpers.sql
2022-03-07 14:15:18.245972 (MainThread): Parsing macros\materializations\test.sql
2022-03-07 14:15:18.287976 (MainThread): Parsing macros\materializations\common\merge.sql
2022-03-07 14:15:18.370969 (MainThread): Parsing macros\materializations\incremental\helpers.sql
2022-03-07 14:15:18.376972 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2022-03-07 14:15:18.403969 (MainThread): Parsing macros\materializations\seed\seed.sql
2022-03-07 14:15:18.475977 (MainThread): Parsing macros\materializations\snapshot\snapshot.sql
2022-03-07 14:15:18.574974 (MainThread): Parsing macros\materializations\snapshot\snapshot_merge.sql
2022-03-07 14:15:18.581980 (MainThread): Parsing macros\materializations\snapshot\strategies.sql
2022-03-07 14:15:18.638974 (MainThread): Parsing macros\materializations\table\table.sql
2022-03-07 14:15:18.660002 (MainThread): Parsing macros\materializations\view\create_or_replace_view.sql
2022-03-07 14:15:18.673979 (MainThread): Parsing macros\materializations\view\view.sql
2022-03-07 14:15:18.698000 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2022-03-07 14:15:18.705972 (MainThread): Parsing macros\schema_tests\not_null.sql
2022-03-07 14:15:18.709976 (MainThread): Parsing macros\schema_tests\relationships.sql
2022-03-07 14:15:18.715974 (MainThread): Parsing macros\schema_tests\unique.sql
2022-03-07 14:15:19.310975 (MainThread): Acquiring new postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 14:15:19.342972 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 14:15:19.355982 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 14:15:19.369973 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 14:15:19.384972 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 14:15:19.398969 (MainThread): Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 14:15:19.512977 (MainThread): [WARNING]: Test 'test.Analytics_dbt.unique_my_first_dbt_model_id.5ef3c774bb' (models\Sales Numbers\schema.yml) depends on a node named 'my_first_dbt_model' which was not found
2022-03-07 14:15:19.516975 (MainThread): [WARNING]: Test 'test.Analytics_dbt.not_null_my_first_dbt_model_id.f479667b44' (models\Sales Numbers\schema.yml) depends on a node named 'my_first_dbt_model' which was not found
2022-03-07 14:15:19.518978 (MainThread): [WARNING]: Test 'test.Analytics_dbt.unique_my_second_dbt_model_id.bc96125570' (models\Sales Numbers\schema.yml) depends on a node named 'my_second_dbt_model' which was not found
2022-03-07 14:15:19.524981 (MainThread): [WARNING]: Test 'test.Analytics_dbt.not_null_my_second_dbt_model_id.dc1d13aa6d' (models\Sales Numbers\schema.yml) depends on a node named 'my_second_dbt_model' which was not found
2022-03-07 14:15:19.606989 (MainThread): [WARNING]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

2022-03-07 14:15:19.662989 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '02e544b6-8141-4cf2-b036-c7ad79d311a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002138B6B5520>]}
2022-03-07 14:15:19.715982 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '02e544b6-8141-4cf2-b036-c7ad79d311a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002138B5C9BE0>]}
2022-03-07 14:15:19.716986 (MainThread): Found 6 models, 0 tests, 0 snapshots, 0 analyses, 147 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2022-03-07 14:15:19.732989 (MainThread): 
2022-03-07 14:15:19.735985 (MainThread): Acquiring new postgres connection "master".
2022-03-07 14:15:19.746983 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_adludio_public".
2022-03-07 14:15:19.903279 (ThreadPoolExecutor-0_0): Using postgres connection "list_adludio_public".
2022-03-07 14:15:19.904279 (ThreadPoolExecutor-0_0): On list_adludio_public: BEGIN
2022-03-07 14:15:19.905277 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-03-07 14:15:20.142275 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.24 seconds
2022-03-07 14:15:20.143276 (ThreadPoolExecutor-0_0): Using postgres connection "list_adludio_public".
2022-03-07 14:15:20.143276 (ThreadPoolExecutor-0_0): On list_adludio_public: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
2022-03-07 14:15:20.210275 (ThreadPoolExecutor-0_0): SQL status: SELECT 7 in 0.07 seconds
2022-03-07 14:15:20.226275 (ThreadPoolExecutor-0_0): On list_adludio_public: ROLLBACK
2022-03-07 14:15:20.228278 (ThreadPoolExecutor-0_0): On list_adludio_public: Close
2022-03-07 14:15:20.251280 (MainThread): Using postgres connection "master".
2022-03-07 14:15:20.252278 (MainThread): On master: BEGIN
2022-03-07 14:15:20.252278 (MainThread): Opening a new connection, currently in state init
2022-03-07 14:15:20.441277 (MainThread): SQL status: BEGIN in 0.19 seconds
2022-03-07 14:15:20.441277 (MainThread): Using postgres connection "master".
2022-03-07 14:15:20.442277 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-03-07 14:15:20.507276 (MainThread): SQL status: SELECT 0 in 0.06 seconds
2022-03-07 14:15:20.508279 (MainThread): On master: ROLLBACK
2022-03-07 14:15:20.509277 (MainThread): On master: Close
2022-03-07 14:15:20.510278 (MainThread): 11:15:20 | Concurrency: 1 threads (target='dev')
2022-03-07 14:15:20.512278 (MainThread): 11:15:20 | 
2022-03-07 14:15:20.578276 (Thread-1): Began running node model.Analytics_dbt.transformed_sales_number_data
2022-03-07 14:15:20.580283 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data".
2022-03-07 14:15:20.581281 (Thread-1): Compiling model.Analytics_dbt.transformed_sales_number_data
2022-03-07 14:15:20.612282 (Thread-1): Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
2022-03-07 14:15:20.623289 (Thread-1): finished collecting timing info
2022-03-07 14:15:20.626834 (Thread-1): finished collecting timing info
2022-03-07 14:15:20.629831 (Thread-1): Finished running node model.Analytics_dbt.transformed_sales_number_data
2022-03-07 14:15:20.637387 (Thread-1): Began running node model.Analytics_dbt. meeting_per_week
2022-03-07 14:15:20.643392 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt. meeting_per_week".
2022-03-07 14:15:20.644390 (Thread-1): Compiling model.Analytics_dbt. meeting_per_week
2022-03-07 14:15:20.679499 (Thread-1): Writing injected SQL for node "model.Analytics_dbt. meeting_per_week"
2022-03-07 14:15:20.685497 (Thread-1): finished collecting timing info
2022-03-07 14:15:20.687498 (Thread-1): finished collecting timing info
2022-03-07 14:15:20.690506 (Thread-1): Finished running node model.Analytics_dbt. meeting_per_week
2022-03-07 14:15:20.690506 (Thread-1): Began running node model.Analytics_dbt.deal_value_per_week
2022-03-07 14:15:20.692500 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week".
2022-03-07 14:15:20.692500 (Thread-1): Compiling model.Analytics_dbt.deal_value_per_week
2022-03-07 14:15:20.705501 (Thread-1): Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
2022-03-07 14:15:20.709499 (Thread-1): finished collecting timing info
2022-03-07 14:15:20.709499 (Thread-1): finished collecting timing info
2022-03-07 14:15:20.713500 (Thread-1): Finished running node model.Analytics_dbt.deal_value_per_week
2022-03-07 14:15:20.714498 (Thread-1): Began running node model.Analytics_dbt.email_per_week
2022-03-07 14:15:20.716500 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt.email_per_week".
2022-03-07 14:15:20.716500 (Thread-1): Compiling model.Analytics_dbt.email_per_week
2022-03-07 14:15:20.728499 (Thread-1): Writing injected SQL for node "model.Analytics_dbt.email_per_week"
2022-03-07 14:15:20.732498 (Thread-1): finished collecting timing info
2022-03-07 14:15:20.733499 (Thread-1): finished collecting timing info
2022-03-07 14:15:20.734500 (Thread-1): Finished running node model.Analytics_dbt.email_per_week
2022-03-07 14:15:20.735498 (Thread-1): Began running node model.Analytics_dbt.ios_per_week
2022-03-07 14:15:20.737499 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt.ios_per_week".
2022-03-07 14:15:20.738500 (Thread-1): Compiling model.Analytics_dbt.ios_per_week
2022-03-07 14:15:20.746495 (Thread-1): Writing injected SQL for node "model.Analytics_dbt.ios_per_week"
2022-03-07 14:15:20.750497 (Thread-1): finished collecting timing info
2022-03-07 14:15:20.751497 (Thread-1): finished collecting timing info
2022-03-07 14:15:20.753496 (Thread-1): Finished running node model.Analytics_dbt.ios_per_week
2022-03-07 14:15:20.753496 (Thread-1): Began running node model.Analytics_dbt.rfps_per_week
2022-03-07 14:15:20.755502 (Thread-1): Acquiring new postgres connection "model.Analytics_dbt.rfps_per_week".
2022-03-07 14:15:20.755502 (Thread-1): Compiling model.Analytics_dbt.rfps_per_week
2022-03-07 14:15:20.769501 (Thread-1): Writing injected SQL for node "model.Analytics_dbt.rfps_per_week"
2022-03-07 14:15:20.771497 (Thread-1): finished collecting timing info
2022-03-07 14:15:20.772495 (Thread-1): finished collecting timing info
2022-03-07 14:15:20.774498 (Thread-1): Finished running node model.Analytics_dbt.rfps_per_week
2022-03-07 14:15:20.777504 (MainThread): Connection 'master' was properly closed.
2022-03-07 14:15:20.778497 (MainThread): Connection 'list_adludio_public' was properly closed.
2022-03-07 14:15:20.778497 (MainThread): Connection 'model.Analytics_dbt.rfps_per_week' was properly closed.
2022-03-07 14:15:20.797498 (MainThread): 11:15:20 | Done.
2022-03-07 14:15:21.720478 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-03-07 14:15:21.720478 (MainThread): 11:15:21 | Building catalog
2022-03-07 14:15:21.737487 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "adludio.information_schema".
2022-03-07 14:15:21.772530 (ThreadPoolExecutor-1_0): Using postgres connection "adludio.information_schema".
2022-03-07 14:15:21.774485 (ThreadPoolExecutor-1_0): On adludio.information_schema: BEGIN
2022-03-07 14:15:21.776501 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-03-07 14:15:21.919482 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.14 seconds
2022-03-07 14:15:21.920480 (ThreadPoolExecutor-1_0): Using postgres connection "adludio.information_schema".
2022-03-07 14:15:21.920480 (ThreadPoolExecutor-1_0): On adludio.information_schema: /* {"app": "dbt", "dbt_version": "0.20.2", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "adludio.information_schema"} */

    
    

    select
        'adludio' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        tbl_desc.description as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        col_desc.description as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid
    left outer join pg_catalog.pg_description tbl_desc on (tbl_desc.objoid = tbl.oid and tbl_desc.objsubid = 0)
    left outer join pg_catalog.pg_description col_desc on (col_desc.objoid = tbl.oid and col_desc.objsubid = col.attnum)

    where (upper(sch.nspname) = upper('public_dbt_test__audit') or upper(sch.nspname) = upper('public'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-03-07 14:15:21.988479 (ThreadPoolExecutor-1_0): SQL status: SELECT 37 in 0.07 seconds
2022-03-07 14:15:22.005476 (ThreadPoolExecutor-1_0): On adludio.information_schema: ROLLBACK
2022-03-07 14:15:22.006481 (ThreadPoolExecutor-1_0): On adludio.information_schema: Close
2022-03-07 14:15:22.037481 (MainThread): 11:15:22 | Catalog written to C:\Users\Abreham\Desktop\adludio-challenge\target\catalog.json
2022-03-07 14:15:22.039482 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000213FBB2F2B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002138B7D32B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002138B7D3340>]}
2022-03-07 14:15:22.040482 (MainThread): Flushing usage events
2022-03-07 14:15:23.808281 (MainThread): Connection 'generate_catalog' was properly closed.
2022-03-07 14:15:23.809290 (MainThread): Connection 'adludio.information_schema' was properly closed.
2022-03-07 14:16:21.498956 (MainThread): Running with dbt=0.20.2
2022-03-07 14:16:21.783924 (MainThread): running dbt with arguments Namespace(record_timing_info=None, debug=False, log_format='default', write_json=True, use_colors=None, strict=False, warn_error=False, partial_parse=None, single_threaded=False, test_new_parser=False, use_experimental_parser=False, project_dir=None, profiles_dir='C:\\Users\\Abreham\\.dbt', profile=None, target=None, vars='{}', log_cache_events=False, use_cache=True, port=8080, open_browser=True, defer=None, state=None, cls=<class 'dbt.task.serve.ServeTask'>, which='serve', rpc_method=None)
2022-03-07 14:16:21.785931 (MainThread): Tracking: tracking
2022-03-07 14:16:21.812928 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA3993580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA399D0D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA399DD90>]}
2022-03-07 14:16:21.823932 (MainThread): Serving docs at 0.0.0.0:8080
2022-03-07 14:16:21.824932 (MainThread): To access from your browser, navigate to:  http://localhost:8080
2022-03-07 14:16:21.825931 (MainThread): Press Ctrl+C to exit.


2022-03-07 14:35:12.050028 (MainThread): Flushing usage events
2022-03-07 14:35:13.640444 (MainThread): ctrl-c
2022-03-07 14:35:47.827265 (MainThread): Running with dbt=0.20.2
2022-03-07 14:35:48.273265 (MainThread): running dbt with arguments Namespace(record_timing_info=None, debug=False, log_format='default', write_json=True, use_colors=None, strict=False, warn_error=False, partial_parse=None, single_threaded=False, test_new_parser=False, use_experimental_parser=False, project_dir=None, profiles_dir='C:\\Users\\Abreham\\.dbt', profile=None, target=None, vars='{}', log_cache_events=False, use_cache=True, port=4040, open_browser=True, defer=None, state=None, cls=<class 'dbt.task.serve.ServeTask'>, which='serve', rpc_method=None)
2022-03-07 14:35:48.277269 (MainThread): Tracking: tracking
2022-03-07 14:35:48.355272 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205245EA220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002052451D220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002052451DF40>]}
2022-03-07 14:35:48.368265 (MainThread): Serving docs at 0.0.0.0:4040
2022-03-07 14:35:48.369263 (MainThread): To access from your browser, navigate to:  http://localhost:4040
2022-03-07 14:35:48.371265 (MainThread): Press Ctrl+C to exit.


2022-03-07 14:37:10.594282 (MainThread): Flushing usage events
2022-03-07 14:37:13.446117 (MainThread): ctrl-c
