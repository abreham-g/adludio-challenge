

============================== 2022-03-06 04:58:58.109377 | 93f1c1f7-2418-462c-9e3e-c5c39883e6de ==============================
04:58:58.109377 [info ] [MainThread]: Running with dbt=1.0.3
04:58:58.109992 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, project_name='Analytics_dbt', skip_profile_setup=False, defer=None, state=None, cls=<class 'dbt.task.init.InitTask'>, which='init', rpc_method=None)
04:58:58.110261 [debug] [MainThread]: Tracking: tracking
04:58:58.115060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb73453ff70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb73453ffd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb73453fdf0>]}
04:58:58.116038 [info ] [MainThread]: Creating dbt configuration folder at /home/abreham/.dbt
04:58:58.117248 [debug] [MainThread]: Starter project path: /opt/miniconda/lib/python3.9/site-packages/dbt/include/starter_project
04:59:18.247465 [info ] [MainThread]: Profile Analytics_dbt written to /home/abreham/.dbt/profiles.yml using target's sample configuration. Once updated, you'll be able to start developing with dbt.
04:59:18.251164 [info ] [MainThread]: 
Your new dbt project "Analytics_dbt" was created!

For more information on how to configure the profiles.yml file,
please consult the dbt documentation here:

  https://docs.getdbt.com/docs/configure-your-profile

One more thing:

Need help? Don't hesitate to reach out to us via GitHub issues or on Slack:

  https://community.getdbt.com/

Happy modeling!

04:59:18.251705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb73451cc70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb73451cd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb73451c610>]}


============================== 2022-03-06 09:26:04.566980 | 9fb2f309-8c7f-4dcf-ba9f-77af8c17b614 ==============================
09:26:04.566980 [info ] [MainThread]: Running with dbt=1.0.3
09:26:04.567650 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, config_dir=False, defer=None, state=None, cls=<class 'dbt.task.debug.DebugTask'>, which='debug', rpc_method=None)
09:26:04.568347 [debug] [MainThread]: Tracking: tracking
09:26:04.573204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f829d3c8640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f829d3c8940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f829d3c8820>]}
09:26:04.695869 [debug] [MainThread]: Executing "git --help"
09:26:04.701380 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
09:26:04.702048 [debug] [MainThread]: STDERR: "b''"
09:26:04.702719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f829d3b1760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f829b965a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f829b965be0>]}


============================== 2022-03-06 09:28:31.158222 | 69717325-01d9-40e3-9319-21e290c400b8 ==============================
09:28:31.158222 [info ] [MainThread]: Running with dbt=1.0.3
09:28:31.158827 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, config_dir=False, defer=None, state=None, cls=<class 'dbt.task.debug.DebugTask'>, which='debug', rpc_method=None)
09:28:31.159120 [debug] [MainThread]: Tracking: tracking
09:28:31.166983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c91ef850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c91ef8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c91efa00>]}
09:28:31.300945 [debug] [MainThread]: Executing "git --help"
09:28:31.309178 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
09:28:31.309833 [debug] [MainThread]: STDERR: "b''"
09:28:31.314083 [debug] [MainThread]: Acquiring new postgres connection "debug"
09:28:31.314750 [debug] [MainThread]: Using postgres connection "debug"
09:28:31.315026 [debug] [MainThread]: On debug: select 1 as id
09:28:31.315288 [debug] [MainThread]: Opening a new connection, currently in state init
09:28:31.341247 [debug] [MainThread]: SQL status: SELECT 1 in 0.03 seconds
09:28:31.343871 [debug] [MainThread]: On debug: Close
09:28:31.348093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c77873d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c7787d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c7787940>]}
09:28:31.663325 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-03-06 09:28:50.603165 | f0e3d4f8-54ab-481a-9ad4-13ad32de3350 ==============================
09:28:50.603165 [info ] [MainThread]: Running with dbt=1.0.3
09:28:50.603771 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, config_dir=False, defer=None, state=None, cls=<class 'dbt.task.debug.DebugTask'>, which='debug', rpc_method=None)
09:28:50.604056 [debug] [MainThread]: Tracking: tracking
09:28:50.608403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f32eec8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f32eec940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f32eec910>]}
09:28:50.714060 [debug] [MainThread]: Executing "git --help"
09:28:50.719423 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
09:28:50.720075 [debug] [MainThread]: STDERR: "b''"
09:28:50.724459 [debug] [MainThread]: Acquiring new postgres connection "debug"
09:28:50.725531 [debug] [MainThread]: Using postgres connection "debug"
09:28:50.725858 [debug] [MainThread]: On debug: select 1 as id
09:28:50.726166 [debug] [MainThread]: Opening a new connection, currently in state init
09:28:50.740217 [debug] [MainThread]: SQL status: SELECT 1 in 0.01 seconds
09:28:50.742506 [debug] [MainThread]: On debug: Close
09:28:50.746386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f31480310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f31480f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f314808e0>]}
09:28:51.042614 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-03-06 10:26:49.479278 | e611a26c-4a51-4992-b62c-a5b71effa97f ==============================
10:26:49.479278 [info ] [MainThread]: Running with dbt=1.0.3
10:26:49.484195 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:26:49.484854 [debug] [MainThread]: Tracking: tracking
10:26:49.490668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53fcb1b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53fcb1460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53fcb1640>]}
10:26:49.508863 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
10:26:49.509977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e611a26c-4a51-4992-b62c-a5b71effa97f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53e242fa0>]}
10:26:49.534738 [debug] [MainThread]: Parsing macros/adapters.sql
10:26:49.590857 [debug] [MainThread]: Parsing macros/catalog.sql
10:26:49.597847 [debug] [MainThread]: Parsing macros/relations.sql
10:26:49.600290 [debug] [MainThread]: Parsing macros/materializations/snapshot_merge.sql
10:26:49.603084 [debug] [MainThread]: Parsing macros/adapters/columns.sql
10:26:49.629835 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
10:26:49.636284 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
10:26:49.640973 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
10:26:49.653254 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
10:26:49.660937 [debug] [MainThread]: Parsing macros/adapters/relation.sql
10:26:49.677513 [debug] [MainThread]: Parsing macros/adapters/schema.sql
10:26:49.681260 [debug] [MainThread]: Parsing macros/etc/datetime.sql
10:26:49.695747 [debug] [MainThread]: Parsing macros/etc/statement.sql
10:26:49.703157 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
10:26:49.705566 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
10:26:49.706583 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
10:26:49.708059 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
10:26:49.709210 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
10:26:49.711622 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
10:26:49.714557 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
10:26:49.718491 [debug] [MainThread]: Parsing macros/materializations/configs.sql
10:26:49.723986 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
10:26:49.730799 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
10:26:49.738331 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
10:26:49.759474 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
10:26:49.762221 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
10:26:49.783877 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
10:26:49.822602 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
10:26:49.827525 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
10:26:49.840509 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
10:26:49.845145 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
10:26:49.849324 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
10:26:49.851619 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
10:26:49.863918 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
10:26:49.895587 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
10:26:49.906141 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
10:26:49.928676 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
10:26:49.949212 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
10:26:49.952307 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
10:26:49.986821 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
10:26:49.990036 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
10:26:49.997613 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
10:26:50.000784 [debug] [MainThread]: Parsing tests/generic/builtin.sql
10:26:50.413479 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:26:50.433561 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
10:26:50.531043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e611a26c-4a51-4992-b62c-a5b71effa97f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53d940a90>]}
10:26:50.544090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e611a26c-4a51-4992-b62c-a5b71effa97f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53fc9f9a0>]}
10:26:50.544601 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:26:50.547645 [info ] [MainThread]: 
10:26:50.548869 [debug] [MainThread]: Acquiring new postgres connection "master"
10:26:50.550726 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:26:50.569423 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:26:50.569841 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:26:50.570091 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:26:50.583360 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:26:50.585822 [debug] [ThreadPool]: On list_adludio: Close
10:26:50.587628 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:26:50.603690 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:26:50.604041 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:26:50.604278 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:26:50.617015 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:26:50.617422 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:26:50.617706 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:26:50.628805 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.01 seconds
10:26:50.631156 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:26:50.634859 [debug] [ThreadPool]: On list_adludio_public: Close
10:26:50.641962 [debug] [MainThread]: Using postgres connection "master"
10:26:50.642369 [debug] [MainThread]: On master: BEGIN
10:26:50.642637 [debug] [MainThread]: Opening a new connection, currently in state init
10:26:50.668831 [debug] [MainThread]: SQL status: BEGIN in 0.03 seconds
10:26:50.669340 [debug] [MainThread]: Using postgres connection "master"
10:26:50.669912 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:26:50.693152 [debug] [MainThread]: SQL status: SELECT 5 in 0.02 seconds
10:26:50.695716 [debug] [MainThread]: On master: ROLLBACK
10:26:50.696189 [debug] [MainThread]: Using postgres connection "master"
10:26:50.696444 [debug] [MainThread]: On master: BEGIN
10:26:50.696894 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:26:50.697154 [debug] [MainThread]: On master: COMMIT
10:26:50.697381 [debug] [MainThread]: Using postgres connection "master"
10:26:50.697621 [debug] [MainThread]: On master: COMMIT
10:26:50.697927 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:26:50.698189 [debug] [MainThread]: On master: Close
10:26:50.698769 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:26:50.699806 [info ] [MainThread]: 
10:26:50.704611 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:26:50.705069 [info ] [Thread-1  ]: 1 of 2 START table model public.my_first_dbt_model.............................. [RUN]
10:26:50.705909 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.706184 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:26:50.706452 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:26:50.710550 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:26:50.711122 [debug] [Thread-1  ]: finished collecting timing info
10:26:50.711388 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:26:50.776691 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:26:50.777506 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.777803 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:26:50.778056 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:26:50.798826 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
10:26:50.799249 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.799553 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:26:50.804768 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.0 seconds
10:26:50.825016 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.825353 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:26:50.826142 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:26:50.830015 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.830292 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:26:50.830878 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:26:50.849942 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:26:50.850330 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.850634 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:26:50.851971 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:26:50.859128 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:26:50.859425 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:26:50.864980 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
10:26:50.866887 [debug] [Thread-1  ]: finished collecting timing info
10:26:50.867215 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:26:50.867871 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e611a26c-4a51-4992-b62c-a5b71effa97f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53c9105b0>]}
10:26:50.868412 [info ] [Thread-1  ]: 1 of 2 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2[0m in 0.16s]
10:26:50.869245 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:26:50.870212 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_second_dbt_model
10:26:50.870876 [info ] [Thread-1  ]: 2 of 2 START view model public.my_second_dbt_model.............................. [RUN]
10:26:50.871593 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:26:50.871846 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_second_dbt_model
10:26:50.872089 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_second_dbt_model
10:26:50.877663 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:26:50.878124 [debug] [Thread-1  ]: finished collecting timing info
10:26:50.878401 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_second_dbt_model
10:26:50.908379 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:26:50.908905 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:26:50.909118 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: BEGIN
10:26:50.909293 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:26:50.919919 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:26:50.920298 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:26:50.920574 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */

  create view "adludio"."public"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "adludio"."public"."my_first_dbt_model"
where id = 1
  );
10:26:50.925149 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.0 seconds
10:26:50.929158 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:26:50.929433 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */
alter table "adludio"."public"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
10:26:50.930147 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:26:50.932176 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: COMMIT
10:26:50.932475 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:26:50.932717 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: COMMIT
10:26:50.933891 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:26:50.936542 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:26:50.936822 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */
drop view if exists "adludio"."public"."my_second_dbt_model__dbt_backup" cascade
10:26:50.937254 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
10:26:50.938943 [debug] [Thread-1  ]: finished collecting timing info
10:26:50.939258 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: Close
10:26:50.939953 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e611a26c-4a51-4992-b62c-a5b71effa97f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53e243940>]}
10:26:50.940504 [info ] [Thread-1  ]: 2 of 2 OK created view model public.my_second_dbt_model......................... [[32mCREATE VIEW[0m in 0.07s]
10:26:50.941474 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_second_dbt_model
10:26:50.947167 [debug] [MainThread]: Acquiring new postgres connection "master"
10:26:50.947513 [debug] [MainThread]: Using postgres connection "master"
10:26:50.947757 [debug] [MainThread]: On master: BEGIN
10:26:50.947984 [debug] [MainThread]: Opening a new connection, currently in state closed
10:26:50.958747 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:26:50.959064 [debug] [MainThread]: On master: COMMIT
10:26:50.959305 [debug] [MainThread]: Using postgres connection "master"
10:26:50.959541 [debug] [MainThread]: On master: COMMIT
10:26:50.959872 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:26:50.960140 [debug] [MainThread]: On master: Close
10:26:50.960712 [info ] [MainThread]: 
10:26:50.961633 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0.41s.
10:26:50.962392 [debug] [MainThread]: Connection 'master' was properly closed.
10:26:50.962667 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:26:50.962883 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_second_dbt_model' was properly closed.
10:26:50.971195 [info ] [MainThread]: 
10:26:50.971689 [info ] [MainThread]: [32mCompleted successfully[0m
10:26:50.977804 [info ] [MainThread]: 
10:26:50.978410 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
10:26:50.979138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53fc9f400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53fc93760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd53e1c9490>]}


============================== 2022-03-06 10:31:42.377658 | 7385daaa-d28d-4624-85c6-c5d5b13a0802 ==============================
10:31:42.377658 [info ] [MainThread]: Running with dbt=1.0.3
10:31:42.378504 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:31:42.378859 [debug] [MainThread]: Tracking: tracking
10:31:42.383967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abcf46190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abcf46370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abcf467c0>]}
10:31:42.423021 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:31:42.423849 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:31:42.442839 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:31:42.507628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7385daaa-d28d-4624-85c6-c5d5b13a0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abc5b40d0>]}
10:31:42.515827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7385daaa-d28d-4624-85c6-c5d5b13a0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abc5b46d0>]}
10:31:42.516322 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:31:42.518393 [info ] [MainThread]: 
10:31:42.519172 [debug] [MainThread]: Acquiring new postgres connection "master"
10:31:42.520506 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:31:42.537378 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:31:42.537775 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:31:42.538054 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:31:42.550967 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:31:42.553338 [debug] [ThreadPool]: On list_adludio: Close
10:31:42.555150 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:31:42.618664 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:31:42.619065 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:31:42.619325 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:31:42.630018 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:31:42.630357 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:31:42.630600 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:31:42.633779 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.0 seconds
10:31:42.635989 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:31:42.636400 [debug] [ThreadPool]: On list_adludio_public: Close
10:31:42.643107 [debug] [MainThread]: Using postgres connection "master"
10:31:42.643431 [debug] [MainThread]: On master: BEGIN
10:31:42.643676 [debug] [MainThread]: Opening a new connection, currently in state init
10:31:42.654265 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:31:42.654582 [debug] [MainThread]: Using postgres connection "master"
10:31:42.654828 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:31:42.682657 [debug] [MainThread]: SQL status: SELECT 5 in 0.03 seconds
10:31:42.685196 [debug] [MainThread]: On master: ROLLBACK
10:31:42.685701 [debug] [MainThread]: Using postgres connection "master"
10:31:42.685970 [debug] [MainThread]: On master: BEGIN
10:31:42.686420 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:31:42.686684 [debug] [MainThread]: On master: COMMIT
10:31:42.686928 [debug] [MainThread]: Using postgres connection "master"
10:31:42.687158 [debug] [MainThread]: On master: COMMIT
10:31:42.687474 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:31:42.687734 [debug] [MainThread]: On master: Close
10:31:42.688353 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:31:42.689391 [info ] [MainThread]: 
10:31:42.693987 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:31:42.694446 [info ] [Thread-1  ]: 1 of 2 START table model public.my_first_dbt_model.............................. [RUN]
10:31:42.695447 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.695715 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:31:42.695988 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:31:42.700273 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:31:42.700774 [debug] [Thread-1  ]: finished collecting timing info
10:31:42.701046 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:31:42.744033 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:31:42.744721 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.745018 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:31:42.745262 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:31:42.755875 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:31:42.756215 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.756458 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select * from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:31:42.762638 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
10:31:42.772281 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.772595 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:31:42.773299 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:31:42.776917 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.777183 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:31:42.777787 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:31:42.794082 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:31:42.794419 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.794675 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:31:42.796956 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:31:42.804162 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:31:42.804472 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:31:42.806843 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
10:31:42.808624 [debug] [Thread-1  ]: finished collecting timing info
10:31:42.808936 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:31:42.809629 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7385daaa-d28d-4624-85c6-c5d5b13a0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abce5de50>]}
10:31:42.810245 [info ] [Thread-1  ]: 1 of 2 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2037[0m in 0.11s]
10:31:42.811399 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:31:42.812646 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_second_dbt_model
10:31:42.813034 [info ] [Thread-1  ]: 2 of 2 START view model public.my_second_dbt_model.............................. [RUN]
10:31:42.813781 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:31:42.814047 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_second_dbt_model
10:31:42.814291 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_second_dbt_model
10:31:42.817940 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:31:42.818438 [debug] [Thread-1  ]: finished collecting timing info
10:31:42.818705 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_second_dbt_model
10:31:42.845217 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:31:42.845903 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:31:42.846199 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: BEGIN
10:31:42.846434 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:31:42.857047 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:31:42.857385 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:31:42.857675 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */

  create view "adludio"."public"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "adludio"."public"."my_first_dbt_model"
where id = 1
  );
10:31:42.858360 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "id" does not exist
LINE 8: where id = 1
              ^

10:31:42.858639 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: ROLLBACK
10:31:42.859188 [debug] [Thread-1  ]: finished collecting timing info
10:31:42.859467 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: Close
10:31:42.860049 [debug] [Thread-1  ]: Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  column "id" does not exist
  LINE 8: where id = 1
                ^
  compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:31:42.860532 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7385daaa-d28d-4624-85c6-c5d5b13a0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abc05d9d0>]}
10:31:42.861124 [error] [Thread-1  ]: 2 of 2 ERROR creating view model public.my_second_dbt_model..................... [[31mERROR[0m in 0.05s]
10:31:42.861767 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_second_dbt_model
10:31:42.863415 [debug] [MainThread]: Acquiring new postgres connection "master"
10:31:42.863730 [debug] [MainThread]: Using postgres connection "master"
10:31:42.863973 [debug] [MainThread]: On master: BEGIN
10:31:42.864194 [debug] [MainThread]: Opening a new connection, currently in state closed
10:31:42.874627 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:31:42.874989 [debug] [MainThread]: On master: COMMIT
10:31:42.875255 [debug] [MainThread]: Using postgres connection "master"
10:31:42.875490 [debug] [MainThread]: On master: COMMIT
10:31:42.875820 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:31:42.876108 [debug] [MainThread]: On master: Close
10:31:42.876725 [info ] [MainThread]: 
10:31:42.877698 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0.36s.
10:31:42.878193 [debug] [MainThread]: Connection 'master' was properly closed.
10:31:42.878443 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:31:42.878659 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_second_dbt_model' was properly closed.
10:31:42.887932 [info ] [MainThread]: 
10:31:42.888444 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:31:42.888913 [info ] [MainThread]: 
10:31:42.889334 [error] [MainThread]: [33mDatabase Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)[0m
10:31:42.889776 [error] [MainThread]:   column "id" does not exist
10:31:42.890160 [error] [MainThread]:   LINE 8: where id = 1
10:31:42.890528 [error] [MainThread]:                 ^
10:31:42.890889 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:31:42.891273 [info ] [MainThread]: 
10:31:42.891653 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
10:31:42.892164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abe9ba490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abcf31cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3abc0725b0>]}


============================== 2022-03-06 10:32:06.140648 | 51fbc5ce-73e1-4764-b690-d84a060365d0 ==============================
10:32:06.140648 [info ] [MainThread]: Running with dbt=1.0.3
10:32:06.141542 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:32:06.141896 [debug] [MainThread]: Tracking: tracking
10:32:06.146649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1e9fbb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1e9fb460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1e9fb640>]}
10:32:06.185830 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
10:32:06.186288 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
10:32:06.196745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '51fbc5ce-73e1-4764-b690-d84a060365d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1ea03040>]}
10:32:06.205542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '51fbc5ce-73e1-4764-b690-d84a060365d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1ced8b50>]}
10:32:06.206066 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:32:06.208160 [info ] [MainThread]: 
10:32:06.208968 [debug] [MainThread]: Acquiring new postgres connection "master"
10:32:06.210374 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:32:06.225855 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:32:06.226237 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:32:06.226503 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:32:06.239393 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:32:06.241815 [debug] [ThreadPool]: On list_adludio: Close
10:32:06.243534 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:32:06.252597 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:32:06.252913 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:32:06.253154 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:32:06.263697 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:32:06.264024 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:32:06.264275 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:32:06.267457 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:32:06.269670 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:32:06.270073 [debug] [ThreadPool]: On list_adludio_public: Close
10:32:06.276798 [debug] [MainThread]: Using postgres connection "master"
10:32:06.277130 [debug] [MainThread]: On master: BEGIN
10:32:06.277371 [debug] [MainThread]: Opening a new connection, currently in state init
10:32:06.287868 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:32:06.288184 [debug] [MainThread]: Using postgres connection "master"
10:32:06.288429 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:32:06.312021 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:32:06.314482 [debug] [MainThread]: On master: ROLLBACK
10:32:06.315007 [debug] [MainThread]: Using postgres connection "master"
10:32:06.315278 [debug] [MainThread]: On master: BEGIN
10:32:06.315738 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:32:06.316028 [debug] [MainThread]: On master: COMMIT
10:32:06.316277 [debug] [MainThread]: Using postgres connection "master"
10:32:06.316498 [debug] [MainThread]: On master: COMMIT
10:32:06.316822 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:32:06.317082 [debug] [MainThread]: On master: Close
10:32:06.317742 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:32:06.318742 [info ] [MainThread]: 
10:32:06.323309 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:32:06.323812 [info ] [Thread-1  ]: 1 of 2 START table model public.my_first_dbt_model.............................. [RUN]
10:32:06.324599 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.324863 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:32:06.325132 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:32:06.329019 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:32:06.329555 [debug] [Thread-1  ]: finished collecting timing info
10:32:06.329840 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:32:06.372662 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:32:06.373395 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.373726 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:32:06.373958 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:32:06.384479 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:32:06.384840 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.385094 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select * from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:32:06.391518 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
10:32:06.401480 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.401888 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:32:06.402585 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:32:06.406397 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.406691 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:32:06.407319 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:32:06.423972 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:32:06.424363 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.424623 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:32:06.427532 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:32:06.436737 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:06.437074 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:32:06.439617 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
10:32:06.441504 [debug] [Thread-1  ]: finished collecting timing info
10:32:06.441816 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:32:06.442505 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '51fbc5ce-73e1-4764-b690-d84a060365d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1c02bb20>]}
10:32:06.443053 [info ] [Thread-1  ]: 1 of 2 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2037[0m in 0.12s]
10:32:06.443855 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:32:06.444585 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_second_dbt_model
10:32:06.445292 [info ] [Thread-1  ]: 2 of 2 START view model public.my_second_dbt_model.............................. [RUN]
10:32:06.446011 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:06.446272 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_second_dbt_model
10:32:06.446523 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_second_dbt_model
10:32:06.449895 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:32:06.450384 [debug] [Thread-1  ]: finished collecting timing info
10:32:06.450642 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_second_dbt_model
10:32:06.531460 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:32:06.532134 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:06.532416 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: BEGIN
10:32:06.532649 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:32:06.543104 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:32:06.543461 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:06.543722 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */

  create view "adludio"."public"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "adludio"."public"."my_first_dbt_model"
where id = 1
  );
10:32:06.544381 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "id" does not exist
LINE 8: where id = 1
              ^

10:32:06.544655 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: ROLLBACK
10:32:06.545190 [debug] [Thread-1  ]: finished collecting timing info
10:32:06.545501 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: Close
10:32:06.546100 [debug] [Thread-1  ]: Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  column "id" does not exist
  LINE 8: where id = 1
                ^
  compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:32:06.546574 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '51fbc5ce-73e1-4764-b690-d84a060365d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1c04ebb0>]}
10:32:06.547142 [error] [Thread-1  ]: 2 of 2 ERROR creating view model public.my_second_dbt_model..................... [[31mERROR[0m in 0.10s]
10:32:06.547750 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_second_dbt_model
10:32:06.549125 [debug] [MainThread]: Acquiring new postgres connection "master"
10:32:06.549483 [debug] [MainThread]: Using postgres connection "master"
10:32:06.549737 [debug] [MainThread]: On master: BEGIN
10:32:06.549962 [debug] [MainThread]: Opening a new connection, currently in state closed
10:32:06.560304 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:32:06.560665 [debug] [MainThread]: On master: COMMIT
10:32:06.560919 [debug] [MainThread]: Using postgres connection "master"
10:32:06.561144 [debug] [MainThread]: On master: COMMIT
10:32:06.561516 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:32:06.561790 [debug] [MainThread]: On master: Close
10:32:06.562405 [info ] [MainThread]: 
10:32:06.563372 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0.35s.
10:32:06.563851 [debug] [MainThread]: Connection 'master' was properly closed.
10:32:06.564165 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:32:06.564437 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_second_dbt_model' was properly closed.
10:32:06.571868 [info ] [MainThread]: 
10:32:06.572388 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:32:06.572863 [info ] [MainThread]: 
10:32:06.573286 [error] [MainThread]: [33mDatabase Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)[0m
10:32:06.573717 [error] [MainThread]:   column "id" does not exist
10:32:06.574088 [error] [MainThread]:   LINE 8: where id = 1
10:32:06.574469 [error] [MainThread]:                 ^
10:32:06.574832 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:32:06.575222 [info ] [MainThread]: 
10:32:06.575606 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
10:32:06.576124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1ce621c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1ce62130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1cf8cc40>]}


============================== 2022-03-06 10:32:14.421374 | 27c28a41-c642-4840-af1f-9c50a8516148 ==============================
10:32:14.421374 [info ] [MainThread]: Running with dbt=1.0.3
10:32:14.422800 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:32:14.423241 [debug] [MainThread]: Tracking: tracking
10:32:14.428055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8380fb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8380f460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8380f640>]}
10:32:14.467558 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:32:14.468352 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_second_dbt_model.sql
10:32:14.487413 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
10:32:14.552141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '27c28a41-c642-4840-af1f-9c50a8516148', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8140e0d0>]}
10:32:14.560541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '27c28a41-c642-4840-af1f-9c50a8516148', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8140e6d0>]}
10:32:14.561100 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:32:14.564220 [info ] [MainThread]: 
10:32:14.565469 [debug] [MainThread]: Acquiring new postgres connection "master"
10:32:14.566813 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:32:14.583725 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:32:14.584107 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:32:14.584380 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:32:14.597350 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:32:14.599728 [debug] [ThreadPool]: On list_adludio: Close
10:32:14.601419 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:32:14.662311 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:32:14.662678 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:32:14.662980 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:32:14.673721 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:32:14.674025 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:32:14.674267 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:32:14.677420 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:32:14.679533 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:32:14.679910 [debug] [ThreadPool]: On list_adludio_public: Close
10:32:14.686473 [debug] [MainThread]: Using postgres connection "master"
10:32:14.686758 [debug] [MainThread]: On master: BEGIN
10:32:14.687021 [debug] [MainThread]: Opening a new connection, currently in state init
10:32:14.697536 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:32:14.697841 [debug] [MainThread]: Using postgres connection "master"
10:32:14.698085 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:32:14.722137 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:32:14.724624 [debug] [MainThread]: On master: ROLLBACK
10:32:14.725070 [debug] [MainThread]: Using postgres connection "master"
10:32:14.725337 [debug] [MainThread]: On master: BEGIN
10:32:14.725836 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:32:14.726113 [debug] [MainThread]: On master: COMMIT
10:32:14.726369 [debug] [MainThread]: Using postgres connection "master"
10:32:14.726598 [debug] [MainThread]: On master: COMMIT
10:32:14.726923 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:32:14.727185 [debug] [MainThread]: On master: Close
10:32:14.727783 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:32:14.728841 [info ] [MainThread]: 
10:32:14.733024 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:32:14.733494 [info ] [Thread-1  ]: 1 of 2 START table model public.my_first_dbt_model.............................. [RUN]
10:32:14.734459 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.734728 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:32:14.735017 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:32:14.739133 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:32:14.739660 [debug] [Thread-1  ]: finished collecting timing info
10:32:14.739961 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:32:14.785571 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:32:14.786302 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.787043 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:32:14.787319 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:32:14.797953 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:32:14.798288 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.798527 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select * from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:32:14.804924 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
10:32:14.814635 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.814942 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:32:14.815583 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:32:14.819150 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.819442 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:32:14.820018 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:32:14.836285 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:32:14.836619 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.836869 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:32:14.839552 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:32:14.846700 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:14.846983 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:32:14.850741 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
10:32:14.852493 [debug] [Thread-1  ]: finished collecting timing info
10:32:14.852791 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:32:14.853427 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27c28a41-c642-4840-af1f-9c50a8516148', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc81df69d0>]}
10:32:14.854050 [info ] [Thread-1  ]: 1 of 2 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2037[0m in 0.12s]
10:32:14.855196 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:32:14.856129 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_second_dbt_model
10:32:14.856521 [info ] [Thread-1  ]: 2 of 2 START view model public.my_second_dbt_model.............................. [RUN]
10:32:14.857210 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:14.857492 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_second_dbt_model
10:32:14.857772 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_second_dbt_model
10:32:14.861236 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:32:14.861792 [debug] [Thread-1  ]: finished collecting timing info
10:32:14.862049 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_second_dbt_model
10:32:14.888541 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:32:14.889217 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:14.889542 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: BEGIN
10:32:14.889797 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:32:14.900476 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:32:14.900801 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:14.901053 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */

  create view "adludio"."public"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

-- select *
-- from "adludio"."public"."my_first_dbt_model"
-- where id = 1
  );
10:32:14.901588 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near ")"
LINE 9:   );
          ^

10:32:14.901863 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: ROLLBACK
10:32:14.902392 [debug] [Thread-1  ]: finished collecting timing info
10:32:14.902669 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: Close
10:32:14.903222 [debug] [Thread-1  ]: Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  syntax error at or near ")"
  LINE 9:   );
            ^
  compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:32:14.903689 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27c28a41-c642-4840-af1f-9c50a8516148', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc803e0a00>]}
10:32:14.904259 [error] [Thread-1  ]: 2 of 2 ERROR creating view model public.my_second_dbt_model..................... [[31mERROR[0m in 0.05s]
10:32:14.904946 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_second_dbt_model
10:32:14.906687 [debug] [MainThread]: Acquiring new postgres connection "master"
10:32:14.907025 [debug] [MainThread]: Using postgres connection "master"
10:32:14.907257 [debug] [MainThread]: On master: BEGIN
10:32:14.907469 [debug] [MainThread]: Opening a new connection, currently in state closed
10:32:14.917952 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:32:14.918265 [debug] [MainThread]: On master: COMMIT
10:32:14.918508 [debug] [MainThread]: Using postgres connection "master"
10:32:14.918735 [debug] [MainThread]: On master: COMMIT
10:32:14.919064 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:32:14.919321 [debug] [MainThread]: On master: Close
10:32:14.919911 [info ] [MainThread]: 
10:32:14.920848 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0.35s.
10:32:14.921408 [debug] [MainThread]: Connection 'master' was properly closed.
10:32:14.921915 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:32:14.922143 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_second_dbt_model' was properly closed.
10:32:14.931170 [info ] [MainThread]: 
10:32:14.931664 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:32:14.932139 [info ] [MainThread]: 
10:32:14.932578 [error] [MainThread]: [33mDatabase Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)[0m
10:32:14.932994 [error] [MainThread]:   syntax error at or near ")"
10:32:14.933384 [error] [MainThread]:   LINE 9:   );
10:32:14.933779 [error] [MainThread]:             ^
10:32:14.934140 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:32:14.934526 [info ] [MainThread]: 
10:32:14.934904 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
10:32:14.935416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc81427df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc83814700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc803f8e80>]}


============================== 2022-03-06 10:32:29.225568 | f5163ffc-884e-469b-8557-4572c29dca4d ==============================
10:32:29.225568 [info ] [MainThread]: Running with dbt=1.0.3
10:32:29.226970 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:32:29.227383 [debug] [MainThread]: Tracking: tracking
10:32:29.233004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0962f18730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0962f18ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0962f18670>]}
10:32:29.272370 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
10:32:29.272876 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
10:32:29.283417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f5163ffc-884e-469b-8557-4572c29dca4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0962f201f0>]}
10:32:29.292337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f5163ffc-884e-469b-8557-4572c29dca4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09613f5b50>]}
10:32:29.292855 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:32:29.294953 [info ] [MainThread]: 
10:32:29.295765 [debug] [MainThread]: Acquiring new postgres connection "master"
10:32:29.297135 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:32:29.312602 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:32:29.312979 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:32:29.313242 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:32:29.326181 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:32:29.328535 [debug] [ThreadPool]: On list_adludio: Close
10:32:29.330361 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:32:29.339371 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:32:29.339708 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:32:29.339952 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:32:29.350846 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:32:29.351215 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:32:29.351469 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:32:29.354689 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:32:29.356813 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:32:29.357362 [debug] [ThreadPool]: On list_adludio_public: Close
10:32:29.364032 [debug] [MainThread]: Using postgres connection "master"
10:32:29.364338 [debug] [MainThread]: On master: BEGIN
10:32:29.364570 [debug] [MainThread]: Opening a new connection, currently in state init
10:32:29.375020 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:32:29.375372 [debug] [MainThread]: Using postgres connection "master"
10:32:29.375611 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:32:29.398719 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:32:29.401171 [debug] [MainThread]: On master: ROLLBACK
10:32:29.401760 [debug] [MainThread]: Using postgres connection "master"
10:32:29.402030 [debug] [MainThread]: On master: BEGIN
10:32:29.402477 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:32:29.402739 [debug] [MainThread]: On master: COMMIT
10:32:29.402975 [debug] [MainThread]: Using postgres connection "master"
10:32:29.403195 [debug] [MainThread]: On master: COMMIT
10:32:29.403502 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:32:29.403754 [debug] [MainThread]: On master: Close
10:32:29.404354 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:32:29.405334 [info ] [MainThread]: 
10:32:29.410371 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:32:29.410872 [info ] [Thread-1  ]: 1 of 2 START table model public.my_first_dbt_model.............................. [RUN]
10:32:29.411687 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.411958 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:32:29.412258 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:32:29.416117 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:32:29.416638 [debug] [Thread-1  ]: finished collecting timing info
10:32:29.416917 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:32:29.459811 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:32:29.460518 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.460818 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:32:29.461053 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:32:29.471697 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:32:29.472075 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.472336 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select * from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:32:29.478664 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
10:32:29.488434 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.488782 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:32:29.489484 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:32:29.493096 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.493369 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:32:29.493959 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:32:29.510235 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:32:29.510603 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.510847 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:32:29.513351 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:32:29.522383 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:32:29.522722 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:32:29.525927 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
10:32:29.527740 [debug] [Thread-1  ]: finished collecting timing info
10:32:29.528083 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:32:29.528778 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5163ffc-884e-469b-8557-4572c29dca4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0960395790>]}
10:32:29.529332 [info ] [Thread-1  ]: 1 of 2 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2037[0m in 0.12s]
10:32:29.529941 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:32:29.530838 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_second_dbt_model
10:32:29.531272 [info ] [Thread-1  ]: 2 of 2 START view model public.my_second_dbt_model.............................. [RUN]
10:32:29.531998 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:29.532258 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_second_dbt_model
10:32:29.532522 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_second_dbt_model
10:32:29.535950 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:32:29.536448 [debug] [Thread-1  ]: finished collecting timing info
10:32:29.536703 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_second_dbt_model
10:32:29.615290 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_second_dbt_model"
10:32:29.615985 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:29.616276 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: BEGIN
10:32:29.616511 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:32:29.627188 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:32:29.627536 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_second_dbt_model"
10:32:29.627791 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_second_dbt_model"} */

  create view "adludio"."public"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

-- select *
-- from "adludio"."public"."my_first_dbt_model"
-- where id = 1
  );
10:32:29.628244 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near ")"
LINE 9:   );
          ^

10:32:29.628511 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: ROLLBACK
10:32:29.629035 [debug] [Thread-1  ]: finished collecting timing info
10:32:29.629316 [debug] [Thread-1  ]: On model.Analytics_dbt.my_second_dbt_model: Close
10:32:29.629927 [debug] [Thread-1  ]: Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  syntax error at or near ")"
  LINE 9:   );
            ^
  compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:32:29.630363 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5163ffc-884e-469b-8557-4572c29dca4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f096140e280>]}
10:32:29.630904 [error] [Thread-1  ]: 2 of 2 ERROR creating view model public.my_second_dbt_model..................... [[31mERROR[0m in 0.10s]
10:32:29.631459 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_second_dbt_model
10:32:29.632944 [debug] [MainThread]: Acquiring new postgres connection "master"
10:32:29.633245 [debug] [MainThread]: Using postgres connection "master"
10:32:29.633503 [debug] [MainThread]: On master: BEGIN
10:32:29.633738 [debug] [MainThread]: Opening a new connection, currently in state closed
10:32:29.644209 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:32:29.644561 [debug] [MainThread]: On master: COMMIT
10:32:29.644811 [debug] [MainThread]: Using postgres connection "master"
10:32:29.645039 [debug] [MainThread]: On master: COMMIT
10:32:29.645367 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:32:29.645692 [debug] [MainThread]: On master: Close
10:32:29.646301 [info ] [MainThread]: 
10:32:29.647223 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0.35s.
10:32:29.647669 [debug] [MainThread]: Connection 'master' was properly closed.
10:32:29.647889 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:32:29.648092 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_second_dbt_model' was properly closed.
10:32:29.655677 [info ] [MainThread]: 
10:32:29.656183 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:32:29.656737 [info ] [MainThread]: 
10:32:29.657142 [error] [MainThread]: [33mDatabase Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)[0m
10:32:29.657548 [error] [MainThread]:   syntax error at or near ")"
10:32:29.657926 [error] [MainThread]:   LINE 9:   );
10:32:29.658291 [error] [MainThread]:             ^
10:32:29.658651 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_second_dbt_model.sql
10:32:29.659035 [info ] [MainThread]: 
10:32:29.659460 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
10:32:29.659983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f096137bc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f096036ebb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0962f187c0>]}


============================== 2022-03-06 10:33:30.324247 | c7ba29b2-33a6-44d9-87ab-c9805c6c595e ==============================
10:33:30.324247 [info ] [MainThread]: Running with dbt=1.0.3
10:33:30.325354 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:33:30.325796 [debug] [MainThread]: Tracking: tracking
10:33:30.341325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac934f9730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac934f9ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac934f9670>]}
10:33:30.400888 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 0 files changed.
10:33:30.401468 [debug] [MainThread]: Partial parsing: deleted file: Analytics_dbt://models/example/my_second_dbt_model.sql
10:33:30.414866 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'my_second_dbt_model' in the 'models' section of file 'models/example/schema.yml'
10:33:30.452594 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.Analytics_dbt.unique_my_second_dbt_model_id.57a0f8c493' (models/example/schema.yml) depends on a node named 'my_second_dbt_model' which was not found
10:33:30.453191 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.Analytics_dbt.not_null_my_second_dbt_model_id.151b76d778' (models/example/schema.yml) depends on a node named 'my_second_dbt_model' which was not found
10:33:30.466243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c7ba29b2-33a6-44d9-87ab-c9805c6c595e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac919570d0>]}
10:33:30.474630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c7ba29b2-33a6-44d9-87ab-c9805c6c595e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac91a928b0>]}
10:33:30.475115 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:33:30.477142 [info ] [MainThread]: 
10:33:30.477920 [debug] [MainThread]: Acquiring new postgres connection "master"
10:33:30.479123 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:33:30.494493 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:33:30.494906 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:33:30.495182 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:33:30.511536 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
10:33:30.514050 [debug] [ThreadPool]: On list_adludio: Close
10:33:30.521123 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:33:30.530502 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:33:30.530810 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:33:30.531058 [debug] [ThreadPool]: Opening a new connection, currently in state closed
10:33:30.541837 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:33:30.542154 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:33:30.542395 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:33:30.546519 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:33:30.548831 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:33:30.552387 [debug] [ThreadPool]: On list_adludio_public: Close
10:33:30.558795 [debug] [MainThread]: Using postgres connection "master"
10:33:30.559089 [debug] [MainThread]: On master: BEGIN
10:33:30.559335 [debug] [MainThread]: Opening a new connection, currently in state init
10:33:30.570113 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:33:30.570461 [debug] [MainThread]: Using postgres connection "master"
10:33:30.570743 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:33:30.594850 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:33:30.597771 [debug] [MainThread]: On master: ROLLBACK
10:33:30.598236 [debug] [MainThread]: Using postgres connection "master"
10:33:30.598512 [debug] [MainThread]: On master: BEGIN
10:33:30.598981 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:33:30.599252 [debug] [MainThread]: On master: COMMIT
10:33:30.599510 [debug] [MainThread]: Using postgres connection "master"
10:33:30.599729 [debug] [MainThread]: On master: COMMIT
10:33:30.600051 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:33:30.600331 [debug] [MainThread]: On master: Close
10:33:30.600916 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:33:30.601721 [info ] [MainThread]: 
10:33:30.606014 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:33:30.606470 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:33:30.607471 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.607737 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:33:30.608032 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:33:30.611787 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:33:30.612303 [debug] [Thread-1  ]: finished collecting timing info
10:33:30.612578 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:33:30.707402 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:33:30.708061 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.708338 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:33:30.708562 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:33:30.720330 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:33:30.720681 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.720975 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select * from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:33:30.727520 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
10:33:30.736951 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.737257 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:33:30.737863 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:33:30.741435 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.741732 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:33:30.745283 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:33:30.761862 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:33:30.762169 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.762409 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:33:30.765309 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:33:30.772561 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:33:30.772841 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:33:30.776143 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
10:33:30.777935 [debug] [Thread-1  ]: finished collecting timing info
10:33:30.778241 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:33:30.778886 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c7ba29b2-33a6-44d9-87ab-c9805c6c595e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac918d0c70>]}
10:33:30.779472 [info ] [Thread-1  ]: 1 of 1 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2037[0m in 0.17s]
10:33:30.783927 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:33:30.785280 [debug] [MainThread]: Acquiring new postgres connection "master"
10:33:30.785654 [debug] [MainThread]: Using postgres connection "master"
10:33:30.785897 [debug] [MainThread]: On master: BEGIN
10:33:30.786125 [debug] [MainThread]: Opening a new connection, currently in state closed
10:33:30.793779 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:33:30.793989 [debug] [MainThread]: On master: COMMIT
10:33:30.794138 [debug] [MainThread]: Using postgres connection "master"
10:33:30.794274 [debug] [MainThread]: On master: COMMIT
10:33:30.794765 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:33:30.794949 [debug] [MainThread]: On master: Close
10:33:30.795386 [info ] [MainThread]: 
10:33:30.795763 [info ] [MainThread]: Finished running 1 table model in 0.32s.
10:33:30.796136 [debug] [MainThread]: Connection 'master' was properly closed.
10:33:30.796338 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:33:30.802237 [info ] [MainThread]: 
10:33:30.802712 [info ] [MainThread]: [32mCompleted successfully[0m
10:33:30.803297 [info ] [MainThread]: 
10:33:30.803708 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
10:33:30.804227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac918ef190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac918ef1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac93b27df0>]}


============================== 2022-03-06 10:35:43.873777 | 9297734d-af77-4b03-bc36-cbed21e901dc ==============================
10:35:43.873777 [info ] [MainThread]: Running with dbt=1.0.3
10:35:43.875191 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:35:43.875587 [debug] [MainThread]: Tracking: tracking
10:35:43.880960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b7425730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b7425ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b7425670>]}
10:35:43.925073 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:35:43.925909 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:35:43.945154 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:35:44.016178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9297734d-af77-4b03-bc36-cbed21e901dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b50220d0>]}
10:35:44.024774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9297734d-af77-4b03-bc36-cbed21e901dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b5022eb0>]}
10:35:44.025319 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:35:44.027369 [info ] [MainThread]: 
10:35:44.028323 [debug] [MainThread]: Acquiring new postgres connection "master"
10:35:44.029527 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:35:44.046722 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:35:44.047193 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:35:44.047517 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:35:44.060834 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:35:44.063313 [debug] [ThreadPool]: On list_adludio: Close
10:35:44.067690 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:35:44.146697 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:35:44.147134 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:35:44.147438 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:35:44.158835 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:35:44.159162 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:35:44.159408 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:35:44.162790 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:35:44.164902 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:35:44.165369 [debug] [ThreadPool]: On list_adludio_public: Close
10:35:44.171911 [debug] [MainThread]: Using postgres connection "master"
10:35:44.172191 [debug] [MainThread]: On master: BEGIN
10:35:44.172417 [debug] [MainThread]: Opening a new connection, currently in state init
10:35:44.186492 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:35:44.186830 [debug] [MainThread]: Using postgres connection "master"
10:35:44.187081 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:35:44.203488 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:35:44.205952 [debug] [MainThread]: On master: ROLLBACK
10:35:44.206425 [debug] [MainThread]: Using postgres connection "master"
10:35:44.206689 [debug] [MainThread]: On master: BEGIN
10:35:44.207128 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:35:44.207416 [debug] [MainThread]: On master: COMMIT
10:35:44.207675 [debug] [MainThread]: Using postgres connection "master"
10:35:44.207928 [debug] [MainThread]: On master: COMMIT
10:35:44.208247 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:35:44.208502 [debug] [MainThread]: On master: Close
10:35:44.209067 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:35:44.209872 [info ] [MainThread]: 
10:35:44.214243 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:35:44.214683 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:35:44.215442 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:35:44.215720 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:35:44.215978 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:35:44.219964 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:35:44.220506 [debug] [Thread-1  ]: finished collecting timing info
10:35:44.220824 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:35:44.268057 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:35:44.268804 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:35:44.269631 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:35:44.269916 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:35:44.280615 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:35:44.280985 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:35:44.281353 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select Deal_id from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:35:44.282060 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal_id" does not exist
LINE 18:     select Deal_id from sales_table
                    ^
HINT:  Perhaps you meant to reference the column "sales_table.Deal_id".

10:35:44.282354 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: ROLLBACK
10:35:44.282890 [debug] [Thread-1  ]: finished collecting timing info
10:35:44.283184 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:35:44.283766 [debug] [Thread-1  ]: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  column "deal_id" does not exist
  LINE 18:     select Deal_id from sales_table
                      ^
  HINT:  Perhaps you meant to reference the column "sales_table.Deal_id".
  compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:35:44.284318 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9297734d-af77-4b03-bc36-cbed21e901dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b5a110a0>]}
10:35:44.284893 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.my_first_dbt_model..................... [[31mERROR[0m in 0.07s]
10:35:44.285524 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:35:44.287076 [debug] [MainThread]: Acquiring new postgres connection "master"
10:35:44.287379 [debug] [MainThread]: Using postgres connection "master"
10:35:44.287610 [debug] [MainThread]: On master: BEGIN
10:35:44.287828 [debug] [MainThread]: Opening a new connection, currently in state closed
10:35:44.304489 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
10:35:44.304888 [debug] [MainThread]: On master: COMMIT
10:35:44.305142 [debug] [MainThread]: Using postgres connection "master"
10:35:44.305400 [debug] [MainThread]: On master: COMMIT
10:35:44.305801 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:35:44.306086 [debug] [MainThread]: On master: Close
10:35:44.306743 [info ] [MainThread]: 
10:35:44.307486 [info ] [MainThread]: Finished running 1 table model in 0.28s.
10:35:44.308129 [debug] [MainThread]: Connection 'master' was properly closed.
10:35:44.308434 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:35:44.308659 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:35:44.326011 [info ] [MainThread]: 
10:35:44.326521 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:35:44.327284 [info ] [MainThread]: 
10:35:44.328050 [error] [MainThread]: [33mDatabase Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
10:35:44.328476 [error] [MainThread]:   column "deal_id" does not exist
10:35:44.328867 [error] [MainThread]:   LINE 18:     select Deal_id from sales_table
10:35:44.329292 [error] [MainThread]:                       ^
10:35:44.329786 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "sales_table.Deal_id".
10:35:44.330190 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:35:44.330571 [info ] [MainThread]: 
10:35:44.330959 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
10:35:44.331490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b742a8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b742a310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f41b5a11040>]}


============================== 2022-03-06 10:36:35.521547 | 7799dc36-f9a5-43a7-a811-56e0b3879fb2 ==============================
10:36:35.521547 [info ] [MainThread]: Running with dbt=1.0.3
10:36:35.522471 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:36:35.522828 [debug] [MainThread]: Tracking: tracking
10:36:35.530515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d60f4b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d60f4460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d60f4640>]}
10:36:35.569422 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:36:35.570228 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:36:35.589224 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:36:35.653617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7799dc36-f9a5-43a7-a811-56e0b3879fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d40f0ee0>]}
10:36:35.662022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7799dc36-f9a5-43a7-a811-56e0b3879fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d60f4730>]}
10:36:35.662521 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:36:35.664482 [info ] [MainThread]: 
10:36:35.665254 [debug] [MainThread]: Acquiring new postgres connection "master"
10:36:35.666464 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:36:35.681374 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:36:35.681706 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:36:35.681973 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:36:35.695181 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:36:35.697489 [debug] [ThreadPool]: On list_adludio: Close
10:36:35.699791 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:36:35.709968 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:36:35.710269 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:36:35.710510 [debug] [ThreadPool]: Opening a new connection, currently in state closed
10:36:35.721285 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:36:35.721676 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:36:35.721931 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:36:35.725109 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:36:35.727185 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:36:35.727572 [debug] [ThreadPool]: On list_adludio_public: Close
10:36:35.733710 [debug] [MainThread]: Using postgres connection "master"
10:36:35.733990 [debug] [MainThread]: On master: BEGIN
10:36:35.734234 [debug] [MainThread]: Opening a new connection, currently in state init
10:36:35.744869 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:36:35.745168 [debug] [MainThread]: Using postgres connection "master"
10:36:35.745438 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:36:35.769773 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:36:35.820292 [debug] [MainThread]: On master: ROLLBACK
10:36:35.820880 [debug] [MainThread]: Using postgres connection "master"
10:36:35.821183 [debug] [MainThread]: On master: BEGIN
10:36:35.821704 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:36:35.821982 [debug] [MainThread]: On master: COMMIT
10:36:35.822230 [debug] [MainThread]: Using postgres connection "master"
10:36:35.822447 [debug] [MainThread]: On master: COMMIT
10:36:35.822776 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:36:35.823038 [debug] [MainThread]: On master: Close
10:36:35.823597 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:36:35.824544 [info ] [MainThread]: 
10:36:35.828857 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:36:35.829312 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:36:35.830124 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:36:35.830392 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:36:35.830654 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:36:35.834294 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:36:35.834792 [debug] [Thread-1  ]: finished collecting timing info
10:36:35.835072 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:36:35.877853 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:36:35.878521 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:36:35.878804 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:36:35.879047 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:36:35.890026 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:36:35.890321 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:36:35.890569 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select Deal _Status from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:36:35.891181 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal" does not exist
LINE 18:     select Deal _Status from sales_table
                    ^

10:36:35.891456 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: ROLLBACK
10:36:35.891988 [debug] [Thread-1  ]: finished collecting timing info
10:36:35.892277 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:36:35.892825 [debug] [Thread-1  ]: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  column "deal" does not exist
  LINE 18:     select Deal _Status from sales_table
                      ^
  compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:36:35.893316 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7799dc36-f9a5-43a7-a811-56e0b3879fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d46df8e0>]}
10:36:35.893885 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.my_first_dbt_model..................... [[31mERROR[0m in 0.06s]
10:36:35.894539 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:36:35.896017 [debug] [MainThread]: Acquiring new postgres connection "master"
10:36:35.896332 [debug] [MainThread]: Using postgres connection "master"
10:36:35.896572 [debug] [MainThread]: On master: BEGIN
10:36:35.896787 [debug] [MainThread]: Opening a new connection, currently in state closed
10:36:35.907370 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:36:35.907701 [debug] [MainThread]: On master: COMMIT
10:36:35.907954 [debug] [MainThread]: Using postgres connection "master"
10:36:35.908189 [debug] [MainThread]: On master: COMMIT
10:36:35.908529 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:36:35.908796 [debug] [MainThread]: On master: Close
10:36:35.909429 [info ] [MainThread]: 
10:36:35.910416 [info ] [MainThread]: Finished running 1 table model in 0.24s.
10:36:35.911655 [debug] [MainThread]: Connection 'master' was properly closed.
10:36:35.912156 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:36:35.919273 [info ] [MainThread]: 
10:36:35.919764 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:36:35.920243 [info ] [MainThread]: 
10:36:35.920644 [error] [MainThread]: [33mDatabase Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
10:36:35.921061 [error] [MainThread]:   column "deal" does not exist
10:36:35.921470 [error] [MainThread]:   LINE 18:     select Deal _Status from sales_table
10:36:35.921852 [error] [MainThread]:                       ^
10:36:35.922256 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:36:35.922655 [info ] [MainThread]: 
10:36:35.923048 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
10:36:35.923571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d69f60a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d69f6d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0d46dfd90>]}


============================== 2022-03-06 10:36:55.735378 | 53b1d7eb-9912-4cb0-bad6-f4e8005377dc ==============================
10:36:55.735378 [info ] [MainThread]: Running with dbt=1.0.3
10:36:55.736829 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:36:55.737615 [debug] [MainThread]: Tracking: tracking
10:36:55.743972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cc1115b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cc1115460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cc1115640>]}
10:36:55.825704 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:36:55.826525 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:36:55.848849 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:36:55.929808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '53b1d7eb-9912-4cb0-bad6-f4e8005377dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cbed0fee0>]}
10:36:55.935661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '53b1d7eb-9912-4cb0-bad6-f4e8005377dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cc1115730>]}
10:36:55.936176 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:36:55.939030 [info ] [MainThread]: 
10:36:55.940143 [debug] [MainThread]: Acquiring new postgres connection "master"
10:36:55.941643 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:36:55.957556 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:36:55.957920 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:36:55.958209 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:36:55.973952 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
10:36:55.976373 [debug] [ThreadPool]: On list_adludio: Close
10:36:55.979866 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:36:55.990952 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:36:55.991259 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:36:55.991489 [debug] [ThreadPool]: Opening a new connection, currently in state closed
10:36:56.003123 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:36:56.003442 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:36:56.003695 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:36:56.009634 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:36:56.011778 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:36:56.012175 [debug] [ThreadPool]: On list_adludio_public: Close
10:36:56.020192 [debug] [MainThread]: Using postgres connection "master"
10:36:56.020509 [debug] [MainThread]: On master: BEGIN
10:36:56.020778 [debug] [MainThread]: Opening a new connection, currently in state init
10:36:56.036167 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
10:36:56.036476 [debug] [MainThread]: Using postgres connection "master"
10:36:56.036727 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:36:56.079783 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
10:36:56.142510 [debug] [MainThread]: On master: ROLLBACK
10:36:56.143045 [debug] [MainThread]: Using postgres connection "master"
10:36:56.143374 [debug] [MainThread]: On master: BEGIN
10:36:56.143932 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:36:56.144244 [debug] [MainThread]: On master: COMMIT
10:36:56.144517 [debug] [MainThread]: Using postgres connection "master"
10:36:56.144812 [debug] [MainThread]: On master: COMMIT
10:36:56.148443 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:36:56.148761 [debug] [MainThread]: On master: Close
10:36:56.149388 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:36:56.150381 [info ] [MainThread]: 
10:36:56.154578 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:36:56.155058 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:36:56.155822 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:36:56.156133 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:36:56.156400 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:36:56.160781 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:36:56.161499 [debug] [Thread-1  ]: finished collecting timing info
10:36:56.161803 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:36:56.220505 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:36:56.221225 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:36:56.221545 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:36:56.221779 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:36:56.235037 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:36:56.235419 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:36:56.235690 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select Deal_Status from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:36:56.236340 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal_status" does not exist
LINE 18:     select Deal_Status from sales_table
                    ^
HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".

10:36:56.236622 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: ROLLBACK
10:36:56.237185 [debug] [Thread-1  ]: finished collecting timing info
10:36:56.240949 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:36:56.241613 [debug] [Thread-1  ]: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  column "deal_status" does not exist
  LINE 18:     select Deal_Status from sales_table
                      ^
  HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
  compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:36:56.242105 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '53b1d7eb-9912-4cb0-bad6-f4e8005377dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cbf700940>]}
10:36:56.242681 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.my_first_dbt_model..................... [[31mERROR[0m in 0.09s]
10:36:56.243581 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:36:56.245782 [debug] [MainThread]: Acquiring new postgres connection "master"
10:36:56.246107 [debug] [MainThread]: Using postgres connection "master"
10:36:56.246345 [debug] [MainThread]: On master: BEGIN
10:36:56.246569 [debug] [MainThread]: Opening a new connection, currently in state closed
10:36:56.270595 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
10:36:56.270947 [debug] [MainThread]: On master: COMMIT
10:36:56.271205 [debug] [MainThread]: Using postgres connection "master"
10:36:56.271437 [debug] [MainThread]: On master: COMMIT
10:36:56.271769 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:36:56.272038 [debug] [MainThread]: On master: Close
10:36:56.272622 [info ] [MainThread]: 
10:36:56.273348 [info ] [MainThread]: Finished running 1 table model in 0.33s.
10:36:56.274006 [debug] [MainThread]: Connection 'master' was properly closed.
10:36:56.274276 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:36:56.282237 [info ] [MainThread]: 
10:36:56.282710 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:36:56.283203 [info ] [MainThread]: 
10:36:56.283625 [error] [MainThread]: [33mDatabase Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
10:36:56.284047 [error] [MainThread]:   column "deal_status" does not exist
10:36:56.284452 [error] [MainThread]:   LINE 18:     select Deal_Status from sales_table
10:36:56.284862 [error] [MainThread]:                       ^
10:36:56.285325 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
10:36:56.285886 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:36:56.286324 [info ] [MainThread]: 
10:36:56.286732 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
10:36:56.287281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cc1a170a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cc1a17d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cbf700460>]}


============================== 2022-03-06 10:37:14.619915 | c7fe64a4-6df0-48cd-8852-ab8d46360eb9 ==============================
10:37:14.619915 [info ] [MainThread]: Running with dbt=1.0.3
10:37:14.628289 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:37:14.628756 [debug] [MainThread]: Tracking: tracking
10:37:14.634185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19f0c03b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19f0c03460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19f0c03640>]}
10:37:14.709865 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:37:14.710685 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:37:14.739319 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:37:14.820486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c7fe64a4-6df0-48cd-8852-ab8d46360eb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19ee7feee0>]}
10:37:14.829358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c7fe64a4-6df0-48cd-8852-ab8d46360eb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19f0c03730>]}
10:37:14.829914 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:37:14.831973 [info ] [MainThread]: 
10:37:14.832797 [debug] [MainThread]: Acquiring new postgres connection "master"
10:37:14.834040 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:37:14.861204 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:37:14.861600 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:37:14.861896 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:37:14.875434 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:37:14.877928 [debug] [ThreadPool]: On list_adludio: Close
10:37:14.882971 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:37:14.908900 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:37:14.909251 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:37:14.909508 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:37:14.929910 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
10:37:14.930286 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:37:14.930542 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:37:14.937884 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:37:14.940142 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:37:14.940584 [debug] [ThreadPool]: On list_adludio_public: Close
10:37:15.003789 [debug] [MainThread]: Using postgres connection "master"
10:37:15.004190 [debug] [MainThread]: On master: BEGIN
10:37:15.004473 [debug] [MainThread]: Opening a new connection, currently in state init
10:37:15.016043 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:37:15.016455 [debug] [MainThread]: Using postgres connection "master"
10:37:15.016730 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:37:15.044708 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
10:37:15.047348 [debug] [MainThread]: On master: ROLLBACK
10:37:15.047807 [debug] [MainThread]: Using postgres connection "master"
10:37:15.048075 [debug] [MainThread]: On master: BEGIN
10:37:15.048568 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:37:15.048826 [debug] [MainThread]: On master: COMMIT
10:37:15.049055 [debug] [MainThread]: Using postgres connection "master"
10:37:15.049290 [debug] [MainThread]: On master: COMMIT
10:37:15.049737 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:37:15.050003 [debug] [MainThread]: On master: Close
10:37:15.050577 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:37:15.052992 [info ] [MainThread]: 
10:37:15.070104 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:37:15.070879 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:37:15.075582 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:37:15.075882 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:37:15.076297 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:37:15.080253 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:37:15.080764 [debug] [Thread-1  ]: finished collecting timing info
10:37:15.081127 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:37:15.144562 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:37:15.145251 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:37:15.145578 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:37:15.145821 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:37:15.156678 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:37:15.156990 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:37:15.157223 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Status" from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:37:15.157886 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "Deal_Status" does not exist
LINE 18:     select "Deal_Status" from sales_table
                    ^
HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".

10:37:15.158165 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: ROLLBACK
10:37:15.158729 [debug] [Thread-1  ]: finished collecting timing info
10:37:15.159014 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:37:15.159616 [debug] [Thread-1  ]: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  column "Deal_Status" does not exist
  LINE 18:     select "Deal_Status" from sales_table
                      ^
  HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
  compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:37:15.160084 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c7fe64a4-6df0-48cd-8852-ab8d46360eb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19ef0936d0>]}
10:37:15.160647 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.my_first_dbt_model..................... [[31mERROR[0m in 0.09s]
10:37:15.161231 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:37:15.165918 [debug] [MainThread]: Acquiring new postgres connection "master"
10:37:15.166218 [debug] [MainThread]: Using postgres connection "master"
10:37:15.166449 [debug] [MainThread]: On master: BEGIN
10:37:15.166666 [debug] [MainThread]: Opening a new connection, currently in state closed
10:37:15.184729 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
10:37:15.185154 [debug] [MainThread]: On master: COMMIT
10:37:15.185448 [debug] [MainThread]: Using postgres connection "master"
10:37:15.185758 [debug] [MainThread]: On master: COMMIT
10:37:15.186132 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:37:15.186426 [debug] [MainThread]: On master: Close
10:37:15.187071 [info ] [MainThread]: 
10:37:15.188049 [info ] [MainThread]: Finished running 1 table model in 0.35s.
10:37:15.189203 [debug] [MainThread]: Connection 'master' was properly closed.
10:37:15.189510 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:37:15.189775 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:37:15.204323 [info ] [MainThread]: 
10:37:15.205309 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:37:15.206460 [info ] [MainThread]: 
10:37:15.207171 [error] [MainThread]: [33mDatabase Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
10:37:15.207644 [error] [MainThread]:   column "Deal_Status" does not exist
10:37:15.208393 [error] [MainThread]:   LINE 18:     select "Deal_Status" from sales_table
10:37:15.208983 [error] [MainThread]:                       ^
10:37:15.210078 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
10:37:15.210918 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:37:15.211796 [info ] [MainThread]: 
10:37:15.212484 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
10:37:15.213385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19f150cb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19f150c940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19ef0859d0>]}


============================== 2022-03-06 10:37:44.097140 | 33d81efc-9345-4a00-9042-3589ba2eeb17 ==============================
10:37:44.097140 [info ] [MainThread]: Running with dbt=1.0.3
10:37:44.097995 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:37:44.098346 [debug] [MainThread]: Tracking: tracking
10:37:44.104038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff366663b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff366663460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff366663640>]}
10:37:44.143004 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:37:44.143836 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:37:44.162869 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:37:44.227934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '33d81efc-9345-4a00-9042-3589ba2eeb17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff36425dee0>]}
10:37:44.236292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '33d81efc-9345-4a00-9042-3589ba2eeb17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff366663730>]}
10:37:44.236793 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:37:44.238761 [info ] [MainThread]: 
10:37:44.239652 [debug] [MainThread]: Acquiring new postgres connection "master"
10:37:44.240856 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:37:44.255955 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:37:44.256305 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:37:44.256570 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:37:44.269776 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:37:44.272080 [debug] [ThreadPool]: On list_adludio: Close
10:37:44.273738 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:37:44.284135 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:37:44.284456 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:37:44.284693 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:37:44.295290 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:37:44.295617 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:37:44.295860 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:37:44.299054 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:37:44.301181 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:37:44.301600 [debug] [ThreadPool]: On list_adludio_public: Close
10:37:44.361979 [debug] [MainThread]: Using postgres connection "master"
10:37:44.362398 [debug] [MainThread]: On master: BEGIN
10:37:44.362659 [debug] [MainThread]: Opening a new connection, currently in state init
10:37:44.373239 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:37:44.373624 [debug] [MainThread]: Using postgres connection "master"
10:37:44.373882 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:37:44.397320 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:37:44.399847 [debug] [MainThread]: On master: ROLLBACK
10:37:44.400341 [debug] [MainThread]: Using postgres connection "master"
10:37:44.400599 [debug] [MainThread]: On master: BEGIN
10:37:44.401055 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:37:44.401322 [debug] [MainThread]: On master: COMMIT
10:37:44.401625 [debug] [MainThread]: Using postgres connection "master"
10:37:44.401869 [debug] [MainThread]: On master: COMMIT
10:37:44.402208 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:37:44.402469 [debug] [MainThread]: On master: Close
10:37:44.403068 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:37:44.404085 [info ] [MainThread]: 
10:37:44.408780 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:37:44.409275 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:37:44.410074 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:37:44.410335 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:37:44.410600 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:37:44.414421 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:37:44.415061 [debug] [Thread-1  ]: finished collecting timing info
10:37:44.415429 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:37:44.458727 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:37:44.459465 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:37:44.459763 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:37:44.459996 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:37:44.470640 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:37:44.471010 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:37:44.471261 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select sales_table."Deal_Status" from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:37:44.472003 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column sales_table.Deal_Status does not exist
LINE 18:     select sales_table."Deal_Status" from sales_table
                    ^
HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".

10:37:44.472275 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: ROLLBACK
10:37:44.472809 [debug] [Thread-1  ]: finished collecting timing info
10:37:44.473100 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:37:44.473697 [debug] [Thread-1  ]: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  column sales_table.Deal_Status does not exist
  LINE 18:     select sales_table."Deal_Status" from sales_table
                      ^
  HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
  compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:37:44.474164 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '33d81efc-9345-4a00-9042-3589ba2eeb17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff364af1760>]}
10:37:44.474727 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.my_first_dbt_model..................... [[31mERROR[0m in 0.06s]
10:37:44.475288 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:37:44.476981 [debug] [MainThread]: Acquiring new postgres connection "master"
10:37:44.477283 [debug] [MainThread]: Using postgres connection "master"
10:37:44.477564 [debug] [MainThread]: On master: BEGIN
10:37:44.477801 [debug] [MainThread]: Opening a new connection, currently in state closed
10:37:44.488280 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:37:44.488749 [debug] [MainThread]: On master: COMMIT
10:37:44.489054 [debug] [MainThread]: Using postgres connection "master"
10:37:44.489312 [debug] [MainThread]: On master: COMMIT
10:37:44.489711 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:37:44.489981 [debug] [MainThread]: On master: Close
10:37:44.490614 [info ] [MainThread]: 
10:37:44.491508 [info ] [MainThread]: Finished running 1 table model in 0.25s.
10:37:44.492311 [debug] [MainThread]: Connection 'master' was properly closed.
10:37:44.492559 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:37:44.492770 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:37:44.500431 [info ] [MainThread]: 
10:37:44.500936 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:37:44.501664 [info ] [MainThread]: 
10:37:44.502331 [error] [MainThread]: [33mDatabase Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
10:37:44.502753 [error] [MainThread]:   column sales_table.Deal_Status does not exist
10:37:44.503144 [error] [MainThread]:   LINE 18:     select sales_table."Deal_Status" from sales_table
10:37:44.503527 [error] [MainThread]:                       ^
10:37:44.503895 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
10:37:44.504294 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:37:44.504702 [info ] [MainThread]: 
10:37:44.505091 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
10:37:44.505747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff36428ae20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff36428a910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff364af1e80>]}


============================== 2022-03-06 10:38:08.471909 | 42e71d29-0d4e-4a40-885f-6117bf53b8a1 ==============================
10:38:08.471909 [info ] [MainThread]: Running with dbt=1.0.3
10:38:08.473004 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:38:08.473437 [debug] [MainThread]: Tracking: tracking
10:38:08.485286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c05897b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c05897460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c05897640>]}
10:38:08.525240 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
10:38:08.525720 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
10:38:08.543131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '42e71d29-0d4e-4a40-885f-6117bf53b8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c03d230d0>]}
10:38:08.558696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '42e71d29-0d4e-4a40-885f-6117bf53b8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c03d71850>]}
10:38:08.559245 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:38:08.562163 [info ] [MainThread]: 
10:38:08.563350 [debug] [MainThread]: Acquiring new postgres connection "master"
10:38:08.565036 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:38:08.580424 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:38:08.580809 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:38:08.581101 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:38:08.593899 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:38:08.596195 [debug] [ThreadPool]: On list_adludio: Close
10:38:08.598364 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:38:08.607510 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:38:08.607861 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:38:08.608138 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:38:08.622468 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:38:08.622889 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:38:08.623167 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:38:08.626714 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:38:08.639115 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:38:08.639602 [debug] [ThreadPool]: On list_adludio_public: Close
10:38:08.646704 [debug] [MainThread]: Using postgres connection "master"
10:38:08.647047 [debug] [MainThread]: On master: BEGIN
10:38:08.647286 [debug] [MainThread]: Opening a new connection, currently in state init
10:38:08.658233 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:38:08.658547 [debug] [MainThread]: Using postgres connection "master"
10:38:08.658795 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:38:08.683505 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
10:38:08.686046 [debug] [MainThread]: On master: ROLLBACK
10:38:08.686490 [debug] [MainThread]: Using postgres connection "master"
10:38:08.686771 [debug] [MainThread]: On master: BEGIN
10:38:08.687229 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:38:08.687508 [debug] [MainThread]: On master: COMMIT
10:38:08.687756 [debug] [MainThread]: Using postgres connection "master"
10:38:08.688004 [debug] [MainThread]: On master: COMMIT
10:38:08.688338 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:38:08.688601 [debug] [MainThread]: On master: Close
10:38:08.689202 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:38:08.690251 [info ] [MainThread]: 
10:38:08.702065 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:38:08.702571 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:38:08.703369 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:08.703681 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:38:08.703970 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:38:08.711722 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:38:08.712282 [debug] [Thread-1  ]: finished collecting timing info
10:38:08.712567 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:38:08.767257 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:38:08.767973 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:08.768293 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:38:08.768577 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:38:08.780245 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:38:08.780548 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:08.780781 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select sales_table."Deal_Status" from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:38:08.781503 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column sales_table.Deal_Status does not exist
LINE 18:     select sales_table."Deal_Status" from sales_table
                    ^
HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".

10:38:08.781774 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: ROLLBACK
10:38:08.782298 [debug] [Thread-1  ]: finished collecting timing info
10:38:08.782593 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:38:08.783230 [debug] [Thread-1  ]: Database Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  column sales_table.Deal_Status does not exist
  LINE 18:     select sales_table."Deal_Status" from sales_table
                      ^
  HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
  compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:38:08.783735 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '42e71d29-0d4e-4a40-885f-6117bf53b8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c00ec4a90>]}
10:38:08.784505 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.my_first_dbt_model..................... [[31mERROR[0m in 0.08s]
10:38:08.785129 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:38:08.786771 [debug] [MainThread]: Acquiring new postgres connection "master"
10:38:08.787097 [debug] [MainThread]: Using postgres connection "master"
10:38:08.787358 [debug] [MainThread]: On master: BEGIN
10:38:08.787580 [debug] [MainThread]: Opening a new connection, currently in state closed
10:38:08.801380 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:38:08.801867 [debug] [MainThread]: On master: COMMIT
10:38:08.802184 [debug] [MainThread]: Using postgres connection "master"
10:38:08.802453 [debug] [MainThread]: On master: COMMIT
10:38:08.802816 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:38:08.803161 [debug] [MainThread]: On master: Close
10:38:08.803785 [info ] [MainThread]: 
10:38:08.804739 [info ] [MainThread]: Finished running 1 table model in 0.24s.
10:38:08.805245 [debug] [MainThread]: Connection 'master' was properly closed.
10:38:08.805692 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:38:08.805936 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:38:08.813061 [info ] [MainThread]: 
10:38:08.815344 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
10:38:08.815893 [info ] [MainThread]: 
10:38:08.816689 [error] [MainThread]: [33mDatabase Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
10:38:08.817299 [error] [MainThread]:   column sales_table.Deal_Status does not exist
10:38:08.818035 [error] [MainThread]:   LINE 18:     select sales_table."Deal_Status" from sales_table
10:38:08.818576 [error] [MainThread]:                       ^
10:38:08.818992 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "sales_table.Deal _Status".
10:38:08.819414 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/example/my_first_dbt_model.sql
10:38:08.819980 [info ] [MainThread]: 
10:38:08.820456 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
10:38:08.821141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c03d05fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c03cf4c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c00ec4970>]}


============================== 2022-03-06 10:38:20.107805 | 515daeb8-dd24-495b-8d3e-c23ee5561df8 ==============================
10:38:20.107805 [info ] [MainThread]: Running with dbt=1.0.3
10:38:20.109483 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
10:38:20.109913 [debug] [MainThread]: Tracking: tracking
10:38:20.119570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afa2b8370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afa2b8160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afa2b8340>]}
10:38:20.178441 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
10:38:20.179717 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/example/my_first_dbt_model.sql
10:38:20.203225 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
10:38:20.287619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '515daeb8-dd24-495b-8d3e-c23ee5561df8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3af80ca0d0>]}
10:38:20.296196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '515daeb8-dd24-495b-8d3e-c23ee5561df8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afa2b8af0>]}
10:38:20.296733 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
10:38:20.298738 [info ] [MainThread]: 
10:38:20.299535 [debug] [MainThread]: Acquiring new postgres connection "master"
10:38:20.300792 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
10:38:20.318597 [debug] [ThreadPool]: Using postgres connection "list_adludio"
10:38:20.319021 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
10:38:20.319332 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:38:20.332923 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
10:38:20.335337 [debug] [ThreadPool]: On list_adludio: Close
10:38:20.337804 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
10:38:20.348384 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:38:20.348707 [debug] [ThreadPool]: On list_adludio_public: BEGIN
10:38:20.348959 [debug] [ThreadPool]: Opening a new connection, currently in state init
10:38:20.360442 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
10:38:20.364002 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
10:38:20.364253 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
10:38:20.367480 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
10:38:20.369652 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
10:38:20.370075 [debug] [ThreadPool]: On list_adludio_public: Close
10:38:20.438881 [debug] [MainThread]: Using postgres connection "master"
10:38:20.439261 [debug] [MainThread]: On master: BEGIN
10:38:20.439526 [debug] [MainThread]: Opening a new connection, currently in state init
10:38:20.449662 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:38:20.450000 [debug] [MainThread]: Using postgres connection "master"
10:38:20.450253 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
10:38:20.476587 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
10:38:20.479102 [debug] [MainThread]: On master: ROLLBACK
10:38:20.479604 [debug] [MainThread]: Using postgres connection "master"
10:38:20.479871 [debug] [MainThread]: On master: BEGIN
10:38:20.480316 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
10:38:20.480587 [debug] [MainThread]: On master: COMMIT
10:38:20.480852 [debug] [MainThread]: Using postgres connection "master"
10:38:20.481084 [debug] [MainThread]: On master: COMMIT
10:38:20.481403 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:38:20.481688 [debug] [MainThread]: On master: Close
10:38:20.482270 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
10:38:20.482786 [info ] [MainThread]: 
10:38:20.489614 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.my_first_dbt_model
10:38:20.490104 [info ] [Thread-1  ]: 1 of 1 START table model public.my_first_dbt_model.............................. [RUN]
10:38:20.490884 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.491152 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.my_first_dbt_model
10:38:20.491423 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.my_first_dbt_model
10:38:20.495250 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:38:20.495748 [debug] [Thread-1  ]: finished collecting timing info
10:38:20.496031 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.my_first_dbt_model
10:38:20.561926 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.my_first_dbt_model"
10:38:20.562836 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.563127 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: BEGIN
10:38:20.563357 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
10:38:20.574589 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
10:38:20.574905 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.575145 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */


  create  table "adludio"."public"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal _Status" from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
10:38:20.580853 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
10:38:20.590796 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.591104 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
10:38:20.591676 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:38:20.595303 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.595577 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
alter table "adludio"."public"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
10:38:20.596107 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
10:38:20.614489 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:38:20.614844 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.615104 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: COMMIT
10:38:20.617240 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
10:38:20.626104 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.my_first_dbt_model"
10:38:20.626404 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.my_first_dbt_model"} */
drop table if exists "adludio"."public"."my_first_dbt_model__dbt_backup" cascade
10:38:20.629543 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
10:38:20.631419 [debug] [Thread-1  ]: finished collecting timing info
10:38:20.631713 [debug] [Thread-1  ]: On model.Analytics_dbt.my_first_dbt_model: Close
10:38:20.634193 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '515daeb8-dd24-495b-8d3e-c23ee5561df8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3af80fd460>]}
10:38:20.634725 [info ] [Thread-1  ]: 1 of 1 OK created table model public.my_first_dbt_model......................... [[32mSELECT 2037[0m in 0.14s]
10:38:20.635292 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.my_first_dbt_model
10:38:20.636867 [debug] [MainThread]: Acquiring new postgres connection "master"
10:38:20.637172 [debug] [MainThread]: Using postgres connection "master"
10:38:20.637408 [debug] [MainThread]: On master: BEGIN
10:38:20.637667 [debug] [MainThread]: Opening a new connection, currently in state closed
10:38:20.648503 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
10:38:20.648812 [debug] [MainThread]: On master: COMMIT
10:38:20.649062 [debug] [MainThread]: Using postgres connection "master"
10:38:20.649325 [debug] [MainThread]: On master: COMMIT
10:38:20.649712 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
10:38:20.649979 [debug] [MainThread]: On master: Close
10:38:20.650567 [info ] [MainThread]: 
10:38:20.651446 [info ] [MainThread]: Finished running 1 table model in 0.35s.
10:38:20.651966 [debug] [MainThread]: Connection 'master' was properly closed.
10:38:20.652249 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
10:38:20.652490 [debug] [MainThread]: Connection 'model.Analytics_dbt.my_first_dbt_model' was properly closed.
10:38:20.662947 [info ] [MainThread]: 
10:38:20.663520 [info ] [MainThread]: [32mCompleted successfully[0m
10:38:20.664096 [info ] [MainThread]: 
10:38:20.664534 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
10:38:20.665128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3af87a3700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afa8f77c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3afa8fcca0>]}


============================== 2022-03-06 11:32:11.237269 | 20c16366-0478-4d45-b105-06ba5c66ac27 ==============================
11:32:11.237269 [info ] [MainThread]: Running with dbt=1.0.3
11:32:11.238149 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:32:11.238497 [debug] [MainThread]: Tracking: tracking
11:32:11.243504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a49fa190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a49fa370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a49fa7c0>]}
11:32:11.283191 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 2 files added, 0 files changed.
11:32:11.283974 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/schema.yml
11:32:11.284427 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
11:32:11.284819 [debug] [MainThread]: Partial parsing: deleted file: Analytics_dbt://models/example/my_first_dbt_model.sql
11:32:11.303847 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
11:32:11.335403 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'my_first_dbt_model' in the 'models' section of file 'models/Sales Numbers/schema.yml'
11:32:11.338196 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'my_second_dbt_model' in the 'models' section of file 'models/Sales Numbers/schema.yml'
11:32:11.367033 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.Analytics_dbt.unique_my_first_dbt_model_id.16e066b321' (models/Sales Numbers/schema.yml) depends on a node named 'my_first_dbt_model' which was not found
11:32:11.367601 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.Analytics_dbt.not_null_my_first_dbt_model_id.5fb22c2710' (models/Sales Numbers/schema.yml) depends on a node named 'my_first_dbt_model' which was not found
11:32:11.368097 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.Analytics_dbt.unique_my_second_dbt_model_id.57a0f8c493' (models/Sales Numbers/schema.yml) depends on a node named 'my_second_dbt_model' which was not found
11:32:11.368530 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.Analytics_dbt.not_null_my_second_dbt_model_id.151b76d778' (models/Sales Numbers/schema.yml) depends on a node named 'my_second_dbt_model' which was not found
11:32:11.374805 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:32:11.382714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '20c16366-0478-4d45-b105-06ba5c66ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a40690d0>]}
11:32:11.391373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '20c16366-0478-4d45-b105-06ba5c66ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a49fac70>]}
11:32:11.391875 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:32:11.393856 [info ] [MainThread]: 
11:32:11.394647 [debug] [MainThread]: Acquiring new postgres connection "master"
11:32:11.395971 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:32:11.412464 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:32:11.412857 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:32:11.413137 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:32:11.426100 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
11:32:11.428488 [debug] [ThreadPool]: On list_adludio: Close
11:32:11.430199 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:32:11.491339 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:32:11.491703 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:32:11.491963 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:32:11.502674 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:32:11.502981 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:32:11.503219 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:32:11.506418 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.0 seconds
11:32:11.508498 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:32:11.508883 [debug] [ThreadPool]: On list_adludio_public: Close
11:32:11.515543 [debug] [MainThread]: Using postgres connection "master"
11:32:11.515851 [debug] [MainThread]: On master: BEGIN
11:32:11.516087 [debug] [MainThread]: Opening a new connection, currently in state init
11:32:11.526677 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:32:11.527014 [debug] [MainThread]: Using postgres connection "master"
11:32:11.527257 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:32:11.550992 [debug] [MainThread]: SQL status: SELECT 4 in 0.02 seconds
11:32:11.553525 [debug] [MainThread]: On master: ROLLBACK
11:32:11.554010 [debug] [MainThread]: Using postgres connection "master"
11:32:11.554262 [debug] [MainThread]: On master: BEGIN
11:32:11.554730 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:32:11.555014 [debug] [MainThread]: On master: COMMIT
11:32:11.555255 [debug] [MainThread]: Using postgres connection "master"
11:32:11.555508 [debug] [MainThread]: On master: COMMIT
11:32:11.555849 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:32:11.556112 [debug] [MainThread]: On master: Close
11:32:11.556691 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:32:11.557715 [info ] [MainThread]: 
11:32:11.562300 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:32:11.562777 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:32:11.563768 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:32:11.564034 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:32:11.564296 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:32:11.568161 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:32:11.568685 [debug] [Thread-1  ]: finished collecting timing info
11:32:11.568981 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:32:11.611958 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:32:11.612676 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:32:11.612976 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:32:11.613214 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:32:11.623719 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:32:11.624096 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:32:11.624361 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value", DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:32:11.630161 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
11:32:11.639772 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:32:11.640107 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:32:11.640748 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:32:11.656964 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:32:11.657330 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:32:11.657610 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:32:11.659488 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:32:11.666708 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:32:11.667027 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:32:11.667546 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:32:11.669637 [debug] [Thread-1  ]: finished collecting timing info
11:32:11.669963 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:32:11.670686 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20c16366-0478-4d45-b105-06ba5c66ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a4957a90>]}
11:32:11.671226 [info ] [Thread-1  ]: 1 of 1 OK created table model public.deal_value_per_week........................ [[32mSELECT 2037[0m in 0.11s]
11:32:11.672247 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:32:11.673990 [debug] [MainThread]: Acquiring new postgres connection "master"
11:32:11.674313 [debug] [MainThread]: Using postgres connection "master"
11:32:11.674577 [debug] [MainThread]: On master: BEGIN
11:32:11.674797 [debug] [MainThread]: Opening a new connection, currently in state closed
11:32:11.685205 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:32:11.685621 [debug] [MainThread]: On master: COMMIT
11:32:11.685919 [debug] [MainThread]: Using postgres connection "master"
11:32:11.686157 [debug] [MainThread]: On master: COMMIT
11:32:11.686500 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:32:11.686762 [debug] [MainThread]: On master: Close
11:32:11.687356 [info ] [MainThread]: 
11:32:11.688063 [info ] [MainThread]: Finished running 1 table model in 0.29s.
11:32:11.688738 [debug] [MainThread]: Connection 'master' was properly closed.
11:32:11.688979 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:32:11.689219 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:32:11.696553 [info ] [MainThread]: 
11:32:11.697055 [info ] [MainThread]: [32mCompleted successfully[0m
11:32:11.697569 [info ] [MainThread]: 
11:32:11.697984 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
11:32:11.698621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a6448a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a49f2b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26a49e1160>]}


============================== 2022-03-06 11:37:12.923275 | 949dc2f4-c414-41f1-844f-1de23f5b7c83 ==============================
11:37:12.923275 [info ] [MainThread]: Running with dbt=1.0.3
11:37:12.924106 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:37:12.924445 [debug] [MainThread]: Tracking: tracking
11:37:12.929581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec527f460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec527f970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec527f070>]}
11:37:12.983391 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:37:12.984221 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
11:37:13.009790 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
11:37:13.044647 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:37:13.052861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '949dc2f4-c414-41f1-844f-1de23f5b7c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec2ec10d0>]}
11:37:13.070260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '949dc2f4-c414-41f1-844f-1de23f5b7c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec3809610>]}
11:37:13.070806 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:37:13.072820 [info ] [MainThread]: 
11:37:13.073677 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:13.074897 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:37:13.090730 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:37:13.091108 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:37:13.091358 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:13.109369 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
11:37:13.112237 [debug] [ThreadPool]: On list_adludio: Close
11:37:13.123319 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:37:13.132980 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:13.133352 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:37:13.133701 [debug] [ThreadPool]: Opening a new connection, currently in state closed
11:37:13.144920 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:37:13.145299 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:13.145608 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:37:13.153706 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.01 seconds
11:37:13.155952 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:37:13.156427 [debug] [ThreadPool]: On list_adludio_public: Close
11:37:13.164603 [debug] [MainThread]: Using postgres connection "master"
11:37:13.164981 [debug] [MainThread]: On master: BEGIN
11:37:13.165249 [debug] [MainThread]: Opening a new connection, currently in state init
11:37:13.185032 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:37:13.185516 [debug] [MainThread]: Using postgres connection "master"
11:37:13.185821 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:37:13.213993 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:37:13.216832 [debug] [MainThread]: On master: ROLLBACK
11:37:13.217411 [debug] [MainThread]: Using postgres connection "master"
11:37:13.217723 [debug] [MainThread]: On master: BEGIN
11:37:13.218168 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:37:13.218454 [debug] [MainThread]: On master: COMMIT
11:37:13.218694 [debug] [MainThread]: Using postgres connection "master"
11:37:13.218904 [debug] [MainThread]: On master: COMMIT
11:37:13.219231 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:13.219484 [debug] [MainThread]: On master: Close
11:37:13.222288 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:37:13.222783 [info ] [MainThread]: 
11:37:13.226796 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:37:13.227328 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:37:13.228374 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:13.228640 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:37:13.228902 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:37:13.232716 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:13.233275 [debug] [Thread-1  ]: finished collecting timing info
11:37:13.233589 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:37:13.357881 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:13.358595 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:13.358891 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:37:13.359120 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:37:13.373026 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:37:13.373396 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:13.373724 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value", DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
ORDER BY week ASC
group by week O

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:37:13.374267 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "group"
LINE 24: group by week O
         ^

11:37:13.374545 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
11:37:13.375084 [debug] [Thread-1  ]: finished collecting timing info
11:37:13.375362 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:37:13.375932 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "group"
  LINE 24: group by week O
           ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:13.379875 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '949dc2f4-c414-41f1-844f-1de23f5b7c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec3855670>]}
11:37:13.380470 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.15s]
11:37:13.381036 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:37:13.382768 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:13.383090 [debug] [MainThread]: Using postgres connection "master"
11:37:13.383323 [debug] [MainThread]: On master: BEGIN
11:37:13.383532 [debug] [MainThread]: Opening a new connection, currently in state closed
11:37:13.394097 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:37:13.394513 [debug] [MainThread]: On master: COMMIT
11:37:13.394787 [debug] [MainThread]: Using postgres connection "master"
11:37:13.395026 [debug] [MainThread]: On master: COMMIT
11:37:13.395381 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:13.395647 [debug] [MainThread]: On master: Close
11:37:13.396273 [info ] [MainThread]: 
11:37:13.397010 [info ] [MainThread]: Finished running 1 table model in 0.32s.
11:37:13.398520 [debug] [MainThread]: Connection 'master' was properly closed.
11:37:13.398798 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:37:13.410256 [info ] [MainThread]: 
11:37:13.411067 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
11:37:13.411731 [info ] [MainThread]: 
11:37:13.412295 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
11:37:13.412855 [error] [MainThread]:   syntax error at or near "group"
11:37:13.413395 [error] [MainThread]:   LINE 24: group by week O
11:37:13.413954 [error] [MainThread]:            ^
11:37:13.414492 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:13.414941 [info ] [MainThread]: 
11:37:13.415333 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
11:37:13.415893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec1e99a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec1e999a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feec3855ee0>]}


============================== 2022-03-06 11:37:31.272660 | 122eacb6-708c-438d-97d4-448021060007 ==============================
11:37:31.272660 [info ] [MainThread]: Running with dbt=1.0.3
11:37:31.274045 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:37:31.274500 [debug] [MainThread]: Tracking: tracking
11:37:31.284338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c868e730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c868eac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c868e670>]}
11:37:31.341234 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
11:37:31.341673 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
11:37:31.342310 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:37:31.352841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '122eacb6-708c-438d-97d4-448021060007', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c6b1b0d0>]}
11:37:31.364174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '122eacb6-708c-438d-97d4-448021060007', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c6b65f10>]}
11:37:31.364787 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:37:31.366990 [info ] [MainThread]: 
11:37:31.367831 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:31.369200 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:37:31.384963 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:37:31.385396 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:37:31.385737 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:31.401258 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
11:37:31.403777 [debug] [ThreadPool]: On list_adludio: Close
11:37:31.405596 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:37:31.418977 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:31.419618 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:37:31.419958 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:31.430930 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:37:31.431365 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:31.431665 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:37:31.434930 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.0 seconds
11:37:31.437296 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:37:31.437783 [debug] [ThreadPool]: On list_adludio_public: Close
11:37:31.444570 [debug] [MainThread]: Using postgres connection "master"
11:37:31.444945 [debug] [MainThread]: On master: BEGIN
11:37:31.445229 [debug] [MainThread]: Opening a new connection, currently in state init
11:37:31.459074 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:37:31.459483 [debug] [MainThread]: Using postgres connection "master"
11:37:31.459731 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:37:31.493665 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:37:31.496206 [debug] [MainThread]: On master: ROLLBACK
11:37:31.496691 [debug] [MainThread]: Using postgres connection "master"
11:37:31.496983 [debug] [MainThread]: On master: BEGIN
11:37:31.497504 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:37:31.497788 [debug] [MainThread]: On master: COMMIT
11:37:31.498034 [debug] [MainThread]: Using postgres connection "master"
11:37:31.498282 [debug] [MainThread]: On master: COMMIT
11:37:31.498618 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:31.498876 [debug] [MainThread]: On master: Close
11:37:31.499497 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:37:31.500379 [info ] [MainThread]: 
11:37:31.508557 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:37:31.509060 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:37:31.509944 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:31.510209 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:37:31.510473 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:37:31.514321 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:31.514834 [debug] [Thread-1  ]: finished collecting timing info
11:37:31.515133 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:37:31.563230 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:31.564031 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:31.564336 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:37:31.564596 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:37:31.581968 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
11:37:31.582449 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:31.582767 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value", DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
ORDER BY week ASC
group by week O

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:37:31.583415 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "group"
LINE 24: group by week O
         ^

11:37:31.583717 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
11:37:31.584274 [debug] [Thread-1  ]: finished collecting timing info
11:37:31.584584 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:37:31.586574 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "group"
  LINE 24: group by week O
           ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:31.587080 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '122eacb6-708c-438d-97d4-448021060007', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c46af970>]}
11:37:31.587649 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.08s]
11:37:31.588223 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:37:31.590090 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:31.590397 [debug] [MainThread]: Using postgres connection "master"
11:37:31.590640 [debug] [MainThread]: On master: BEGIN
11:37:31.590856 [debug] [MainThread]: Opening a new connection, currently in state closed
11:37:31.607974 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:37:31.608438 [debug] [MainThread]: On master: COMMIT
11:37:31.608729 [debug] [MainThread]: Using postgres connection "master"
11:37:31.608992 [debug] [MainThread]: On master: COMMIT
11:37:31.609370 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:31.609697 [debug] [MainThread]: On master: Close
11:37:31.612172 [info ] [MainThread]: 
11:37:31.612899 [info ] [MainThread]: Finished running 1 table model in 0.24s.
11:37:31.613939 [debug] [MainThread]: Connection 'master' was properly closed.
11:37:31.614231 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:37:31.614483 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:37:31.624953 [info ] [MainThread]: 
11:37:31.625749 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
11:37:31.626283 [info ] [MainThread]: 
11:37:31.626870 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
11:37:31.627455 [error] [MainThread]:   syntax error at or near "group"
11:37:31.627917 [error] [MainThread]:   LINE 24: group by week O
11:37:31.628288 [error] [MainThread]:            ^
11:37:31.628668 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:31.632479 [info ] [MainThread]: 
11:37:31.632967 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
11:37:31.639741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c6aeac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c6aeaee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2c46af910>]}


============================== 2022-03-06 11:37:37.289234 | 8373085d-6276-4ea6-94ca-c6bff77483be ==============================
11:37:37.289234 [info ] [MainThread]: Running with dbt=1.0.3
11:37:37.290097 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:37:37.290449 [debug] [MainThread]: Tracking: tracking
11:37:37.295363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb99f8c370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb99f8c310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb99f8c250>]}
11:37:37.334712 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:37:37.335669 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
11:37:37.355591 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
11:37:37.379697 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:37:37.387691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8373085d-6276-4ea6-94ca-c6bff77483be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb97bc80d0>]}
11:37:37.396435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8373085d-6276-4ea6-94ca-c6bff77483be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb99f66400>]}
11:37:37.396944 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:37:37.398910 [info ] [MainThread]: 
11:37:37.399880 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:37.401228 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:37:37.416390 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:37:37.416763 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:37:37.417072 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:37.429991 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
11:37:37.432313 [debug] [ThreadPool]: On list_adludio: Close
11:37:37.434033 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:37:37.443204 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:37.443539 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:37:37.443773 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:37.454349 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:37:37.454685 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:37.454925 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:37:37.458100 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.0 seconds
11:37:37.460285 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:37:37.460680 [debug] [ThreadPool]: On list_adludio_public: Close
11:37:37.467250 [debug] [MainThread]: Using postgres connection "master"
11:37:37.467558 [debug] [MainThread]: On master: BEGIN
11:37:37.467797 [debug] [MainThread]: Opening a new connection, currently in state init
11:37:37.478308 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:37:37.478664 [debug] [MainThread]: Using postgres connection "master"
11:37:37.478913 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:37:37.506426 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:37:37.508838 [debug] [MainThread]: On master: ROLLBACK
11:37:37.509354 [debug] [MainThread]: Using postgres connection "master"
11:37:37.509692 [debug] [MainThread]: On master: BEGIN
11:37:37.510153 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:37:37.510417 [debug] [MainThread]: On master: COMMIT
11:37:37.510657 [debug] [MainThread]: Using postgres connection "master"
11:37:37.510879 [debug] [MainThread]: On master: COMMIT
11:37:37.511201 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:37.511461 [debug] [MainThread]: On master: Close
11:37:37.512073 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:37:37.512827 [info ] [MainThread]: 
11:37:37.517544 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:37:37.518029 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:37:37.518840 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:37.519123 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:37:37.519481 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:37:37.523586 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:37.524102 [debug] [Thread-1  ]: finished collecting timing info
11:37:37.524381 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:37:37.625268 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:37.626014 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:37.626294 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:37:37.626515 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:37:37.637050 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:37:37.637416 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:37.637709 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value", DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week O

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:37:37.638213 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "O"
LINE 23: group by week O
                       ^

11:37:37.638477 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
11:37:37.638999 [debug] [Thread-1  ]: finished collecting timing info
11:37:37.639288 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:37:37.639891 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "O"
  LINE 23: group by week O
                         ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:37.640383 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8373085d-6276-4ea6-94ca-c6bff77483be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb9851ce20>]}
11:37:37.640953 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.12s]
11:37:37.641661 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:37:37.643181 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:37.643480 [debug] [MainThread]: Using postgres connection "master"
11:37:37.643726 [debug] [MainThread]: On master: BEGIN
11:37:37.643935 [debug] [MainThread]: Opening a new connection, currently in state closed
11:37:37.654411 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:37:37.654781 [debug] [MainThread]: On master: COMMIT
11:37:37.655029 [debug] [MainThread]: Using postgres connection "master"
11:37:37.655292 [debug] [MainThread]: On master: COMMIT
11:37:37.655622 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:37.655874 [debug] [MainThread]: On master: Close
11:37:37.656474 [info ] [MainThread]: 
11:37:37.657177 [info ] [MainThread]: Finished running 1 table model in 0.26s.
11:37:37.657896 [debug] [MainThread]: Connection 'master' was properly closed.
11:37:37.658148 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:37:37.658362 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:37:37.665443 [info ] [MainThread]: 
11:37:37.665946 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
11:37:37.666405 [info ] [MainThread]: 
11:37:37.666878 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
11:37:37.667290 [error] [MainThread]:   syntax error at or near "O"
11:37:37.667658 [error] [MainThread]:   LINE 23: group by week O
11:37:37.668088 [error] [MainThread]:                          ^
11:37:37.668508 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:37.668972 [info ] [MainThread]: 
11:37:37.669382 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
11:37:37.670029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcba4d3cdc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb9619b490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb9851c160>]}


============================== 2022-03-06 11:37:52.238856 | 6b8f27cb-9fb2-451d-b076-95502d53f23f ==============================
11:37:52.238856 [info ] [MainThread]: Running with dbt=1.0.3
11:37:52.243887 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:37:52.244396 [debug] [MainThread]: Tracking: tracking
11:37:52.250764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b5122c730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b5122cac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b5122c670>]}
11:37:52.301873 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:37:52.302621 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
11:37:52.325291 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
11:37:52.349527 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:37:52.357931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6b8f27cb-9fb2-451d-b076-95502d53f23f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b4ee700d0>]}
11:37:52.366765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6b8f27cb-9fb2-451d-b076-95502d53f23f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b4f7b6dc0>]}
11:37:52.367280 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:37:52.369647 [info ] [MainThread]: 
11:37:52.370627 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:52.372127 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:37:52.389069 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:37:52.389491 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:37:52.389722 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:52.417664 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.03 seconds
11:37:52.420248 [debug] [ThreadPool]: On list_adludio: Close
11:37:52.423968 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:37:52.434771 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:52.435176 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:37:52.435450 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:37:52.445853 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:37:52.446182 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:37:52.446430 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:37:52.456257 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.01 seconds
11:37:52.458459 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:37:52.458894 [debug] [ThreadPool]: On list_adludio_public: Close
11:37:52.465520 [debug] [MainThread]: Using postgres connection "master"
11:37:52.465814 [debug] [MainThread]: On master: BEGIN
11:37:52.466055 [debug] [MainThread]: Opening a new connection, currently in state init
11:37:52.482401 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:37:52.482768 [debug] [MainThread]: Using postgres connection "master"
11:37:52.483048 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:37:52.523785 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
11:37:52.526412 [debug] [MainThread]: On master: ROLLBACK
11:37:52.526923 [debug] [MainThread]: Using postgres connection "master"
11:37:52.527190 [debug] [MainThread]: On master: BEGIN
11:37:52.527639 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:37:52.527926 [debug] [MainThread]: On master: COMMIT
11:37:52.528201 [debug] [MainThread]: Using postgres connection "master"
11:37:52.528432 [debug] [MainThread]: On master: COMMIT
11:37:52.528796 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:52.529058 [debug] [MainThread]: On master: Close
11:37:52.529759 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:37:52.530706 [info ] [MainThread]: 
11:37:52.542930 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:37:52.543533 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:37:52.551468 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:52.551804 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:37:52.552107 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:37:52.556075 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:52.556615 [debug] [Thread-1  ]: finished collecting timing info
11:37:52.556893 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:37:52.676753 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:37:52.677445 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:52.677780 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:37:52.678009 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:37:52.688882 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:37:52.689262 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:37:52.689587 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value", DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:37:52.692736 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal_value" does not exist
LINE 21: select AVG(deal_value), week
                    ^
HINT:  Perhaps you meant to reference the column "source_data.Deal_Value".

11:37:52.693023 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
11:37:52.693598 [debug] [Thread-1  ]: finished collecting timing info
11:37:52.693897 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:37:52.694507 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  column "deal_value" does not exist
  LINE 21: select AVG(deal_value), week
                      ^
  HINT:  Perhaps you meant to reference the column "source_data.Deal_Value".
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:52.694964 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b8f27cb-9fb2-451d-b076-95502d53f23f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b4f802340>]}
11:37:52.695518 [error] [Thread-1  ]: 1 of 1 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.14s]
11:37:52.697940 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:37:52.702980 [debug] [MainThread]: Acquiring new postgres connection "master"
11:37:52.703299 [debug] [MainThread]: Using postgres connection "master"
11:37:52.703529 [debug] [MainThread]: On master: BEGIN
11:37:52.703738 [debug] [MainThread]: Opening a new connection, currently in state closed
11:37:52.714253 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:37:52.714607 [debug] [MainThread]: On master: COMMIT
11:37:52.714859 [debug] [MainThread]: Using postgres connection "master"
11:37:52.715092 [debug] [MainThread]: On master: COMMIT
11:37:52.715418 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:37:52.715678 [debug] [MainThread]: On master: Close
11:37:52.716288 [info ] [MainThread]: 
11:37:52.727717 [info ] [MainThread]: Finished running 1 table model in 0.35s.
11:37:52.728305 [debug] [MainThread]: Connection 'master' was properly closed.
11:37:52.728557 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:37:52.728761 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:37:52.737011 [info ] [MainThread]: 
11:37:52.737945 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
11:37:52.738930 [info ] [MainThread]: 
11:37:52.739593 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
11:37:52.740223 [error] [MainThread]:   column "deal_value" does not exist
11:37:52.741121 [error] [MainThread]:   LINE 21: select AVG(deal_value), week
11:37:52.741761 [error] [MainThread]:                       ^
11:37:52.742182 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "source_data.Deal_Value".
11:37:52.742701 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
11:37:52.743391 [info ] [MainThread]: 
11:37:52.743947 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
11:37:52.744859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b4ee08eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b4ee08ca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b4f802fd0>]}


============================== 2022-03-06 11:38:15.122034 | 0791b5f3-9c7f-4b06-9a26-4e80a7c4a719 ==============================
11:38:15.122034 [info ] [MainThread]: Running with dbt=1.0.3
11:38:15.123010 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:38:15.123356 [debug] [MainThread]: Tracking: tracking
11:38:15.128177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e7773eb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e7773e460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e7773e640>]}
11:38:15.209386 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:38:15.210135 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
11:38:15.233518 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
11:38:15.258121 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:38:15.266214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0791b5f3-9c7f-4b06-9a26-4e80a7c4a719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e753830d0>]}
11:38:15.275096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0791b5f3-9c7f-4b06-9a26-4e80a7c4a719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e777204c0>]}
11:38:15.275644 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:38:15.278237 [info ] [MainThread]: 
11:38:15.279065 [debug] [MainThread]: Acquiring new postgres connection "master"
11:38:15.280297 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:38:15.299772 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:38:15.300202 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:38:15.300519 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:38:15.317174 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
11:38:15.319621 [debug] [ThreadPool]: On list_adludio: Close
11:38:15.336833 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:38:15.348551 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:38:15.348904 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:38:15.349159 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:38:15.368747 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
11:38:15.369144 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:38:15.369503 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:38:15.371935 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.0 seconds
11:38:15.374106 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:38:15.374524 [debug] [ThreadPool]: On list_adludio_public: Close
11:38:15.381081 [debug] [MainThread]: Using postgres connection "master"
11:38:15.381351 [debug] [MainThread]: On master: BEGIN
11:38:15.381633 [debug] [MainThread]: Opening a new connection, currently in state init
11:38:15.391871 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:38:15.392185 [debug] [MainThread]: Using postgres connection "master"
11:38:15.392446 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:38:15.421711 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:38:15.424197 [debug] [MainThread]: On master: ROLLBACK
11:38:15.424705 [debug] [MainThread]: Using postgres connection "master"
11:38:15.424996 [debug] [MainThread]: On master: BEGIN
11:38:15.425482 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:38:15.425793 [debug] [MainThread]: On master: COMMIT
11:38:15.426057 [debug] [MainThread]: Using postgres connection "master"
11:38:15.426307 [debug] [MainThread]: On master: COMMIT
11:38:15.426650 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:38:15.426917 [debug] [MainThread]: On master: Close
11:38:15.427527 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:38:15.427997 [info ] [MainThread]: 
11:38:15.434029 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:38:15.434494 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:38:15.435257 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.435525 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:38:15.435815 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:38:15.439707 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:38:15.440615 [debug] [Thread-1  ]: finished collecting timing info
11:38:15.440894 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:38:15.559429 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:38:15.560128 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.560415 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:38:15.560653 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:38:15.575104 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:38:15.575460 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.575749 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:38:15.584767 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
11:38:15.598200 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.598510 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:38:15.599099 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:38:15.602479 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.602763 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:38:15.603381 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:38:15.622305 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:38:15.622642 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.622908 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:38:15.626926 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:38:15.637552 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:15.637840 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:38:15.640196 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:38:15.642054 [debug] [Thread-1  ]: finished collecting timing info
11:38:15.642370 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:38:15.643046 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0791b5f3-9c7f-4b06-9a26-4e80a7c4a719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e7437dfa0>]}
11:38:15.643593 [info ] [Thread-1  ]: 1 of 1 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.21s]
11:38:15.644577 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:38:15.650056 [debug] [MainThread]: Acquiring new postgres connection "master"
11:38:15.650376 [debug] [MainThread]: Using postgres connection "master"
11:38:15.650635 [debug] [MainThread]: On master: BEGIN
11:38:15.650867 [debug] [MainThread]: Opening a new connection, currently in state closed
11:38:15.672038 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:38:15.672404 [debug] [MainThread]: On master: COMMIT
11:38:15.672659 [debug] [MainThread]: Using postgres connection "master"
11:38:15.672921 [debug] [MainThread]: On master: COMMIT
11:38:15.673281 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:38:15.673578 [debug] [MainThread]: On master: Close
11:38:15.674174 [info ] [MainThread]: 
11:38:15.675047 [info ] [MainThread]: Finished running 1 table model in 0.40s.
11:38:15.676108 [debug] [MainThread]: Connection 'master' was properly closed.
11:38:15.676349 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:38:15.676561 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:38:15.685691 [info ] [MainThread]: 
11:38:15.686196 [info ] [MainThread]: [32mCompleted successfully[0m
11:38:15.686682 [info ] [MainThread]: 
11:38:15.687095 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
11:38:15.687622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e75315430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e75324490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e7437f8b0>]}


============================== 2022-03-06 11:38:42.311919 | 59983e57-39e7-46ba-af5a-19929a6da8b1 ==============================
11:38:42.311919 [info ] [MainThread]: Running with dbt=1.0.3
11:38:42.313050 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:38:42.313802 [debug] [MainThread]: Tracking: tracking
11:38:42.319722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58314d3b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58314d3460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58314d3640>]}
11:38:42.366688 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:38:42.367516 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
11:38:42.387569 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
11:38:42.414709 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:38:42.423158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '59983e57-39e7-46ba-af5a-19929a6da8b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f582f1180d0>]}
11:38:42.432576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '59983e57-39e7-46ba-af5a-19929a6da8b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58314b54c0>]}
11:38:42.433140 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:38:42.435179 [info ] [MainThread]: 
11:38:42.436038 [debug] [MainThread]: Acquiring new postgres connection "master"
11:38:42.437297 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:38:42.452955 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:38:42.453393 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:38:42.453736 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:38:42.469003 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
11:38:42.471550 [debug] [ThreadPool]: On list_adludio: Close
11:38:42.477055 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:38:42.489536 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:38:42.489962 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:38:42.490283 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:38:42.502408 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:38:42.502861 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:38:42.503154 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:38:42.506612 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.0 seconds
11:38:42.508874 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:38:42.511648 [debug] [ThreadPool]: On list_adludio_public: Close
11:38:42.521770 [debug] [MainThread]: Using postgres connection "master"
11:38:42.522162 [debug] [MainThread]: On master: BEGIN
11:38:42.526746 [debug] [MainThread]: Opening a new connection, currently in state init
11:38:42.537441 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:38:42.537885 [debug] [MainThread]: Using postgres connection "master"
11:38:42.538158 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:38:42.570923 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:38:42.573437 [debug] [MainThread]: On master: ROLLBACK
11:38:42.574011 [debug] [MainThread]: Using postgres connection "master"
11:38:42.574319 [debug] [MainThread]: On master: BEGIN
11:38:42.574821 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:38:42.575113 [debug] [MainThread]: On master: COMMIT
11:38:42.575381 [debug] [MainThread]: Using postgres connection "master"
11:38:42.575621 [debug] [MainThread]: On master: COMMIT
11:38:42.575952 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:38:42.576228 [debug] [MainThread]: On master: Close
11:38:42.576845 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:38:42.579512 [info ] [MainThread]: 
11:38:42.584061 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:38:42.584669 [info ] [Thread-1  ]: 1 of 1 START table model public.deal_value_per_week............................. [RUN]
11:38:42.585918 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.586242 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:38:42.586588 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:38:42.593255 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:38:42.596915 [debug] [Thread-1  ]: finished collecting timing info
11:38:42.597279 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:38:42.722183 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:38:42.722901 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.723185 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:38:42.723406 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:38:42.733961 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:38:42.734342 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.734640 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:38:42.739600 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
11:38:42.749726 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.750082 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:38:42.750772 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:38:42.754551 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.754842 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:38:42.755538 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:38:42.772959 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:38:42.773342 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.773665 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:38:42.774893 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:38:42.782700 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:38:42.783038 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:38:42.788107 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:38:42.790343 [debug] [Thread-1  ]: finished collecting timing info
11:38:42.790702 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:38:42.793413 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59983e57-39e7-46ba-af5a-19929a6da8b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f582d711fa0>]}
11:38:42.794067 [info ] [Thread-1  ]: 1 of 1 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.21s]
11:38:42.794844 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:38:42.797148 [debug] [MainThread]: Acquiring new postgres connection "master"
11:38:42.797494 [debug] [MainThread]: Using postgres connection "master"
11:38:42.797751 [debug] [MainThread]: On master: BEGIN
11:38:42.797960 [debug] [MainThread]: Opening a new connection, currently in state closed
11:38:42.809669 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:38:42.810088 [debug] [MainThread]: On master: COMMIT
11:38:42.810420 [debug] [MainThread]: Using postgres connection "master"
11:38:42.810683 [debug] [MainThread]: On master: COMMIT
11:38:42.811028 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:38:42.811297 [debug] [MainThread]: On master: Close
11:38:42.811932 [info ] [MainThread]: 
11:38:42.812571 [info ] [MainThread]: Finished running 1 table model in 0.38s.
11:38:42.813045 [debug] [MainThread]: Connection 'master' was properly closed.
11:38:42.813239 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:38:42.813407 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
11:38:42.820716 [info ] [MainThread]: 
11:38:42.821224 [info ] [MainThread]: [32mCompleted successfully[0m
11:38:42.821779 [info ] [MainThread]: 
11:38:42.822203 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
11:38:42.822772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f582f0c2f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f582f0aec70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58314d3730>]}


============================== 2022-03-06 11:53:00.817872 | 24ee8774-1053-4fd5-933c-1ec33c05f739 ==============================
11:53:00.817872 [info ] [MainThread]: Running with dbt=1.0.3
11:53:00.819536 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:53:00.820040 [debug] [MainThread]: Tracking: tracking
11:53:00.827397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd790633460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd790633970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd790633070>]}
11:53:00.871595 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
11:53:00.872351 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
11:53:00.891976 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
11:53:00.920739 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:53:00.928974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '24ee8774-1053-4fd5-933c-1ec33c05f739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd78e2760d0>]}
11:53:00.938524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '24ee8774-1053-4fd5-933c-1ec33c05f739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd78ebbfac0>]}
11:53:00.939130 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:53:00.941695 [info ] [MainThread]: 
11:53:00.943045 [debug] [MainThread]: Acquiring new postgres connection "master"
11:53:00.944473 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:53:00.960677 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:53:00.961222 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:53:00.961535 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:53:00.974386 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
11:53:00.976899 [debug] [ThreadPool]: On list_adludio: Close
11:53:00.978898 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:53:00.988692 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:53:00.989072 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:53:00.989373 [debug] [ThreadPool]: Opening a new connection, currently in state closed
11:53:00.999880 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:53:01.000272 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:53:01.000522 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:53:01.003785 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.0 seconds
11:53:01.006075 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:53:01.006609 [debug] [ThreadPool]: On list_adludio_public: Close
11:53:01.013173 [debug] [MainThread]: Using postgres connection "master"
11:53:01.013581 [debug] [MainThread]: On master: BEGIN
11:53:01.013873 [debug] [MainThread]: Opening a new connection, currently in state init
11:53:01.024943 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:53:01.025344 [debug] [MainThread]: Using postgres connection "master"
11:53:01.025645 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:53:01.054810 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:53:01.057632 [debug] [MainThread]: On master: ROLLBACK
11:53:01.058141 [debug] [MainThread]: Using postgres connection "master"
11:53:01.058422 [debug] [MainThread]: On master: BEGIN
11:53:01.058872 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:53:01.059142 [debug] [MainThread]: On master: COMMIT
11:53:01.059396 [debug] [MainThread]: Using postgres connection "master"
11:53:01.059610 [debug] [MainThread]: On master: COMMIT
11:53:01.059932 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:53:01.060193 [debug] [MainThread]: On master: Close
11:53:01.060807 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:53:01.062048 [info ] [MainThread]: 
11:53:01.066614 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:53:01.067119 [info ] [Thread-1  ]: 1 of 2 START table model public.deal_value_per_week............................. [RUN]
11:53:01.068230 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.068519 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:53:01.068786 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:53:01.072824 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:53:01.073408 [debug] [Thread-1  ]: finished collecting timing info
11:53:01.073829 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:53:01.180294 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:53:01.181036 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.181340 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:53:01.181587 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:53:01.192012 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:53:01.192418 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.192671 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:53:01.197759 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
11:53:01.207739 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.208107 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:53:01.210769 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:53:01.214329 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.214621 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:53:01.215245 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:53:01.234812 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:53:01.235287 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.235581 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:53:01.236873 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:53:01.244669 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:01.245034 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:53:01.247360 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:53:01.249408 [debug] [Thread-1  ]: finished collecting timing info
11:53:01.249839 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:53:01.250616 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24ee8774-1053-4fd5-933c-1ec33c05f739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd790c44190>]}
11:53:01.251223 [info ] [Thread-1  ]: 1 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.18s]
11:53:01.252121 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:53:01.252481 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
11:53:01.253024 [info ] [Thread-1  ]: 2 of 2 START table model public.transformed_sales_number_data................... [RUN]
11:53:01.253962 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.254266 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
11:53:01.254543 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
11:53:01.258886 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.259441 [debug] [Thread-1  ]: finished collecting timing info
11:53:01.259746 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
11:53:01.264027 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.264597 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.264861 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
11:53:01.265089 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:53:01.277733 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:53:01.278199 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.278512 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:53:01.283555 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
11:53:01.287535 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.287857 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
11:53:01.288499 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:53:01.291348 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
11:53:01.291625 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.291891 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
11:53:01.296063 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:53:01.298836 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:01.299131 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
11:53:01.299656 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:53:01.301903 [debug] [Thread-1  ]: finished collecting timing info
11:53:01.302216 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
11:53:01.304996 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24ee8774-1053-4fd5-933c-1ec33c05f739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd78e210d90>]}
11:53:01.305650 [info ] [Thread-1  ]: 2 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 53[0m in 0.05s]
11:53:01.306208 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
11:53:01.307760 [debug] [MainThread]: Acquiring new postgres connection "master"
11:53:01.308083 [debug] [MainThread]: Using postgres connection "master"
11:53:01.308319 [debug] [MainThread]: On master: BEGIN
11:53:01.308535 [debug] [MainThread]: Opening a new connection, currently in state closed
11:53:01.319042 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:53:01.319453 [debug] [MainThread]: On master: COMMIT
11:53:01.319761 [debug] [MainThread]: Using postgres connection "master"
11:53:01.320026 [debug] [MainThread]: On master: COMMIT
11:53:01.320383 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:53:01.320647 [debug] [MainThread]: On master: Close
11:53:01.321367 [info ] [MainThread]: 
11:53:01.325019 [info ] [MainThread]: Finished running 2 table models in 0.38s.
11:53:01.325430 [debug] [MainThread]: Connection 'master' was properly closed.
11:53:01.325747 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
11:53:01.333084 [info ] [MainThread]: 
11:53:01.333664 [info ] [MainThread]: [32mCompleted successfully[0m
11:53:01.334281 [info ] [MainThread]: 
11:53:01.334704 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
11:53:01.335251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd78e212040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd78e21f1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd78d246d30>]}


============================== 2022-03-06 11:53:54.400226 | 2c056cdb-fae4-498d-a7fa-3548f3dd55d5 ==============================
11:53:54.400226 [info ] [MainThread]: Running with dbt=1.0.3
11:53:54.401837 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:53:54.402269 [debug] [MainThread]: Tracking: tracking
11:53:54.418268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a267c7b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a267c7460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a267c7640>]}
11:53:54.486173 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:53:54.486973 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
11:53:54.513144 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
11:53:54.550553 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:53:54.563078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2c056cdb-fae4-498d-a7fa-3548f3dd55d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a2440c0d0>]}
11:53:54.572764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2c056cdb-fae4-498d-a7fa-3548f3dd55d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a267a84c0>]}
11:53:54.573361 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:53:54.575515 [info ] [MainThread]: 
11:53:54.584844 [debug] [MainThread]: Acquiring new postgres connection "master"
11:53:54.586466 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:53:54.602393 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:53:54.602778 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:53:54.603058 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:53:54.619926 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
11:53:54.622536 [debug] [ThreadPool]: On list_adludio: Close
11:53:54.629877 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:53:54.639417 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:53:54.639786 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:53:54.640068 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:53:54.654607 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:53:54.655032 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:53:54.655393 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:53:54.660187 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
11:53:54.662565 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:53:54.670288 [debug] [ThreadPool]: On list_adludio_public: Close
11:53:54.680467 [debug] [MainThread]: Using postgres connection "master"
11:53:54.680826 [debug] [MainThread]: On master: BEGIN
11:53:54.681068 [debug] [MainThread]: Opening a new connection, currently in state init
11:53:54.697103 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:53:54.697606 [debug] [MainThread]: Using postgres connection "master"
11:53:54.697930 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:53:54.740864 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
11:53:54.744504 [debug] [MainThread]: On master: ROLLBACK
11:53:54.745063 [debug] [MainThread]: Using postgres connection "master"
11:53:54.745416 [debug] [MainThread]: On master: BEGIN
11:53:54.746106 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:53:54.747652 [debug] [MainThread]: On master: COMMIT
11:53:54.747931 [debug] [MainThread]: Using postgres connection "master"
11:53:54.748226 [debug] [MainThread]: On master: COMMIT
11:53:54.749429 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:53:54.749785 [debug] [MainThread]: On master: Close
11:53:54.752275 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:53:54.752809 [info ] [MainThread]: 
11:53:54.759868 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:53:54.760362 [info ] [Thread-1  ]: 1 of 2 START table model public.deal_value_per_week............................. [RUN]
11:53:54.761147 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.761390 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:53:54.761712 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:53:54.767105 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:53:54.767635 [debug] [Thread-1  ]: finished collecting timing info
11:53:54.767916 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:53:54.907625 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:53:54.908228 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.908450 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:53:54.908629 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:53:54.921595 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:53:54.922008 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.922276 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:53:54.927351 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
11:53:54.937586 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.937919 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:53:54.938590 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:53:54.942188 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.942472 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:53:54.946099 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:53:54.963442 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:53:54.963841 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.964324 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:53:54.965692 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:53:54.973270 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:53:54.986014 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:53:54.988497 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:53:54.990653 [debug] [Thread-1  ]: finished collecting timing info
11:53:54.991013 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:53:54.991725 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c056cdb-fae4-498d-a7fa-3548f3dd55d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a24c760d0>]}
11:53:54.992276 [info ] [Thread-1  ]: 1 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.23s]
11:53:54.993106 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:53:54.993416 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
11:53:54.993801 [info ] [Thread-1  ]: 2 of 2 START table model public.transformed_sales_number_data................... [RUN]
11:53:54.997917 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:55.001758 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
11:53:55.002037 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
11:53:55.009741 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:53:55.010381 [debug] [Thread-1  ]: finished collecting timing info
11:53:55.010677 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
11:53:55.015064 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:53:55.015608 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:55.018976 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
11:53:55.019255 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:53:55.031405 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:53:55.031834 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:53:55.032153 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select Deal_id as id, Deal_created_at as Deal_created_at
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency
"Deal_Region" as deal_region
from source_data
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:53:55.032725 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near ""Deal_Value""
LINE 22: "Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
         ^

11:53:55.033017 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: ROLLBACK
11:53:55.033601 [debug] [Thread-1  ]: finished collecting timing info
11:53:55.036992 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
11:53:55.037653 [debug] [Thread-1  ]: Database Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)
  syntax error at or near ""Deal_Value""
  LINE 22: "Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
           ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
11:53:55.038140 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c056cdb-fae4-498d-a7fa-3548f3dd55d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a27b5f4f0>]}
11:53:55.038793 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.transformed_sales_number_data.......... [[31mERROR[0m in 0.04s]
11:53:55.039790 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
11:53:55.043394 [debug] [MainThread]: Acquiring new postgres connection "master"
11:53:55.043711 [debug] [MainThread]: Using postgres connection "master"
11:53:55.043967 [debug] [MainThread]: On master: BEGIN
11:53:55.044176 [debug] [MainThread]: Opening a new connection, currently in state closed
11:53:55.061755 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:53:55.062206 [debug] [MainThread]: On master: COMMIT
11:53:55.062471 [debug] [MainThread]: Using postgres connection "master"
11:53:55.062706 [debug] [MainThread]: On master: COMMIT
11:53:55.063093 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:53:55.063364 [debug] [MainThread]: On master: Close
11:53:55.064004 [info ] [MainThread]: 
11:53:55.064956 [info ] [MainThread]: Finished running 2 table models in 0.48s.
11:53:55.065483 [debug] [MainThread]: Connection 'master' was properly closed.
11:53:55.065756 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:53:55.066001 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
11:53:55.076722 [info ] [MainThread]: 
11:53:55.077590 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
11:53:55.078299 [info ] [MainThread]: 
11:53:55.078725 [error] [MainThread]: [33mDatabase Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)[0m
11:53:55.079612 [error] [MainThread]:   syntax error at or near ""Deal_Value""
11:53:55.080164 [error] [MainThread]:   LINE 22: "Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
11:53:55.080724 [error] [MainThread]:            ^
11:53:55.081203 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
11:53:55.081722 [info ] [MainThread]: 
11:53:55.082203 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
11:53:55.083039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a243a0dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a169b71f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a24c77670>]}


============================== 2022-03-06 11:54:45.850200 | 02e381f8-25e1-4824-8104-10653bf07b87 ==============================
11:54:45.850200 [info ] [MainThread]: Running with dbt=1.0.3
11:54:45.851037 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:54:45.851387 [debug] [MainThread]: Tracking: tracking
11:54:45.856150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148f785b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148f7851c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148f785b50>]}
11:54:45.895467 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:54:45.896235 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
11:54:45.915372 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
11:54:45.939755 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:54:45.947849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '02e381f8-25e1-4824-8104-10653bf07b87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148d3db0d0>]}
11:54:45.956581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '02e381f8-25e1-4824-8104-10653bf07b87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148f779460>]}
11:54:45.957141 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:54:45.959200 [info ] [MainThread]: 
11:54:45.960003 [debug] [MainThread]: Acquiring new postgres connection "master"
11:54:45.961295 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:54:45.976331 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:54:45.976709 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:54:45.977039 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:54:45.989924 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
11:54:45.992280 [debug] [ThreadPool]: On list_adludio: Close
11:54:45.994004 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:54:46.003136 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:54:46.003454 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:54:46.003684 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:54:46.014262 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:54:46.014612 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:54:46.014892 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:54:46.018087 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
11:54:46.020270 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:54:46.020658 [debug] [ThreadPool]: On list_adludio_public: Close
11:54:46.027285 [debug] [MainThread]: Using postgres connection "master"
11:54:46.027597 [debug] [MainThread]: On master: BEGIN
11:54:46.027831 [debug] [MainThread]: Opening a new connection, currently in state init
11:54:46.038310 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:54:46.038646 [debug] [MainThread]: Using postgres connection "master"
11:54:46.038898 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:54:46.071301 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
11:54:46.073799 [debug] [MainThread]: On master: ROLLBACK
11:54:46.074286 [debug] [MainThread]: Using postgres connection "master"
11:54:46.074561 [debug] [MainThread]: On master: BEGIN
11:54:46.075048 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:54:46.075324 [debug] [MainThread]: On master: COMMIT
11:54:46.075568 [debug] [MainThread]: Using postgres connection "master"
11:54:46.075793 [debug] [MainThread]: On master: COMMIT
11:54:46.076123 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:54:46.076383 [debug] [MainThread]: On master: Close
11:54:46.076994 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:54:46.077810 [info ] [MainThread]: 
11:54:46.082292 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:54:46.082755 [info ] [Thread-1  ]: 1 of 2 START table model public.deal_value_per_week............................. [RUN]
11:54:46.083879 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.084155 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:54:46.084450 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:54:46.089613 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:54:46.090155 [debug] [Thread-1  ]: finished collecting timing info
11:54:46.090459 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:54:46.187089 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:54:46.187783 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.188072 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:54:46.188296 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:54:46.198942 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:54:46.199258 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.199521 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:54:46.204364 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
11:54:46.214269 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.214598 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:54:46.215272 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:54:46.218729 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.219004 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:54:46.219596 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:54:46.236128 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:54:46.236507 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.236764 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:54:46.238031 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:54:46.245430 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:54:46.245780 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:54:46.247911 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:54:46.249753 [debug] [Thread-1  ]: finished collecting timing info
11:54:46.250074 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:54:46.250802 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '02e381f8-25e1-4824-8104-10653bf07b87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148fdc5190>]}
11:54:46.251393 [info ] [Thread-1  ]: 1 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.17s]
11:54:46.252371 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:54:46.252701 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
11:54:46.253248 [info ] [Thread-1  ]: 2 of 2 START table model public.transformed_sales_number_data................... [RUN]
11:54:46.254097 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:54:46.254352 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
11:54:46.254594 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
11:54:46.258432 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:54:46.258949 [debug] [Thread-1  ]: finished collecting timing info
11:54:46.259211 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
11:54:46.263370 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:54:46.263906 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:54:46.264156 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
11:54:46.264378 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:54:46.274899 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:54:46.275233 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:54:46.275479 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select Deal_id as id, Deal_created_at as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region
from source_data
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:54:46.276320 [debug] [Thread-1  ]: Postgres adapter: Postgres error: column "deal_id" does not exist
LINE 21: select Deal_id as id, Deal_created_at as Deal_created_at,
                ^
HINT:  Perhaps you meant to reference the column "source_data.Deal_id".

11:54:46.276586 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: ROLLBACK
11:54:46.277113 [debug] [Thread-1  ]: finished collecting timing info
11:54:46.277382 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
11:54:46.277969 [debug] [Thread-1  ]: Database Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)
  column "deal_id" does not exist
  LINE 21: select Deal_id as id, Deal_created_at as Deal_created_at,
                  ^
  HINT:  Perhaps you meant to reference the column "source_data.Deal_id".
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
11:54:46.278433 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '02e381f8-25e1-4824-8104-10653bf07b87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1490b41490>]}
11:54:46.279000 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.transformed_sales_number_data.......... [[31mERROR[0m in 0.02s]
11:54:46.279681 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
11:54:46.281381 [debug] [MainThread]: Acquiring new postgres connection "master"
11:54:46.281719 [debug] [MainThread]: Using postgres connection "master"
11:54:46.281961 [debug] [MainThread]: On master: BEGIN
11:54:46.282181 [debug] [MainThread]: Opening a new connection, currently in state closed
11:54:46.292618 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:54:46.292970 [debug] [MainThread]: On master: COMMIT
11:54:46.293221 [debug] [MainThread]: Using postgres connection "master"
11:54:46.293493 [debug] [MainThread]: On master: COMMIT
11:54:46.293842 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:54:46.294110 [debug] [MainThread]: On master: Close
11:54:46.294726 [info ] [MainThread]: 
11:54:46.296907 [info ] [MainThread]: Finished running 2 table models in 0.33s.
11:54:46.297656 [debug] [MainThread]: Connection 'master' was properly closed.
11:54:46.298160 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:54:46.298398 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
11:54:46.305877 [info ] [MainThread]: 
11:54:46.306375 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
11:54:46.306846 [info ] [MainThread]: 
11:54:46.307247 [error] [MainThread]: [33mDatabase Error in model transformed_sales_number_data (models/Sales Numbers/transformed_sales_number_data.sql)[0m
11:54:46.307639 [error] [MainThread]:   column "deal_id" does not exist
11:54:46.308010 [error] [MainThread]:   LINE 21: select Deal_id as id, Deal_created_at as Deal_created_at,
11:54:46.308382 [error] [MainThread]:                   ^
11:54:46.308772 [error] [MainThread]:   HINT:  Perhaps you meant to reference the column "source_data.Deal_id".
11:54:46.309140 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/transformed_sales_number_data.sql
11:54:46.309559 [info ] [MainThread]: 
11:54:46.309957 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
11:54:46.310480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148d371e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f148d3775b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1490b34a60>]}


============================== 2022-03-06 11:55:04.065816 | 2510d18b-bb50-4eee-9e9f-4b8a6ca17c84 ==============================
11:55:04.065816 [info ] [MainThread]: Running with dbt=1.0.3
11:55:04.067410 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:55:04.067833 [debug] [MainThread]: Tracking: tracking
11:55:04.073597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0cfb1b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0cfb1460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0cfb1640>]}
11:55:04.142629 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:55:04.143435 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
11:55:04.163107 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
11:55:04.191063 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:55:04.199178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2510d18b-bb50-4eee-9e9f-4b8a6ca17c84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0abf50d0>]}
11:55:04.207989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2510d18b-bb50-4eee-9e9f-4b8a6ca17c84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0cf934c0>]}
11:55:04.208494 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:55:04.210589 [info ] [MainThread]: 
11:55:04.211392 [debug] [MainThread]: Acquiring new postgres connection "master"
11:55:04.212657 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:55:04.233207 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:55:04.233603 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:55:04.233894 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:55:04.250167 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
11:55:04.252661 [debug] [ThreadPool]: On list_adludio: Close
11:55:04.256574 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:55:04.266243 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:55:04.266582 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:55:04.266855 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:55:04.283428 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
11:55:04.283860 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:55:04.284158 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:55:04.287528 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
11:55:04.289810 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:55:04.290195 [debug] [ThreadPool]: On list_adludio_public: Close
11:55:04.298661 [debug] [MainThread]: Using postgres connection "master"
11:55:04.299007 [debug] [MainThread]: On master: BEGIN
11:55:04.299279 [debug] [MainThread]: Opening a new connection, currently in state init
11:55:04.317828 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
11:55:04.318248 [debug] [MainThread]: Using postgres connection "master"
11:55:04.318509 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:55:04.355290 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
11:55:04.357810 [debug] [MainThread]: On master: ROLLBACK
11:55:04.358303 [debug] [MainThread]: Using postgres connection "master"
11:55:04.358617 [debug] [MainThread]: On master: BEGIN
11:55:04.359077 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:55:04.359357 [debug] [MainThread]: On master: COMMIT
11:55:04.359600 [debug] [MainThread]: Using postgres connection "master"
11:55:04.359846 [debug] [MainThread]: On master: COMMIT
11:55:04.365313 [debug] [MainThread]: SQL status: COMMIT in 0.01 seconds
11:55:04.365681 [debug] [MainThread]: On master: Close
11:55:04.368288 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:55:04.372277 [info ] [MainThread]: 
11:55:04.376648 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:55:04.377149 [info ] [Thread-1  ]: 1 of 2 START table model public.deal_value_per_week............................. [RUN]
11:55:04.378285 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.378564 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:55:04.378847 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:55:04.383022 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:55:04.383630 [debug] [Thread-1  ]: finished collecting timing info
11:55:04.383923 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:55:04.500021 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:55:04.500688 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.500967 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:55:04.501243 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:55:04.514680 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:55:04.515067 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.515342 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:55:04.522919 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
11:55:04.533406 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.533794 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:55:04.534475 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:55:04.538255 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.538559 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:55:04.539245 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:55:04.559598 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:55:04.559938 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.560165 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:55:04.561386 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:55:04.571148 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:55:04.571527 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:55:04.574444 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:55:04.579535 [debug] [Thread-1  ]: finished collecting timing info
11:55:04.579880 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:55:04.582699 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2510d18b-bb50-4eee-9e9f-4b8a6ca17c84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f091d49d0>]}
11:55:04.583261 [info ] [Thread-1  ]: 1 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.20s]
11:55:04.583786 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:55:04.584080 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
11:55:04.584435 [info ] [Thread-1  ]: 2 of 2 START table model public.transformed_sales_number_data................... [RUN]
11:55:04.585109 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.585354 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
11:55:04.585655 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
11:55:04.591214 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.591696 [debug] [Thread-1  ]: finished collecting timing info
11:55:04.591951 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
11:55:04.596396 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.596949 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.597200 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
11:55:04.597417 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:55:04.615251 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
11:55:04.615669 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.616018 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region
from source_data
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:55:04.622775 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
11:55:04.626778 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.627068 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
11:55:04.627659 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:55:04.631193 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.631464 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
11:55:04.631995 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:55:04.634473 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
11:55:04.634729 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.634948 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
11:55:04.644152 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
11:55:04.647141 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:55:04.647422 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
11:55:04.653404 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
11:55:04.655400 [debug] [Thread-1  ]: finished collecting timing info
11:55:04.655747 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
11:55:04.657942 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2510d18b-bb50-4eee-9e9f-4b8a6ca17c84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f091f3e20>]}
11:55:04.658498 [info ] [Thread-1  ]: 2 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.07s]
11:55:04.659044 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
11:55:04.664475 [debug] [MainThread]: Acquiring new postgres connection "master"
11:55:04.664800 [debug] [MainThread]: Using postgres connection "master"
11:55:04.665033 [debug] [MainThread]: On master: BEGIN
11:55:04.665252 [debug] [MainThread]: Opening a new connection, currently in state closed
11:55:04.676099 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:55:04.676457 [debug] [MainThread]: On master: COMMIT
11:55:04.676706 [debug] [MainThread]: Using postgres connection "master"
11:55:04.677142 [debug] [MainThread]: On master: COMMIT
11:55:04.677488 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:55:04.677763 [debug] [MainThread]: On master: Close
11:55:04.680112 [info ] [MainThread]: 
11:55:04.680780 [info ] [MainThread]: Finished running 2 table models in 0.47s.
11:55:04.681342 [debug] [MainThread]: Connection 'master' was properly closed.
11:55:04.681601 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:55:04.681808 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
11:55:04.693171 [info ] [MainThread]: 
11:55:04.694083 [info ] [MainThread]: [32mCompleted successfully[0m
11:55:04.694798 [info ] [MainThread]: 
11:55:04.695359 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
11:55:04.696004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0ab90160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f0aba11c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f091c6d30>]}


============================== 2022-03-06 11:59:15.783129 | ac4f8ee6-53b5-42c7-a82b-225fd2f84359 ==============================
11:59:15.783129 [info ] [MainThread]: Running with dbt=1.0.3
11:59:15.784489 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
11:59:15.784850 [debug] [MainThread]: Tracking: tracking
11:59:15.789997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442d9a43a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442d9a4c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442d9a4520>]}
11:59:15.886681 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
11:59:15.887483 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
11:59:15.917446 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
11:59:15.946200 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

11:59:15.954553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ac4f8ee6-53b5-42c7-a82b-225fd2f84359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442b5fa0d0>]}
11:59:15.963494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ac4f8ee6-53b5-42c7-a82b-225fd2f84359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442d997460>]}
11:59:15.964028 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
11:59:15.966071 [info ] [MainThread]: 
11:59:15.966864 [debug] [MainThread]: Acquiring new postgres connection "master"
11:59:15.968240 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
11:59:15.994902 [debug] [ThreadPool]: Using postgres connection "list_adludio"
11:59:15.995248 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
11:59:15.995497 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:59:16.010035 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
11:59:16.012482 [debug] [ThreadPool]: On list_adludio: Close
11:59:16.016541 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
11:59:16.025832 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:59:16.026186 [debug] [ThreadPool]: On list_adludio_public: BEGIN
11:59:16.026447 [debug] [ThreadPool]: Opening a new connection, currently in state init
11:59:16.036966 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
11:59:16.040129 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
11:59:16.040391 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
11:59:16.046051 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.01 seconds
11:59:16.048032 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
11:59:16.058570 [debug] [ThreadPool]: On list_adludio_public: Close
11:59:16.076569 [debug] [MainThread]: Using postgres connection "master"
11:59:16.076916 [debug] [MainThread]: On master: BEGIN
11:59:16.077216 [debug] [MainThread]: Opening a new connection, currently in state init
11:59:16.109006 [debug] [MainThread]: SQL status: BEGIN in 0.03 seconds
11:59:16.109376 [debug] [MainThread]: Using postgres connection "master"
11:59:16.123932 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
11:59:16.165810 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
11:59:16.168459 [debug] [MainThread]: On master: ROLLBACK
11:59:16.168942 [debug] [MainThread]: Using postgres connection "master"
11:59:16.169269 [debug] [MainThread]: On master: BEGIN
11:59:16.169779 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
11:59:16.170061 [debug] [MainThread]: On master: COMMIT
11:59:16.170325 [debug] [MainThread]: Using postgres connection "master"
11:59:16.170560 [debug] [MainThread]: On master: COMMIT
11:59:16.170886 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:59:16.171160 [debug] [MainThread]: On master: Close
11:59:16.171823 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
11:59:16.172756 [info ] [MainThread]: 
11:59:16.178188 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
11:59:16.178654 [info ] [Thread-1  ]: 1 of 2 START table model public.deal_value_per_week............................. [RUN]
11:59:16.179463 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.179751 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
11:59:16.180028 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
11:59:16.186192 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
11:59:16.187198 [debug] [Thread-1  ]: finished collecting timing info
11:59:16.187852 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
11:59:16.345753 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
11:59:16.346514 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.346877 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
11:59:16.347156 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:59:16.357358 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
11:59:16.357768 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.358038 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select "Deal_Value" as deal_value, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:59:16.363075 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
11:59:16.390841 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.391249 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
11:59:16.391896 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:59:16.395600 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.395918 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
11:59:16.396486 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:59:16.433027 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:59:16.433381 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.433673 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
11:59:16.437938 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:59:16.446613 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
11:59:16.446963 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
11:59:16.453603 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
11:59:16.459163 [debug] [Thread-1  ]: finished collecting timing info
11:59:16.459512 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
11:59:16.460340 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ac4f8ee6-53b5-42c7-a82b-225fd2f84359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442d97ce20>]}
11:59:16.460893 [info ] [Thread-1  ]: 1 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.28s]
11:59:16.461587 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
11:59:16.462226 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
11:59:16.463429 [info ] [Thread-1  ]: 2 of 2 START table model public.transformed_sales_number_data................... [RUN]
11:59:16.464563 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.464885 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
11:59:16.465176 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
11:59:16.469944 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.470651 [debug] [Thread-1  ]: finished collecting timing info
11:59:16.471128 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
11:59:16.479299 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.479927 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.480222 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
11:59:16.480486 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
11:59:16.496271 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
11:59:16.496588 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.496797 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
11:59:16.505911 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
11:59:16.521871 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.522249 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
11:59:16.522986 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:59:16.545078 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.545540 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
11:59:16.549910 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
11:59:16.560554 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
11:59:16.560810 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.561000 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
11:59:16.563888 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
11:59:16.566205 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
11:59:16.566385 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
11:59:16.569415 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
11:59:16.570597 [debug] [Thread-1  ]: finished collecting timing info
11:59:16.570811 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
11:59:16.571372 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ac4f8ee6-53b5-42c7-a82b-225fd2f84359', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4429be69a0>]}
11:59:16.571882 [info ] [Thread-1  ]: 2 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.11s]
11:59:16.572350 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
11:59:16.574026 [debug] [MainThread]: Acquiring new postgres connection "master"
11:59:16.574347 [debug] [MainThread]: Using postgres connection "master"
11:59:16.574594 [debug] [MainThread]: On master: BEGIN
11:59:16.574807 [debug] [MainThread]: Opening a new connection, currently in state closed
11:59:16.589392 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
11:59:16.589725 [debug] [MainThread]: On master: COMMIT
11:59:16.589918 [debug] [MainThread]: Using postgres connection "master"
11:59:16.590094 [debug] [MainThread]: On master: COMMIT
11:59:16.590365 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
11:59:16.590582 [debug] [MainThread]: On master: Close
11:59:16.591100 [info ] [MainThread]: 
11:59:16.591658 [info ] [MainThread]: Finished running 2 table models in 0.62s.
11:59:16.592102 [debug] [MainThread]: Connection 'master' was properly closed.
11:59:16.592291 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
11:59:16.592456 [debug] [MainThread]: Connection 'model.Analytics_dbt.transformed_sales_number_data' was properly closed.
11:59:16.602734 [info ] [MainThread]: 
11:59:16.603390 [info ] [MainThread]: [32mCompleted successfully[0m
11:59:16.603872 [info ] [MainThread]: 
11:59:16.607693 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
11:59:16.608386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442e6f7b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4429bd82e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4429bc8c40>]}


============================== 2022-03-06 12:00:53.472818 | 79f1601e-c6f7-4020-bd10-349a62e1c7ed ==============================
12:00:53.472818 [info ] [MainThread]: Running with dbt=1.0.3
12:00:53.474328 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:00:53.474734 [debug] [MainThread]: Tracking: tracking
12:00:53.480572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1d3f0b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1d3f01c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1d3f00d0>]}
12:00:53.528724 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
12:00:53.529578 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:00:53.530103 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/transformed_sales_number_data.sql
12:00:53.556708 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:00:53.577314 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/transformed_sales_number_data.sql
12:00:53.586367 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:00:53.595395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '79f1601e-c6f7-4020-bd10-349a62e1c7ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1b02d0d0>]}
12:00:53.606605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '79f1601e-c6f7-4020-bd10-349a62e1c7ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1d3ca7c0>]}
12:00:53.607130 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:00:53.609216 [info ] [MainThread]: 
12:00:53.610033 [debug] [MainThread]: Acquiring new postgres connection "master"
12:00:53.611362 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:00:53.626815 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:00:53.627167 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:00:53.627433 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:00:53.641492 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
12:00:53.643884 [debug] [ThreadPool]: On list_adludio: Close
12:00:53.648149 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:00:53.658977 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:00:53.659301 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:00:53.659544 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:00:53.670420 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:00:53.670787 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:00:53.671041 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:00:53.674274 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:00:53.676478 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:00:53.676844 [debug] [ThreadPool]: On list_adludio_public: Close
12:00:53.694021 [debug] [MainThread]: Using postgres connection "master"
12:00:53.694414 [debug] [MainThread]: On master: BEGIN
12:00:53.694746 [debug] [MainThread]: Opening a new connection, currently in state init
12:00:53.712421 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
12:00:53.712990 [debug] [MainThread]: Using postgres connection "master"
12:00:53.713249 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:00:53.752225 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
12:00:53.754751 [debug] [MainThread]: On master: ROLLBACK
12:00:53.755224 [debug] [MainThread]: Using postgres connection "master"
12:00:53.755492 [debug] [MainThread]: On master: BEGIN
12:00:53.755959 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:00:53.756219 [debug] [MainThread]: On master: COMMIT
12:00:53.756458 [debug] [MainThread]: Using postgres connection "master"
12:00:53.756679 [debug] [MainThread]: On master: COMMIT
12:00:53.756995 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:00:53.757264 [debug] [MainThread]: On master: Close
12:00:53.757952 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:00:53.758972 [info ] [MainThread]: 
12:00:53.768723 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:00:53.769228 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:00:53.770052 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.770564 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:00:53.770866 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:00:53.777958 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.778527 [debug] [Thread-1  ]: finished collecting timing info
12:00:53.778824 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:00:53.898609 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.899344 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.899659 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:00:53.900180 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:00:53.913960 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:00:53.914349 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.914632 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:00:53.923617 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:00:53.934167 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.934549 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:00:53.935244 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:00:53.938685 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.938955 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:00:53.939579 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:00:53.958892 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:00:53.959309 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.959601 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:00:53.965007 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
12:00:53.975293 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:00:53.975599 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:00:53.978337 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:00:53.980214 [debug] [Thread-1  ]: finished collecting timing info
12:00:53.980519 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:00:53.981172 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '79f1601e-c6f7-4020-bd10-349a62e1c7ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1da0ee20>]}
12:00:53.985688 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.21s]
12:00:53.986241 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:00:53.986955 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:00:53.987301 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:00:53.987904 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:00:53.988131 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:00:53.988423 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:00:53.992644 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:00:53.993233 [debug] [Thread-1  ]: finished collecting timing info
12:00:53.994343 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:00:53.998872 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:00:53.999442 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:00:53.999722 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:00:53.999970 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:00:54.011904 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:00:54.012242 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:00:54.012513 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/


select .flow_99,
select AVG("adludio"."public"."transformed_sales_number_data".deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:00:54.013008 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "."
LINE 15: select .flow_99,
                ^

12:00:54.013296 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:00:54.017507 [debug] [Thread-1  ]: finished collecting timing info
12:00:54.017853 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:00:54.018472 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "."
  LINE 15: select .flow_99,
                  ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:00:54.018947 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '79f1601e-c6f7-4020-bd10-349a62e1c7ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1b9d2d90>]}
12:00:54.019521 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.03s]
12:00:54.020717 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:00:54.022418 [debug] [MainThread]: Acquiring new postgres connection "master"
12:00:54.022717 [debug] [MainThread]: Using postgres connection "master"
12:00:54.022947 [debug] [MainThread]: On master: BEGIN
12:00:54.023162 [debug] [MainThread]: Opening a new connection, currently in state closed
12:00:54.038177 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:00:54.038587 [debug] [MainThread]: On master: COMMIT
12:00:54.038883 [debug] [MainThread]: Using postgres connection "master"
12:00:54.039162 [debug] [MainThread]: On master: COMMIT
12:00:54.039518 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:00:54.039835 [debug] [MainThread]: On master: Close
12:00:54.042012 [info ] [MainThread]: 
12:00:54.042377 [info ] [MainThread]: Finished running 2 table models in 0.43s.
12:00:54.042752 [debug] [MainThread]: Connection 'master' was properly closed.
12:00:54.042929 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:00:54.043071 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:00:54.053090 [info ] [MainThread]: 
12:00:54.053503 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:00:54.053916 [info ] [MainThread]: 
12:00:54.054231 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:00:54.054577 [error] [MainThread]:   syntax error at or near "."
12:00:54.054877 [error] [MainThread]:   LINE 15: select .flow_99,
12:00:54.055180 [error] [MainThread]:                   ^
12:00:54.055471 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:00:54.055780 [info ] [MainThread]: 
12:00:54.056082 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:00:54.056512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1afc9c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d19609730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d1b9d2160>]}


============================== 2022-03-06 12:17:54.233439 | 1bd23033-c94f-4147-a0e7-966013a8336e ==============================
12:17:54.233439 [info ] [MainThread]: Running with dbt=1.0.3
12:17:54.234270 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:17:54.234622 [debug] [MainThread]: Tracking: tracking
12:17:54.242413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5546981370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5546981310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5546981a90>]}
12:17:54.286020 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:17:54.286861 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:17:54.309606 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:17:54.334344 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:17:54.342567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1bd23033-c94f-4147-a0e7-966013a8336e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55445d70d0>]}
12:17:54.351812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1bd23033-c94f-4147-a0e7-966013a8336e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55469741c0>]}
12:17:54.352368 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:17:54.354513 [info ] [MainThread]: 
12:17:54.355447 [debug] [MainThread]: Acquiring new postgres connection "master"
12:17:54.356837 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:17:54.372511 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:17:54.372971 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:17:54.373297 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:17:54.387631 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
12:17:54.390208 [debug] [ThreadPool]: On list_adludio: Close
12:17:54.392054 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:17:54.401329 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:17:54.401766 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:17:54.402070 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:17:54.413812 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:17:54.414264 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:17:54.414569 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:17:54.417834 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:17:54.420170 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:17:54.420619 [debug] [ThreadPool]: On list_adludio_public: Close
12:17:54.427384 [debug] [MainThread]: Using postgres connection "master"
12:17:54.427772 [debug] [MainThread]: On master: BEGIN
12:17:54.428050 [debug] [MainThread]: Opening a new connection, currently in state init
12:17:54.441726 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:17:54.442118 [debug] [MainThread]: Using postgres connection "master"
12:17:54.442370 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:17:54.476105 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
12:17:54.478628 [debug] [MainThread]: On master: ROLLBACK
12:17:54.479156 [debug] [MainThread]: Using postgres connection "master"
12:17:54.479430 [debug] [MainThread]: On master: BEGIN
12:17:54.479881 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:17:54.480193 [debug] [MainThread]: On master: COMMIT
12:17:54.480438 [debug] [MainThread]: Using postgres connection "master"
12:17:54.480747 [debug] [MainThread]: On master: COMMIT
12:17:54.484361 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:17:54.484721 [debug] [MainThread]: On master: Close
12:17:54.485383 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:17:54.485970 [info ] [MainThread]: 
12:17:54.493404 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:17:54.493935 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:17:54.494713 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.494982 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:17:54.495253 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:17:54.500926 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.501593 [debug] [Thread-1  ]: finished collecting timing info
12:17:54.501914 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:17:54.603732 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.604450 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.604744 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:17:54.604975 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:17:54.615694 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:17:54.616052 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.616305 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:17:54.626311 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:17:54.636750 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.637174 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:17:54.637924 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:17:54.641440 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.641775 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:17:54.642420 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:17:54.659173 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:17:54.659658 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.659994 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:17:54.662630 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:17:54.670154 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:17:54.670498 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:17:54.674018 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:17:54.675945 [debug] [Thread-1  ]: finished collecting timing info
12:17:54.676294 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:17:54.676989 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1bd23033-c94f-4147-a0e7-966013a8336e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5544e428b0>]}
12:17:54.677615 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.18s]
12:17:54.678189 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:17:54.679443 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:17:54.679857 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:17:54.680578 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:17:54.680832 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:17:54.681072 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:17:54.685698 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:17:54.686307 [debug] [Thread-1  ]: finished collecting timing info
12:17:54.686618 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:17:54.690880 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:17:54.691450 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:17:54.691709 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:17:54.691928 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:17:54.702642 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:17:54.703098 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:17:54.703401 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



select AVG("adludio"."public"."transformed_sales_number_data".deal_value),
"adludio"."public"."transformed_sales_number_data".week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:17:54.704009 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "source_data" does not exist
LINE 18: from source_data
              ^

12:17:54.704318 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:17:54.704891 [debug] [Thread-1  ]: finished collecting timing info
12:17:54.705191 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:17:54.705960 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  relation "source_data" does not exist
  LINE 18: from source_data
                ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:17:54.706436 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1bd23033-c94f-4147-a0e7-966013a8336e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5547d45430>]}
12:17:54.707006 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.03s]
12:17:54.708112 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:17:54.709964 [debug] [MainThread]: Acquiring new postgres connection "master"
12:17:54.710278 [debug] [MainThread]: Using postgres connection "master"
12:17:54.710519 [debug] [MainThread]: On master: BEGIN
12:17:54.710743 [debug] [MainThread]: Opening a new connection, currently in state closed
12:17:54.721242 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:17:54.721704 [debug] [MainThread]: On master: COMMIT
12:17:54.722003 [debug] [MainThread]: Using postgres connection "master"
12:17:54.722275 [debug] [MainThread]: On master: COMMIT
12:17:54.722683 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:17:54.722992 [debug] [MainThread]: On master: Close
12:17:54.723814 [info ] [MainThread]: 
12:17:54.724532 [info ] [MainThread]: Finished running 2 table models in 0.37s.
12:17:54.725232 [debug] [MainThread]: Connection 'master' was properly closed.
12:17:54.725541 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:17:54.725847 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:17:54.733564 [info ] [MainThread]: 
12:17:54.734323 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:17:54.734975 [info ] [MainThread]: 
12:17:54.735514 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:17:54.736074 [error] [MainThread]:   relation "source_data" does not exist
12:17:54.736593 [error] [MainThread]:   LINE 18: from source_data
12:17:54.737101 [error] [MainThread]:                 ^
12:17:54.737657 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:17:54.738212 [info ] [MainThread]: 
12:17:54.738793 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:17:54.739522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55472291c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f554408c040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5547d3c940>]}


============================== 2022-03-06 12:18:33.665782 | e67dcff2-5173-487d-8305-a80edf0b196b ==============================
12:18:33.665782 [info ] [MainThread]: Running with dbt=1.0.3
12:18:33.667186 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:18:33.667648 [debug] [MainThread]: Tracking: tracking
12:18:33.673302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04008683a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0400868c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0400868520>]}
12:18:33.716963 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
12:18:33.717518 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
12:18:33.718294 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:18:33.730298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e67dcff2-5173-487d-8305-a80edf0b196b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04008685e0>]}
12:18:33.739419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e67dcff2-5173-487d-8305-a80edf0b196b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03fed51f40>]}
12:18:33.740005 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:18:33.744758 [info ] [MainThread]: 
12:18:33.745964 [debug] [MainThread]: Acquiring new postgres connection "master"
12:18:33.747654 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:18:33.763063 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:18:33.763441 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:18:33.763709 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:18:33.779889 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:18:33.782504 [debug] [ThreadPool]: On list_adludio: Close
12:18:33.784538 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:18:33.797562 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:18:33.798030 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:18:33.798340 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:18:33.808871 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:18:33.809317 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:18:33.809671 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:18:33.813006 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:18:33.815360 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:18:33.815800 [debug] [ThreadPool]: On list_adludio_public: Close
12:18:33.822623 [debug] [MainThread]: Using postgres connection "master"
12:18:33.822991 [debug] [MainThread]: On master: BEGIN
12:18:33.823265 [debug] [MainThread]: Opening a new connection, currently in state init
12:18:33.834000 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:18:33.834477 [debug] [MainThread]: Using postgres connection "master"
12:18:33.834776 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:18:33.867256 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
12:18:33.870021 [debug] [MainThread]: On master: ROLLBACK
12:18:33.870571 [debug] [MainThread]: Using postgres connection "master"
12:18:33.870890 [debug] [MainThread]: On master: BEGIN
12:18:33.871430 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:18:33.871728 [debug] [MainThread]: On master: COMMIT
12:18:33.871995 [debug] [MainThread]: Using postgres connection "master"
12:18:33.872244 [debug] [MainThread]: On master: COMMIT
12:18:33.872601 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:18:33.872894 [debug] [MainThread]: On master: Close
12:18:33.873594 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:18:33.874619 [info ] [MainThread]: 
12:18:33.880330 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:18:33.880863 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:18:33.881999 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.882283 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:18:33.882566 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:18:33.886664 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.887409 [debug] [Thread-1  ]: finished collecting timing info
12:18:33.887713 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:18:33.942822 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.943662 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.943986 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:18:33.944271 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:18:33.957212 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:18:33.957657 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.957957 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:18:33.972333 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:18:33.982547 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.982919 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:18:33.983628 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:18:33.987433 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:33.987717 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:18:33.988310 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:18:34.010537 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:18:34.011045 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:34.011380 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:18:34.019187 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
12:18:34.033018 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:18:34.033480 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:18:34.037410 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:18:34.039469 [debug] [Thread-1  ]: finished collecting timing info
12:18:34.039853 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:18:34.040668 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e67dcff2-5173-487d-8305-a80edf0b196b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03fc8bff70>]}
12:18:34.041287 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.16s]
12:18:34.041948 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:18:34.042973 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:18:34.043348 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:18:34.044693 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:18:34.044946 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:18:34.045173 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:18:34.127671 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:18:34.128457 [debug] [Thread-1  ]: finished collecting timing info
12:18:34.128868 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:18:34.133630 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:18:34.134326 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:18:34.134615 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:18:34.135089 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:18:34.148612 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:18:34.149134 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:18:34.149520 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



select AVG("adludio"."public"."transformed_sales_number_data".deal_value),
"adludio"."public"."transformed_sales_number_data".week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:18:34.150171 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "source_data" does not exist
LINE 18: from source_data
              ^

12:18:34.150467 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:18:34.151021 [debug] [Thread-1  ]: finished collecting timing info
12:18:34.151338 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:18:34.151988 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  relation "source_data" does not exist
  LINE 18: from source_data
                ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:18:34.152465 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e67dcff2-5173-487d-8305-a80edf0b196b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0400eaceb0>]}
12:18:34.153044 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.11s]
12:18:34.154213 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:18:34.155661 [debug] [MainThread]: Acquiring new postgres connection "master"
12:18:34.155965 [debug] [MainThread]: Using postgres connection "master"
12:18:34.156196 [debug] [MainThread]: On master: BEGIN
12:18:34.156436 [debug] [MainThread]: Opening a new connection, currently in state closed
12:18:34.171320 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:18:34.171784 [debug] [MainThread]: On master: COMMIT
12:18:34.172110 [debug] [MainThread]: Using postgres connection "master"
12:18:34.172370 [debug] [MainThread]: On master: COMMIT
12:18:34.172729 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:18:34.173025 [debug] [MainThread]: On master: Close
12:18:34.182172 [info ] [MainThread]: 
12:18:34.183195 [info ] [MainThread]: Finished running 2 table models in 0.44s.
12:18:34.184383 [debug] [MainThread]: Connection 'master' was properly closed.
12:18:34.184862 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:18:34.185119 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:18:34.196437 [info ] [MainThread]: 
12:18:34.197425 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:18:34.198766 [info ] [MainThread]: 
12:18:34.199502 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:18:34.200584 [error] [MainThread]:   relation "source_data" does not exist
12:18:34.201258 [error] [MainThread]:   LINE 18: from source_data
12:18:34.202516 [error] [MainThread]:                 ^
12:18:34.203490 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:18:34.204704 [info ] [MainThread]: 
12:18:34.205401 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:18:34.206518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0400e797c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0400ea7700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0400eac460>]}


============================== 2022-03-06 12:20:01.548542 | 959fc47d-d524-4136-a312-35c5589eb33d ==============================
12:20:01.548542 [info ] [MainThread]: Running with dbt=1.0.3
12:20:01.550005 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:20:01.550453 [debug] [MainThread]: Tracking: tracking
12:20:01.556680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845e69c3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845e69c7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845e69c520>]}
12:20:01.603177 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
12:20:01.603679 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
12:20:01.604413 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:20:01.616668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '959fc47d-d524-4136-a312-35c5589eb33d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845e69c0d0>]}
12:20:01.626215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '959fc47d-d524-4136-a312-35c5589eb33d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845cb86f40>]}
12:20:01.626802 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:20:01.629889 [info ] [MainThread]: 
12:20:01.630989 [debug] [MainThread]: Acquiring new postgres connection "master"
12:20:01.632581 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:20:01.648297 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:20:01.648738 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:20:01.649024 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:20:01.664911 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:20:01.667410 [debug] [ThreadPool]: On list_adludio: Close
12:20:01.669265 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:20:01.678523 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:20:01.678902 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:20:01.679165 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:20:01.691428 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:20:01.691809 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:20:01.692053 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:20:01.695328 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:20:01.697609 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:20:01.698256 [debug] [ThreadPool]: On list_adludio_public: Close
12:20:01.705104 [debug] [MainThread]: Using postgres connection "master"
12:20:01.705554 [debug] [MainThread]: On master: BEGIN
12:20:01.705861 [debug] [MainThread]: Opening a new connection, currently in state init
12:20:01.716386 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:20:01.716809 [debug] [MainThread]: Using postgres connection "master"
12:20:01.717095 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:20:01.749923 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
12:20:01.752447 [debug] [MainThread]: On master: ROLLBACK
12:20:01.753050 [debug] [MainThread]: Using postgres connection "master"
12:20:01.753351 [debug] [MainThread]: On master: BEGIN
12:20:01.753893 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:20:01.754194 [debug] [MainThread]: On master: COMMIT
12:20:01.754475 [debug] [MainThread]: Using postgres connection "master"
12:20:01.754739 [debug] [MainThread]: On master: COMMIT
12:20:01.755096 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:20:01.755406 [debug] [MainThread]: On master: Close
12:20:01.756060 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:20:01.756797 [info ] [MainThread]: 
12:20:01.762606 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:20:01.763067 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:20:01.763832 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.764106 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:20:01.764400 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:20:01.768382 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.768899 [debug] [Thread-1  ]: finished collecting timing info
12:20:01.769182 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:20:01.812540 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.813289 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.813599 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:20:01.813843 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:20:01.824534 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:20:01.825029 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.825342 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:20:01.836760 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:20:01.846780 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.847185 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:20:01.847894 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:20:01.851583 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.851894 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:20:01.852526 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:20:01.869082 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:20:01.869688 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.870038 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:20:01.872890 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:20:01.882255 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:20:01.882620 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:20:01.885155 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:20:01.887017 [debug] [Thread-1  ]: finished collecting timing info
12:20:01.887373 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:20:01.888102 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '959fc47d-d524-4136-a312-35c5589eb33d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8455cf42e0>]}
12:20:01.888677 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.12s]
12:20:01.889268 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:20:01.890490 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:20:01.890890 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:20:01.891604 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:20:01.891873 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:20:01.893822 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:20:01.951732 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:20:01.952517 [debug] [Thread-1  ]: finished collecting timing info
12:20:01.952859 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:20:01.957239 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:20:01.957941 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:20:01.958251 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:20:01.958511 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:20:01.969168 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:20:01.969552 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:20:01.969818 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



select AVG("adludio"."public"."transformed_sales_number_data".deal_value),
"adludio"."public"."transformed_sales_number_data".week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:20:01.970423 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "source_data" does not exist
LINE 18: from source_data
              ^

12:20:01.970694 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:20:01.971216 [debug] [Thread-1  ]: finished collecting timing info
12:20:01.971505 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:20:01.972070 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  relation "source_data" does not exist
  LINE 18: from source_data
                ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:20:01.972496 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '959fc47d-d524-4136-a312-35c5589eb33d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845ece1dc0>]}
12:20:01.973014 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.08s]
12:20:01.973616 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:20:01.975262 [debug] [MainThread]: Acquiring new postgres connection "master"
12:20:01.975571 [debug] [MainThread]: Using postgres connection "master"
12:20:01.975791 [debug] [MainThread]: On master: BEGIN
12:20:01.976000 [debug] [MainThread]: Opening a new connection, currently in state closed
12:20:01.986945 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:20:01.987350 [debug] [MainThread]: On master: COMMIT
12:20:01.987645 [debug] [MainThread]: Using postgres connection "master"
12:20:01.987916 [debug] [MainThread]: On master: COMMIT
12:20:01.988306 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:20:01.988634 [debug] [MainThread]: On master: Close
12:20:01.989346 [info ] [MainThread]: 
12:20:01.989882 [info ] [MainThread]: Finished running 2 table models in 0.36s.
12:20:01.990328 [debug] [MainThread]: Connection 'master' was properly closed.
12:20:01.990577 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:20:01.991062 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:20:01.999119 [info ] [MainThread]: 
12:20:01.999668 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:20:02.000433 [info ] [MainThread]: 
12:20:02.001428 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:20:02.002070 [error] [MainThread]:   relation "source_data" does not exist
12:20:02.002659 [error] [MainThread]:   LINE 18: from source_data
12:20:02.003530 [error] [MainThread]:                 ^
12:20:02.004101 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:20:02.004564 [info ] [MainThread]: 
12:20:02.005343 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:20:02.006261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845ecbff70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845ecdc220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845ece1400>]}


============================== 2022-03-06 12:21:09.922212 | a2feabf8-2df0-4fd9-a9ff-d0d26ba8588e ==============================
12:21:09.922212 [info ] [MainThread]: Running with dbt=1.0.3
12:21:09.923122 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:21:09.923509 [debug] [MainThread]: Tracking: tracking
12:21:09.929249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758a32f130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758a32f2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758a32f3d0>]}
12:21:09.987921 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:21:09.988727 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:21:10.014962 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:21:10.043397 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:21:10.051577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a2feabf8-2df0-4fd9-a9ff-d0d26ba8588e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7587f870d0>]}
12:21:10.071015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a2feabf8-2df0-4fd9-a9ff-d0d26ba8588e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758a323f10>]}
12:21:10.071570 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:21:10.073986 [info ] [MainThread]: 
12:21:10.074987 [debug] [MainThread]: Acquiring new postgres connection "master"
12:21:10.076288 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:21:10.099095 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:21:10.099497 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:21:10.099786 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:21:10.115723 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:21:10.118190 [debug] [ThreadPool]: On list_adludio: Close
12:21:10.120004 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:21:10.131405 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:21:10.131800 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:21:10.132113 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:21:10.142677 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:21:10.149637 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:21:10.150032 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:21:10.153405 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:21:10.155896 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:21:10.156336 [debug] [ThreadPool]: On list_adludio_public: Close
12:21:10.168304 [debug] [MainThread]: Using postgres connection "master"
12:21:10.168753 [debug] [MainThread]: On master: BEGIN
12:21:10.169041 [debug] [MainThread]: Opening a new connection, currently in state init
12:21:10.179436 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:21:10.179791 [debug] [MainThread]: Using postgres connection "master"
12:21:10.180061 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:21:10.219623 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
12:21:10.222391 [debug] [MainThread]: On master: ROLLBACK
12:21:10.222866 [debug] [MainThread]: Using postgres connection "master"
12:21:10.223138 [debug] [MainThread]: On master: BEGIN
12:21:10.223597 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:21:10.223853 [debug] [MainThread]: On master: COMMIT
12:21:10.224076 [debug] [MainThread]: Using postgres connection "master"
12:21:10.224285 [debug] [MainThread]: On master: COMMIT
12:21:10.224815 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:21:10.225073 [debug] [MainThread]: On master: Close
12:21:10.229007 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:21:10.229693 [info ] [MainThread]: 
12:21:10.238124 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:21:10.238641 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:21:10.239437 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.239698 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:21:10.239950 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:21:10.245035 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.245721 [debug] [Thread-1  ]: finished collecting timing info
12:21:10.246042 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:21:10.362905 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.363813 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.364183 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:21:10.364474 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:21:10.378822 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:21:10.379201 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.379472 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:21:10.389988 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:21:10.403642 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.404041 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:21:10.404762 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:21:10.408365 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.408664 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:21:10.409300 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:21:10.426879 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:21:10.427340 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.427660 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:21:10.434733 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
12:21:10.442882 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:21:10.443276 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:21:10.445974 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:21:10.447899 [debug] [Thread-1  ]: finished collecting timing info
12:21:10.448244 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:21:10.448995 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a2feabf8-2df0-4fd9-a9ff-d0d26ba8588e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758a953f40>]}
12:21:10.449630 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.21s]
12:21:10.450477 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:21:10.451439 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:21:10.451862 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:21:10.452768 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:21:10.453061 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:21:10.453340 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:21:10.457695 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:21:10.461393 [debug] [Thread-1  ]: finished collecting timing info
12:21:10.461778 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:21:10.466450 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:21:10.467230 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:21:10.467557 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:21:10.467889 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:21:10.478737 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:21:10.479138 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:21:10.479451 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(source_data.deal_value), source_data.week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:21:10.479989 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "select"
LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
             ^

12:21:10.480299 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:21:10.480904 [debug] [Thread-1  ]: finished collecting timing info
12:21:10.481235 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:21:10.481925 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "select"
  LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
               ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:21:10.482358 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a2feabf8-2df0-4fd9-a9ff-d0d26ba8588e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758b6e7400>]}
12:21:10.482890 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.03s]
12:21:10.489148 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:21:10.501376 [debug] [MainThread]: Acquiring new postgres connection "master"
12:21:10.501808 [debug] [MainThread]: Using postgres connection "master"
12:21:10.502062 [debug] [MainThread]: On master: BEGIN
12:21:10.502283 [debug] [MainThread]: Opening a new connection, currently in state closed
12:21:10.521763 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
12:21:10.522979 [debug] [MainThread]: On master: COMMIT
12:21:10.523346 [debug] [MainThread]: Using postgres connection "master"
12:21:10.523632 [debug] [MainThread]: On master: COMMIT
12:21:10.524075 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:21:10.524406 [debug] [MainThread]: On master: Close
12:21:10.525689 [info ] [MainThread]: 
12:21:10.527340 [info ] [MainThread]: Finished running 2 table models in 0.45s.
12:21:10.528252 [debug] [MainThread]: Connection 'master' was properly closed.
12:21:10.528678 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:21:10.528967 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:21:10.540343 [info ] [MainThread]: 
12:21:10.540987 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:21:10.541553 [info ] [MainThread]: 
12:21:10.542165 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:21:10.542764 [error] [MainThread]:   syntax error at or near "select"
12:21:10.545946 [error] [MainThread]:   LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
12:21:10.546409 [error] [MainThread]:                ^
12:21:10.546844 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:21:10.547255 [info ] [MainThread]: 
12:21:10.547683 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:21:10.548233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7587f06910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7586557370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f758b6dd910>]}


============================== 2022-03-06 12:22:14.019597 | ddb7b315-4776-47ab-b045-895846062bf1 ==============================
12:22:14.019597 [info ] [MainThread]: Running with dbt=1.0.3
12:22:14.024260 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:22:14.024768 [debug] [MainThread]: Tracking: tracking
12:22:14.031660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e4df4b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e4df4820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e4df4b50>]}
12:22:14.108720 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:22:14.109646 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:22:14.135575 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:22:14.165866 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:22:14.178354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ddb7b315-4776-47ab-b045-895846062bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e2a4d0d0>]}
12:22:14.188106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ddb7b315-4776-47ab-b045-895846062bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e4de8460>]}
12:22:14.188651 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:22:14.191772 [info ] [MainThread]: 
12:22:14.192793 [debug] [MainThread]: Acquiring new postgres connection "master"
12:22:14.194182 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:22:14.213143 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:22:14.213604 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:22:14.213968 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:22:14.232484 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:22:14.234943 [debug] [ThreadPool]: On list_adludio: Close
12:22:14.237505 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:22:14.247145 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:22:14.247511 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:22:14.247799 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:22:14.258481 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:22:14.258891 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:22:14.259207 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:22:14.262764 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:22:14.268592 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:22:14.269069 [debug] [ThreadPool]: On list_adludio_public: Close
12:22:14.278149 [debug] [MainThread]: Using postgres connection "master"
12:22:14.278558 [debug] [MainThread]: On master: BEGIN
12:22:14.278827 [debug] [MainThread]: Opening a new connection, currently in state init
12:22:14.289661 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:22:14.290050 [debug] [MainThread]: Using postgres connection "master"
12:22:14.290326 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:22:14.324516 [debug] [MainThread]: SQL status: SELECT 4 in 0.03 seconds
12:22:14.327152 [debug] [MainThread]: On master: ROLLBACK
12:22:14.327635 [debug] [MainThread]: Using postgres connection "master"
12:22:14.327911 [debug] [MainThread]: On master: BEGIN
12:22:14.328367 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:22:14.328629 [debug] [MainThread]: On master: COMMIT
12:22:14.328859 [debug] [MainThread]: Using postgres connection "master"
12:22:14.329119 [debug] [MainThread]: On master: COMMIT
12:22:14.329547 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:22:14.330024 [debug] [MainThread]: On master: Close
12:22:14.332847 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:22:14.333353 [info ] [MainThread]: 
12:22:14.338420 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:22:14.338883 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:22:14.339689 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.339948 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:22:14.340208 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:22:14.346293 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.346844 [debug] [Thread-1  ]: finished collecting timing info
12:22:14.347168 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:22:14.456340 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.457106 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.457407 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:22:14.457687 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:22:14.468361 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:22:14.468763 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.469070 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:22:14.479531 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:22:14.493353 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.493771 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:22:14.494469 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:22:14.498280 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.498583 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:22:14.499297 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:22:14.523771 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:22:14.524243 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.524588 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:22:14.527363 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:22:14.535230 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:22:14.535566 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:22:14.538272 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:22:14.547520 [debug] [Thread-1  ]: finished collecting timing info
12:22:14.547941 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:22:14.550597 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddb7b315-4776-47ab-b045-895846062bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e54279d0>]}
12:22:14.551295 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.21s]
12:22:14.552398 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:22:14.553623 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:22:14.554034 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:22:14.554848 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:22:14.561956 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:22:14.562315 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:22:14.566824 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:22:14.567504 [debug] [Thread-1  ]: finished collecting timing info
12:22:14.567828 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:22:14.572242 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:22:14.572862 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:22:14.573145 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:22:14.573401 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:22:14.583415 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:22:14.583798 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:22:14.584105 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:22:14.584606 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "select"
LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
             ^

12:22:14.584934 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:22:14.585551 [debug] [Thread-1  ]: finished collecting timing info
12:22:14.585895 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:22:14.586565 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "select"
  LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
               ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:22:14.587119 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddb7b315-4776-47ab-b045-895846062bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e61b8490>]}
12:22:14.592300 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.03s]
12:22:14.592864 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:22:14.595263 [debug] [MainThread]: Acquiring new postgres connection "master"
12:22:14.595580 [debug] [MainThread]: Using postgres connection "master"
12:22:14.595874 [debug] [MainThread]: On master: BEGIN
12:22:14.596092 [debug] [MainThread]: Opening a new connection, currently in state closed
12:22:14.606662 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:22:14.607103 [debug] [MainThread]: On master: COMMIT
12:22:14.607428 [debug] [MainThread]: Using postgres connection "master"
12:22:14.607709 [debug] [MainThread]: On master: COMMIT
12:22:14.608094 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:22:14.608400 [debug] [MainThread]: On master: Close
12:22:14.609117 [info ] [MainThread]: 
12:22:14.609712 [info ] [MainThread]: Finished running 2 table models in 0.42s.
12:22:14.610212 [debug] [MainThread]: Connection 'master' was properly closed.
12:22:14.610494 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:22:14.610766 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:22:14.618392 [info ] [MainThread]: 
12:22:14.618946 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:22:14.619656 [info ] [MainThread]: 
12:22:14.620144 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:22:14.620599 [error] [MainThread]:   syntax error at or near "select"
12:22:14.621038 [error] [MainThread]:   LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
12:22:14.621488 [error] [MainThread]:                ^
12:22:14.622056 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:22:14.622524 [info ] [MainThread]: 
12:22:14.623076 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:22:14.623832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e29e5dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e29e55e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65e6199460>]}


============================== 2022-03-06 12:23:18.665314 | 3cfdcea3-da2f-4dd4-8815-162d6ce160be ==============================
12:23:18.665314 [info ] [MainThread]: Running with dbt=1.0.3
12:23:18.666900 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:23:18.667342 [debug] [MainThread]: Tracking: tracking
12:23:18.676439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46c9944d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46c99446d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46c9944160>]}
12:23:18.715926 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:23:18.716826 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:23:18.743676 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:23:18.779113 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:23:18.787300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3cfdcea3-da2f-4dd4-8815-162d6ce160be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46c8ff50d0>]}
12:23:18.796214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3cfdcea3-da2f-4dd4-8815-162d6ce160be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46c9945e80>]}
12:23:18.796764 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:23:18.799072 [info ] [MainThread]: 
12:23:18.799911 [debug] [MainThread]: Acquiring new postgres connection "master"
12:23:18.801298 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:23:18.816906 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:23:18.817349 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:23:18.817709 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:23:18.834685 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:23:18.837072 [debug] [ThreadPool]: On list_adludio: Close
12:23:18.838891 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:23:18.848272 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:23:18.848597 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:23:18.848876 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:23:18.859508 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:23:18.859883 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:23:18.860167 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:23:18.863460 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:23:18.865811 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:23:18.866255 [debug] [ThreadPool]: On list_adludio_public: Close
12:23:18.873830 [debug] [MainThread]: Using postgres connection "master"
12:23:18.874120 [debug] [MainThread]: On master: BEGIN
12:23:18.874321 [debug] [MainThread]: Opening a new connection, currently in state init
12:23:18.886471 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:23:18.886837 [debug] [MainThread]: Using postgres connection "master"
12:23:18.887118 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:23:18.926551 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
12:23:18.929204 [debug] [MainThread]: On master: ROLLBACK
12:23:18.929771 [debug] [MainThread]: Using postgres connection "master"
12:23:18.930101 [debug] [MainThread]: On master: BEGIN
12:23:18.930611 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:23:18.930987 [debug] [MainThread]: On master: COMMIT
12:23:18.931276 [debug] [MainThread]: Using postgres connection "master"
12:23:18.931546 [debug] [MainThread]: On master: COMMIT
12:23:18.931919 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:23:18.932264 [debug] [MainThread]: On master: Close
12:23:18.932908 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:23:18.933946 [info ] [MainThread]: 
12:23:18.942911 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:23:18.943397 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:23:18.944159 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:18.944418 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:23:18.944680 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:23:18.949983 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:23:18.950591 [debug] [Thread-1  ]: finished collecting timing info
12:23:18.950879 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:23:19.061980 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.062693 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.062973 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:23:19.063200 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:23:19.076505 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:23:19.076855 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.077088 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:23:19.087747 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:23:19.099433 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.099755 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:23:19.100727 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:23:19.104279 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.104593 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:23:19.105182 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:23:19.123130 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:23:19.123537 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.123879 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:23:19.134675 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
12:23:19.146322 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:23:19.146734 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:23:19.149614 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:23:19.151488 [debug] [Thread-1  ]: finished collecting timing info
12:23:19.151796 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:23:19.153937 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3cfdcea3-da2f-4dd4-8815-162d6ce160be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46c8f72df0>]}
12:23:19.154571 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.21s]
12:23:19.155159 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:23:19.156096 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:23:19.156498 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:23:19.157414 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:23:19.157718 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:23:19.157969 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:23:19.162679 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:23:19.163204 [debug] [Thread-1  ]: finished collecting timing info
12:23:19.163492 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:23:19.173385 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:23:19.174027 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:23:19.174284 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:23:19.174556 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:23:19.189486 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:23:19.189907 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:23:19.190203 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as
    select * from "adludio"."public"."transformed_sales_number_data"
)

select *
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:23:19.190730 [debug] [Thread-1  ]: Postgres adapter: Postgres error: syntax error at or near "select"
LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
             ^

12:23:19.191050 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: ROLLBACK
12:23:19.191706 [debug] [Thread-1  ]: finished collecting timing info
12:23:19.192041 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:23:19.192714 [debug] [Thread-1  ]: Database Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)
  syntax error at or near "select"
  LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
               ^
  compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:23:19.193200 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3cfdcea3-da2f-4dd4-8815-162d6ce160be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46cc7603d0>]}
12:23:19.193817 [error] [Thread-1  ]: 2 of 2 ERROR creating table model public.deal_value_per_week.................... [[31mERROR[0m in 0.04s]
12:23:19.194517 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:23:19.204187 [debug] [MainThread]: Acquiring new postgres connection "master"
12:23:19.204565 [debug] [MainThread]: Using postgres connection "master"
12:23:19.204821 [debug] [MainThread]: On master: BEGIN
12:23:19.205067 [debug] [MainThread]: Opening a new connection, currently in state closed
12:23:19.221385 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
12:23:19.221871 [debug] [MainThread]: On master: COMMIT
12:23:19.222212 [debug] [MainThread]: Using postgres connection "master"
12:23:19.222496 [debug] [MainThread]: On master: COMMIT
12:23:19.223075 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:23:19.223391 [debug] [MainThread]: On master: Close
12:23:19.224086 [info ] [MainThread]: 
12:23:19.224628 [info ] [MainThread]: Finished running 2 table models in 0.42s.
12:23:19.225099 [debug] [MainThread]: Connection 'master' was properly closed.
12:23:19.225398 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:23:19.226229 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:23:19.238180 [info ] [MainThread]: 
12:23:19.239212 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
12:23:19.240439 [info ] [MainThread]: 
12:23:19.240953 [error] [MainThread]: [33mDatabase Error in model deal_value_per_week (models/Sales Numbers/deal_value_per_week.sql)[0m
12:23:19.241426 [error] [MainThread]:   syntax error at or near "select"
12:23:19.241983 [error] [MainThread]:   LINE 17:     select * from "adludio"."public"."transformed_sales_numb...
12:23:19.242406 [error] [MainThread]:                ^
12:23:19.242895 [error] [MainThread]:   compiled SQL at target/run/Analytics_dbt/models/Sales Numbers/deal_value_per_week.sql
12:23:19.243361 [info ] [MainThread]: 
12:23:19.243872 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
12:23:19.244479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46b35c7160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46b35c70a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46cc7583a0>]}


============================== 2022-03-06 12:24:20.886314 | ef68da2d-fd60-4578-a06c-7505ec664095 ==============================
12:24:20.886314 [info ] [MainThread]: Running with dbt=1.0.3
12:24:20.887624 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:24:20.888055 [debug] [MainThread]: Tracking: tracking
12:24:20.900906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc9e01d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc9e016d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc9e01160>]}
12:24:20.955159 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:24:20.956016 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:24:20.979377 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:24:21.015101 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:24:21.024844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ef68da2d-fd60-4578-a06c-7505ec664095', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc94b10d0>]}
12:24:21.036686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ef68da2d-fd60-4578-a06c-7505ec664095', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc9df3e80>]}
12:24:21.037271 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:24:21.039947 [info ] [MainThread]: 
12:24:21.040839 [debug] [MainThread]: Acquiring new postgres connection "master"
12:24:21.042368 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:24:21.061527 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:24:21.061982 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:24:21.062296 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:24:21.078378 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:24:21.080864 [debug] [ThreadPool]: On list_adludio: Close
12:24:21.082764 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:24:21.092422 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:24:21.092804 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:24:21.093080 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:24:21.104759 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:24:21.105292 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:24:21.105668 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:24:21.109095 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:24:21.112007 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:24:21.112344 [debug] [ThreadPool]: On list_adludio_public: Close
12:24:21.125631 [debug] [MainThread]: Using postgres connection "master"
12:24:21.126055 [debug] [MainThread]: On master: BEGIN
12:24:21.126355 [debug] [MainThread]: Opening a new connection, currently in state init
12:24:21.140118 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:24:21.140550 [debug] [MainThread]: Using postgres connection "master"
12:24:21.140844 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:24:21.176856 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
12:24:21.179463 [debug] [MainThread]: On master: ROLLBACK
12:24:21.179999 [debug] [MainThread]: Using postgres connection "master"
12:24:21.180326 [debug] [MainThread]: On master: BEGIN
12:24:21.180840 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:24:21.181152 [debug] [MainThread]: On master: COMMIT
12:24:21.181435 [debug] [MainThread]: Using postgres connection "master"
12:24:21.181765 [debug] [MainThread]: On master: COMMIT
12:24:21.182151 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:24:21.182481 [debug] [MainThread]: On master: Close
12:24:21.183121 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:24:21.184005 [info ] [MainThread]: 
12:24:21.198608 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:24:21.199218 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:24:21.200032 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.200300 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:24:21.200573 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:24:21.212587 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.213310 [debug] [Thread-1  ]: finished collecting timing info
12:24:21.219462 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:24:21.340902 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.341681 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.341969 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:24:21.342201 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:24:21.352707 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:24:21.353106 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.353501 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:24:21.363438 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:24:21.373876 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.374314 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:24:21.375042 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:24:21.381940 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.382269 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:24:21.382917 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:24:21.400061 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:24:21.400439 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.400679 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:24:21.403560 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:24:21.415793 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:24:21.416111 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:24:21.424137 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:24:21.426558 [debug] [Thread-1  ]: finished collecting timing info
12:24:21.427007 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:24:21.427769 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ef68da2d-fd60-4578-a06c-7505ec664095', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc942a220>]}
12:24:21.428390 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.23s]
12:24:21.429243 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:24:21.430114 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:24:21.430477 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:24:21.431092 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.431303 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:24:21.431505 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:24:21.435861 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:24:21.439736 [debug] [Thread-1  ]: finished collecting timing info
12:24:21.440046 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:24:21.444549 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:24:21.445131 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.445377 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:24:21.445709 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:24:21.456421 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:24:21.456862 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.457173 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value), week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:24:21.460751 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
12:24:21.464922 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.465258 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
12:24:21.465981 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:24:21.470083 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.470356 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
12:24:21.470929 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:24:21.473437 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:24:21.473752 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.473962 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:24:21.475046 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:24:21.477819 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:24:21.478100 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
12:24:21.480067 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:24:21.482039 [debug] [Thread-1  ]: finished collecting timing info
12:24:21.482379 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:24:21.483299 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ef68da2d-fd60-4578-a06c-7505ec664095', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdcc0f89d0>]}
12:24:21.483860 [info ] [Thread-1  ]: 2 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.05s]
12:24:21.484616 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:24:21.486061 [debug] [MainThread]: Acquiring new postgres connection "master"
12:24:21.486356 [debug] [MainThread]: Using postgres connection "master"
12:24:21.486584 [debug] [MainThread]: On master: BEGIN
12:24:21.486831 [debug] [MainThread]: Opening a new connection, currently in state closed
12:24:21.497298 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:24:21.497782 [debug] [MainThread]: On master: COMMIT
12:24:21.498191 [debug] [MainThread]: Using postgres connection "master"
12:24:21.498464 [debug] [MainThread]: On master: COMMIT
12:24:21.498861 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:24:21.499199 [debug] [MainThread]: On master: Close
12:24:21.499940 [info ] [MainThread]: 
12:24:21.500708 [info ] [MainThread]: Finished running 2 table models in 0.46s.
12:24:21.501342 [debug] [MainThread]: Connection 'master' was properly closed.
12:24:21.501687 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:24:21.502041 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:24:21.510902 [info ] [MainThread]: 
12:24:21.511495 [info ] [MainThread]: [32mCompleted successfully[0m
12:24:21.512052 [info ] [MainThread]: 
12:24:21.515612 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
12:24:21.516208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc9d429d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc944a430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcdc8488ee0>]}


============================== 2022-03-06 12:25:28.695073 | f9ad3b01-6dcd-4163-8bcf-2d783f988d4c ==============================
12:25:28.695073 [info ] [MainThread]: Running with dbt=1.0.3
12:25:28.696285 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:25:28.696677 [debug] [MainThread]: Tracking: tracking
12:25:28.701989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442ec153a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442ec15160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442ec15520>]}
12:25:28.749332 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:25:28.750213 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/deal_value_per_week.sql
12:25:28.780494 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/deal_value_per_week.sql
12:25:28.810827 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:25:28.820039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f9ad3b01-6dcd-4163-8bcf-2d783f988d4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442c86c0d0>]}
12:25:28.833151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f9ad3b01-6dcd-4163-8bcf-2d783f988d4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442ec08460>]}
12:25:28.833798 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:25:28.836828 [info ] [MainThread]: 
12:25:28.838040 [debug] [MainThread]: Acquiring new postgres connection "master"
12:25:28.841097 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:25:28.862820 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:25:28.863221 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:25:28.863527 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:25:28.881844 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
12:25:28.884348 [debug] [ThreadPool]: On list_adludio: Close
12:25:28.888249 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:25:28.899091 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:25:28.899548 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:25:28.899859 [debug] [ThreadPool]: Opening a new connection, currently in state closed
12:25:28.913762 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:25:28.914147 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:25:28.914404 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:25:28.922515 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.01 seconds
12:25:28.924832 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:25:28.925293 [debug] [ThreadPool]: On list_adludio_public: Close
12:25:28.931949 [debug] [MainThread]: Using postgres connection "master"
12:25:28.932284 [debug] [MainThread]: On master: BEGIN
12:25:28.932522 [debug] [MainThread]: Opening a new connection, currently in state init
12:25:28.944954 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:25:28.945331 [debug] [MainThread]: Using postgres connection "master"
12:25:28.945653 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:25:28.994634 [debug] [MainThread]: SQL status: SELECT 4 in 0.05 seconds
12:25:28.997293 [debug] [MainThread]: On master: ROLLBACK
12:25:28.997925 [debug] [MainThread]: Using postgres connection "master"
12:25:28.998264 [debug] [MainThread]: On master: BEGIN
12:25:28.998746 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:25:28.999065 [debug] [MainThread]: On master: COMMIT
12:25:28.999370 [debug] [MainThread]: Using postgres connection "master"
12:25:28.999649 [debug] [MainThread]: On master: COMMIT
12:25:29.000016 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:25:29.000310 [debug] [MainThread]: On master: Close
12:25:29.000951 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:25:29.002084 [info ] [MainThread]: 
12:25:29.007329 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:25:29.007845 [info ] [Thread-1  ]: 1 of 2 START table model public.transformed_sales_number_data................... [RUN]
12:25:29.008678 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.008943 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:25:29.009205 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:25:29.023045 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.023951 [debug] [Thread-1  ]: finished collecting timing info
12:25:29.024314 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:25:29.145998 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.146864 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.147171 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:25:29.147422 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:25:29.161338 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:25:29.161861 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.162147 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:25:29.172173 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:25:29.182779 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.183299 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:25:29.184103 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:25:29.187784 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.188103 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:25:29.191805 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:25:29.212405 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:25:29.212895 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.213182 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:25:29.215900 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:25:29.223727 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:25:29.224196 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:25:29.229513 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:25:29.231959 [debug] [Thread-1  ]: finished collecting timing info
12:25:29.232388 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:25:29.235388 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9ad3b01-6dcd-4163-8bcf-2d783f988d4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442d0d6a00>]}
12:25:29.236095 [info ] [Thread-1  ]: 1 of 2 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.23s]
12:25:29.240257 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:25:29.241803 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:25:29.245972 [info ] [Thread-1  ]: 2 of 2 START table model public.deal_value_per_week............................. [RUN]
12:25:29.258532 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.258979 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:25:29.259444 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:25:29.264921 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:25:29.265717 [debug] [Thread-1  ]: finished collecting timing info
12:25:29.269158 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:25:29.273819 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:25:29.274608 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.274922 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:25:29.275198 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:25:29.298449 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
12:25:29.298938 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.299305 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:25:29.308279 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
12:25:29.311177 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.311452 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
12:25:29.313132 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:25:29.317010 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.317358 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
12:25:29.318063 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:25:29.320780 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:25:29.321107 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.321404 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:25:29.322845 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:25:29.325671 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:25:29.325986 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
12:25:29.331208 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:25:29.333237 [debug] [Thread-1  ]: finished collecting timing info
12:25:29.333643 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:25:29.334355 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9ad3b01-6dcd-4163-8bcf-2d783f988d4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442c806460>]}
12:25:29.334995 [info ] [Thread-1  ]: 2 of 2 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.08s]
12:25:29.335829 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:25:29.337646 [debug] [MainThread]: Acquiring new postgres connection "master"
12:25:29.337977 [debug] [MainThread]: Using postgres connection "master"
12:25:29.338201 [debug] [MainThread]: On master: BEGIN
12:25:29.338409 [debug] [MainThread]: Opening a new connection, currently in state closed
12:25:29.348814 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:25:29.349280 [debug] [MainThread]: On master: COMMIT
12:25:29.349653 [debug] [MainThread]: Using postgres connection "master"
12:25:29.349922 [debug] [MainThread]: On master: COMMIT
12:25:29.350304 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:25:29.350618 [debug] [MainThread]: On master: Close
12:25:29.351315 [info ] [MainThread]: 
12:25:29.352223 [info ] [MainThread]: Finished running 2 table models in 0.51s.
12:25:29.353380 [debug] [MainThread]: Connection 'master' was properly closed.
12:25:29.353709 [debug] [MainThread]: Connection 'model.Analytics_dbt.deal_value_per_week' was properly closed.
12:25:29.361721 [info ] [MainThread]: 
12:25:29.362754 [info ] [MainThread]: [32mCompleted successfully[0m
12:25:29.367071 [info ] [MainThread]: 
12:25:29.368041 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
12:25:29.369434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442c24c220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442c8126d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f442c23ef70>]}


============================== 2022-03-06 12:39:59.224101 | 05942e40-1c2b-4a64-91aa-03ea1ded67c6 ==============================
12:39:59.224101 [info ] [MainThread]: Running with dbt=1.0.3
12:39:59.225656 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:39:59.226160 [debug] [MainThread]: Tracking: tracking
12:39:59.231938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c3a7ab80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c3a7a1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c3a7a0d0>]}
12:39:59.276639 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
12:39:59.277359 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/email_per_week copy.sql
12:39:59.307429 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/email_per_week copy.sql
12:39:59.339342 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:39:59.349035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '05942e40-1c2b-4a64-91aa-03ea1ded67c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c16d30d0>]}
12:39:59.358163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '05942e40-1c2b-4a64-91aa-03ea1ded67c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c3a6ed30>]}
12:39:59.358709 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:39:59.361193 [info ] [MainThread]: 
12:39:59.362034 [debug] [MainThread]: Acquiring new postgres connection "master"
12:39:59.363707 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:39:59.382326 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:39:59.382725 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:39:59.383024 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:39:59.396196 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
12:39:59.398727 [debug] [ThreadPool]: On list_adludio: Close
12:39:59.401336 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:39:59.410506 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:39:59.410868 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:39:59.411170 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:39:59.421735 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:39:59.422075 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:39:59.422438 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:39:59.425732 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.0 seconds
12:39:59.427986 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:39:59.428378 [debug] [ThreadPool]: On list_adludio_public: Close
12:39:59.435143 [debug] [MainThread]: Using postgres connection "master"
12:39:59.435441 [debug] [MainThread]: On master: BEGIN
12:39:59.435785 [debug] [MainThread]: Opening a new connection, currently in state init
12:39:59.447331 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:39:59.447667 [debug] [MainThread]: Using postgres connection "master"
12:39:59.447917 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:39:59.489642 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
12:39:59.493825 [debug] [MainThread]: On master: ROLLBACK
12:39:59.494336 [debug] [MainThread]: Using postgres connection "master"
12:39:59.494677 [debug] [MainThread]: On master: BEGIN
12:39:59.495196 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:39:59.495521 [debug] [MainThread]: On master: COMMIT
12:39:59.495805 [debug] [MainThread]: Using postgres connection "master"
12:39:59.496067 [debug] [MainThread]: On master: COMMIT
12:39:59.496465 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:39:59.496760 [debug] [MainThread]: On master: Close
12:39:59.497405 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:39:59.498212 [info ] [MainThread]: 
12:39:59.509853 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:39:59.510337 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
12:39:59.511341 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.511614 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:39:59.511897 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:39:59.517492 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.518129 [debug] [Thread-1  ]: finished collecting timing info
12:39:59.518456 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:39:59.624385 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.625177 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.625496 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:39:59.625758 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:39:59.636690 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:39:59.637008 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.637290 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:39:59.647622 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:39:59.658047 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.658363 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:39:59.659022 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:39:59.662668 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.662997 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:39:59.663615 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:39:59.683004 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:39:59.683396 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.683667 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:39:59.686154 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:39:59.697338 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:39:59.697689 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:39:59.700361 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:39:59.702238 [debug] [Thread-1  ]: finished collecting timing info
12:39:59.702600 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:39:59.703381 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05942e40-1c2b-4a64-91aa-03ea1ded67c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c409e190>]}
12:39:59.703998 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.19s]
12:39:59.704829 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:39:59.705817 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:39:59.706377 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
12:39:59.707160 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.707424 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:39:59.707675 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:39:59.712025 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:39:59.712539 [debug] [Thread-1  ]: finished collecting timing info
12:39:59.712817 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:39:59.716951 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:39:59.717495 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.717761 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:39:59.717994 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:39:59.728842 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:39:59.729164 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.729411 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:39:59.736729 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
12:39:59.740724 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.741072 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
12:39:59.741684 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:39:59.745279 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.745591 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
12:39:59.746278 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:39:59.749048 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:39:59.749336 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.749644 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:39:59.750791 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:39:59.753402 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:39:59.753732 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
12:39:59.755664 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:39:59.757329 [debug] [Thread-1  ]: finished collecting timing info
12:39:59.757699 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:39:59.758397 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05942e40-1c2b-4a64-91aa-03ea1ded67c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c4e29d00>]}
12:39:59.759063 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.05s]
12:39:59.760148 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:39:59.760632 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week copy
12:39:59.761613 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week copy............................. [RUN]
12:39:59.762724 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week copy"
12:39:59.763041 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week copy
12:39:59.763741 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week copy
12:39:59.767884 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week copy"
12:39:59.768521 [debug] [Thread-1  ]: finished collecting timing info
12:39:59.768871 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week copy
12:39:59.773098 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week copy"
12:39:59.773744 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week copy"
12:39:59.774033 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: BEGIN
12:39:59.774298 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:39:59.785569 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:39:59.785899 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week copy"
12:39:59.786167 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week copy"} */


  create  table "adludio"."public"."email_per_week copy__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:39:59.789609 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
12:39:59.793712 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week copy"
12:39:59.794019 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week copy"} */
alter table "adludio"."public"."email_per_week copy__dbt_tmp" rename to "email_per_week copy"
12:39:59.794611 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:39:59.797184 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: COMMIT
12:39:59.797525 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week copy"
12:39:59.797886 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: COMMIT
12:39:59.799271 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:39:59.803903 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week copy"
12:39:59.804198 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week copy"} */
drop table if exists "adludio"."public"."email_per_week copy__dbt_backup" cascade
12:39:59.804716 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:39:59.806396 [debug] [Thread-1  ]: finished collecting timing info
12:39:59.806740 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week copy: Close
12:39:59.810659 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05942e40-1c2b-4a64-91aa-03ea1ded67c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c06ce040>]}
12:39:59.811244 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week copy........................ [[32mSELECT 53[0m in 0.05s]
12:39:59.811822 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week copy
12:39:59.813552 [debug] [MainThread]: Acquiring new postgres connection "master"
12:39:59.813849 [debug] [MainThread]: Using postgres connection "master"
12:39:59.814105 [debug] [MainThread]: On master: BEGIN
12:39:59.814320 [debug] [MainThread]: Opening a new connection, currently in state closed
12:39:59.826804 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:39:59.827215 [debug] [MainThread]: On master: COMMIT
12:39:59.827476 [debug] [MainThread]: Using postgres connection "master"
12:39:59.827706 [debug] [MainThread]: On master: COMMIT
12:39:59.828042 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:39:59.828312 [debug] [MainThread]: On master: Close
12:39:59.829125 [info ] [MainThread]: 
12:39:59.829863 [info ] [MainThread]: Finished running 3 table models in 0.47s.
12:39:59.830579 [debug] [MainThread]: Connection 'master' was properly closed.
12:39:59.830848 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:39:59.831091 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week copy' was properly closed.
12:39:59.839130 [info ] [MainThread]: 
12:39:59.839685 [info ] [MainThread]: [32mCompleted successfully[0m
12:39:59.840228 [info ] [MainThread]: 
12:39:59.840625 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
12:39:59.841155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c1669730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c1669790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2c065a250>]}


============================== 2022-03-06 12:40:16.210991 | 1de40758-4fc9-4ec9-b48a-6c8622a456c0 ==============================
12:40:16.210991 [info ] [MainThread]: Running with dbt=1.0.3
12:40:16.212795 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:40:16.213475 [debug] [MainThread]: Tracking: tracking
12:40:16.223554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b0b8d2c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b0b8d2820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b0b8d2af0>]}
12:40:16.274798 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
12:40:16.275571 [debug] [MainThread]: Partial parsing: added file: Analytics_dbt://models/Sales Numbers/email_per_week.sql
12:40:16.275950 [debug] [MainThread]: Partial parsing: deleted file: Analytics_dbt://models/Sales Numbers/email_per_week copy.sql
12:40:16.298986 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/email_per_week.sql
12:40:16.321923 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:40:16.331152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1de40758-4fc9-4ec9-b48a-6c8622a456c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b095270d0>]}
12:40:16.340593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1de40758-4fc9-4ec9-b48a-6c8622a456c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b09e46820>]}
12:40:16.341192 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:40:16.343531 [info ] [MainThread]: 
12:40:16.347407 [debug] [MainThread]: Acquiring new postgres connection "master"
12:40:16.348817 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:40:16.370974 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:40:16.371397 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:40:16.371729 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:40:16.385286 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
12:40:16.387967 [debug] [ThreadPool]: On list_adludio: Close
12:40:16.391700 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:40:16.401088 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:40:16.401520 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:40:16.401853 [debug] [ThreadPool]: Opening a new connection, currently in state closed
12:40:16.412882 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:40:16.413335 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:40:16.413712 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:40:16.417022 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.0 seconds
12:40:16.419305 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:40:16.419730 [debug] [ThreadPool]: On list_adludio_public: Close
12:40:16.426588 [debug] [MainThread]: Using postgres connection "master"
12:40:16.426960 [debug] [MainThread]: On master: BEGIN
12:40:16.427312 [debug] [MainThread]: Opening a new connection, currently in state init
12:40:16.438565 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:40:16.438997 [debug] [MainThread]: Using postgres connection "master"
12:40:16.439326 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:40:16.481969 [debug] [MainThread]: SQL status: SELECT 4 in 0.04 seconds
12:40:16.484607 [debug] [MainThread]: On master: ROLLBACK
12:40:16.485070 [debug] [MainThread]: Using postgres connection "master"
12:40:16.485398 [debug] [MainThread]: On master: BEGIN
12:40:16.485941 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:40:16.486254 [debug] [MainThread]: On master: COMMIT
12:40:16.486542 [debug] [MainThread]: Using postgres connection "master"
12:40:16.486838 [debug] [MainThread]: On master: COMMIT
12:40:16.487215 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:40:16.487523 [debug] [MainThread]: On master: Close
12:40:16.488230 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:40:16.489005 [info ] [MainThread]: 
12:40:16.499557 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:40:16.500057 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
12:40:16.501065 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.501345 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:40:16.501645 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:40:16.559200 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.560006 [debug] [Thread-1  ]: finished collecting timing info
12:40:16.560385 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:40:16.611958 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.612811 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.613185 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:40:16.613793 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:40:16.630578 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
12:40:16.631062 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.631394 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:40:16.646436 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
12:40:16.656649 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.657086 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:40:16.657833 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:16.661484 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.661847 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:40:16.662555 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:16.687310 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:40:16.687684 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.687945 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:40:16.690274 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:40:16.701925 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:16.702265 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:40:16.705423 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:40:16.707292 [debug] [Thread-1  ]: finished collecting timing info
12:40:16.707625 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:40:16.708282 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1de40758-4fc9-4ec9-b48a-6c8622a456c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b09d6e130>]}
12:40:16.708859 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.21s]
12:40:16.709414 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:40:16.710415 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:40:16.710870 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
12:40:16.711610 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.711945 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:40:16.712245 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:40:16.716579 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:40:16.717226 [debug] [Thread-1  ]: finished collecting timing info
12:40:16.717564 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:40:16.721856 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:40:16.722556 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.722899 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:40:16.723186 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:40:16.742195 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
12:40:16.742689 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.743008 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:40:16.746402 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
12:40:16.750410 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.750733 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
12:40:16.751329 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:16.756481 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.756858 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
12:40:16.757513 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:16.760110 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:40:16.760416 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.760687 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:40:16.761991 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:40:16.764648 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:16.764947 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
12:40:16.767738 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:40:16.769501 [debug] [Thread-1  ]: finished collecting timing info
12:40:16.769848 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:40:16.770567 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1de40758-4fc9-4ec9-b48a-6c8622a456c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b0949ca60>]}
12:40:16.771215 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.06s]
12:40:16.772028 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:40:16.772471 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
12:40:16.773014 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week.................................. [RUN]
12:40:16.773957 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
12:40:16.774271 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
12:40:16.774567 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
12:40:16.779235 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
12:40:16.779841 [debug] [Thread-1  ]: finished collecting timing info
12:40:16.780189 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
12:40:16.784550 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
12:40:16.785210 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:16.785533 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
12:40:16.789172 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:40:16.801196 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:40:16.801618 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:16.801924 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:40:16.805903 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
12:40:16.811960 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:16.812294 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
12:40:16.814375 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:16.816943 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
12:40:16.817237 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:16.817495 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
12:40:16.825021 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
12:40:16.827896 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:16.828439 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
12:40:16.828985 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:40:16.830901 [debug] [Thread-1  ]: finished collecting timing info
12:40:16.831236 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
12:40:16.831884 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1de40758-4fc9-4ec9-b48a-6c8622a456c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b084b2fd0>]}
12:40:16.832439 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week............................. [[32mSELECT 53[0m in 0.06s]
12:40:16.833476 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
12:40:16.835330 [debug] [MainThread]: Acquiring new postgres connection "master"
12:40:16.835620 [debug] [MainThread]: Using postgres connection "master"
12:40:16.835839 [debug] [MainThread]: On master: BEGIN
12:40:16.836088 [debug] [MainThread]: Opening a new connection, currently in state closed
12:40:16.855100 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
12:40:16.855570 [debug] [MainThread]: On master: COMMIT
12:40:16.855856 [debug] [MainThread]: Using postgres connection "master"
12:40:16.856106 [debug] [MainThread]: On master: COMMIT
12:40:16.856460 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:40:16.856751 [debug] [MainThread]: On master: Close
12:40:16.861053 [info ] [MainThread]: 
12:40:16.861594 [info ] [MainThread]: Finished running 3 table models in 0.51s.
12:40:16.862338 [debug] [MainThread]: Connection 'master' was properly closed.
12:40:16.863289 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week' was properly closed.
12:40:16.870887 [info ] [MainThread]: 
12:40:16.871380 [info ] [MainThread]: [32mCompleted successfully[0m
12:40:16.871915 [info ] [MainThread]: 
12:40:16.872377 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
12:40:16.872947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b084b9130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b084b9100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b084fa9d0>]}


============================== 2022-03-06 12:40:54.199328 | 0981a7e3-053a-4869-a61a-4b25b441aef8 ==============================
12:40:54.199328 [info ] [MainThread]: Running with dbt=1.0.3
12:40:54.200797 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
12:40:54.201257 [debug] [MainThread]: Tracking: tracking
12:40:54.208007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5abb8d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5abb86d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5abb8160>]}
12:40:54.264627 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
12:40:54.266201 [debug] [MainThread]: Partial parsing: updated file: Analytics_dbt://models/Sales Numbers/email_per_week.sql
12:40:54.300560 [debug] [MainThread]: 1699: static parser successfully parsed Sales Numbers/email_per_week.sql
12:40:54.343895 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

12:40:54.352399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0981a7e3-053a-4869-a61a-4b25b441aef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5a26eaf0>]}
12:40:54.363824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0981a7e3-053a-4869-a61a-4b25b441aef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5ab0b970>]}
12:40:54.364543 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:40:54.367882 [info ] [MainThread]: 
12:40:54.369271 [debug] [MainThread]: Acquiring new postgres connection "master"
12:40:54.374483 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
12:40:54.403541 [debug] [ThreadPool]: Using postgres connection "list_adludio"
12:40:54.404021 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
12:40:54.404318 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:40:54.418618 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
12:40:54.421143 [debug] [ThreadPool]: On list_adludio: Close
12:40:54.427293 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
12:40:54.437615 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:40:54.438095 [debug] [ThreadPool]: On list_adludio_public: BEGIN
12:40:54.438411 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:40:54.452848 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
12:40:54.453291 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
12:40:54.453612 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
12:40:54.459917 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.01 seconds
12:40:54.462471 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
12:40:54.462992 [debug] [ThreadPool]: On list_adludio_public: Close
12:40:54.475547 [debug] [MainThread]: Using postgres connection "master"
12:40:54.476008 [debug] [MainThread]: On master: BEGIN
12:40:54.476309 [debug] [MainThread]: Opening a new connection, currently in state init
12:40:54.491041 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:40:54.491522 [debug] [MainThread]: Using postgres connection "master"
12:40:54.491867 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
12:40:54.576082 [debug] [MainThread]: SQL status: SELECT 4 in 0.08 seconds
12:40:54.578753 [debug] [MainThread]: On master: ROLLBACK
12:40:54.579387 [debug] [MainThread]: Using postgres connection "master"
12:40:54.579824 [debug] [MainThread]: On master: BEGIN
12:40:54.580369 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
12:40:54.580752 [debug] [MainThread]: On master: COMMIT
12:40:54.581049 [debug] [MainThread]: Using postgres connection "master"
12:40:54.581383 [debug] [MainThread]: On master: COMMIT
12:40:54.581825 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:40:54.582141 [debug] [MainThread]: On master: Close
12:40:54.582848 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:40:54.583665 [info ] [MainThread]: 
12:40:54.618331 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
12:40:54.618924 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
12:40:54.619741 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.620018 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
12:40:54.620264 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
12:40:54.727846 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.728547 [debug] [Thread-1  ]: finished collecting timing info
12:40:54.728827 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
12:40:54.776249 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.780302 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.781199 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
12:40:54.781420 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:40:54.792942 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
12:40:54.793409 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.793779 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:40:54.814208 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.02 seconds
12:40:54.833005 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.833342 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
12:40:54.840823 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:54.844213 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.848132 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
12:40:54.850464 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:54.878049 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:40:54.881437 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.881845 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
12:40:54.884322 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:40:54.895893 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
12:40:54.896419 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
12:40:54.903484 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
12:40:54.914833 [debug] [Thread-1  ]: finished collecting timing info
12:40:54.915353 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
12:40:54.916177 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0981a7e3-053a-4869-a61a-4b25b441aef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5c5f5c40>]}
12:40:54.916838 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.30s]
12:40:54.918155 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
12:40:54.919555 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
12:40:54.921993 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
12:40:54.922936 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.923200 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
12:40:54.923426 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
12:40:54.927511 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
12:40:54.928841 [debug] [Thread-1  ]: finished collecting timing info
12:40:54.929178 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
12:40:54.933773 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
12:40:54.934442 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.934732 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
12:40:54.934994 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:40:54.953925 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
12:40:54.954326 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.954691 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:40:54.958684 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
12:40:54.966976 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.967350 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
12:40:54.968000 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:54.971810 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.972118 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
12:40:54.972697 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:54.975217 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:40:54.975521 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.975778 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
12:40:54.978448 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:40:54.981331 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
12:40:54.981688 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
12:40:54.983649 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:40:54.985521 [debug] [Thread-1  ]: finished collecting timing info
12:40:54.985867 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
12:40:54.986589 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0981a7e3-053a-4869-a61a-4b25b441aef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5cc59790>]}
12:40:54.987155 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.06s]
12:40:54.987701 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
12:40:54.988407 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
12:40:54.988930 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week.................................. [RUN]
12:40:54.989731 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
12:40:54.990028 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
12:40:54.990325 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
12:40:54.994589 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
12:40:54.995202 [debug] [Thread-1  ]: finished collecting timing info
12:40:54.995498 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
12:40:54.999829 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
12:40:55.000525 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:55.000809 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
12:40:55.001070 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
12:40:55.017179 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
12:40:55.017655 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:55.017980 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_email_messages_count) as avg_email_count, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
12:40:55.024842 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
12:40:55.031293 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:55.031651 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week" rename to "email_per_week__dbt_backup"
12:40:55.032303 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:55.035814 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:55.036184 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
12:40:55.036824 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
12:40:55.039400 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
12:40:55.039718 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:55.039975 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
12:40:55.041294 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
12:40:55.044023 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
12:40:55.044307 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
12:40:55.046425 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
12:40:55.048250 [debug] [Thread-1  ]: finished collecting timing info
12:40:55.048597 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
12:40:55.051322 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0981a7e3-053a-4869-a61a-4b25b441aef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e587f3fd0>]}
12:40:55.051933 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week............................. [[32mSELECT 53[0m in 0.06s]
12:40:55.054106 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
12:40:55.055948 [debug] [MainThread]: Acquiring new postgres connection "master"
12:40:55.056197 [debug] [MainThread]: Using postgres connection "master"
12:40:55.056348 [debug] [MainThread]: On master: BEGIN
12:40:55.056483 [debug] [MainThread]: Opening a new connection, currently in state closed
12:40:55.066848 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
12:40:55.067333 [debug] [MainThread]: On master: COMMIT
12:40:55.067652 [debug] [MainThread]: Using postgres connection "master"
12:40:55.067944 [debug] [MainThread]: On master: COMMIT
12:40:55.068327 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
12:40:55.068625 [debug] [MainThread]: On master: Close
12:40:55.071139 [info ] [MainThread]: 
12:40:55.072113 [info ] [MainThread]: Finished running 3 table models in 0.70s.
12:40:55.073386 [debug] [MainThread]: Connection 'master' was properly closed.
12:40:55.073692 [debug] [MainThread]: Connection 'list_adludio' was properly closed.
12:40:55.090060 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week' was properly closed.
12:40:55.099058 [info ] [MainThread]: 
12:40:55.099672 [info ] [MainThread]: [32mCompleted successfully[0m
12:40:55.100222 [info ] [MainThread]: 
12:40:55.100686 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
12:40:55.101304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5883bcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5883bd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0e5886a5e0>]}


============================== 2022-03-06 13:22:32.159716 | fbe89c67-7760-4a1a-927e-3e66c01ed02a ==============================
13:22:32.159716 [info ] [MainThread]: Running with dbt=1.0.3
13:22:32.160744 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/home/abreham/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
13:22:32.161152 [debug] [MainThread]: Tracking: tracking
13:22:32.166787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32dab49190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32dab49370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32dab497c0>]}
13:22:32.246399 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
13:22:32.246813 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
13:22:32.247573 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.Analytics_dbt.example

13:22:32.258389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fbe89c67-7760-4a1a-927e-3e66c01ed02a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32daa470d0>]}
13:22:32.267759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fbe89c67-7760-4a1a-927e-3e66c01ed02a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32dab18880>]}
13:22:32.268290 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 165 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
13:22:32.272659 [info ] [MainThread]: 
13:22:32.273612 [debug] [MainThread]: Acquiring new postgres connection "master"
13:22:32.275341 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio"
13:22:32.290970 [debug] [ThreadPool]: Using postgres connection "list_adludio"
13:22:32.291356 [debug] [ThreadPool]: On list_adludio: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio"} */

    select distinct nspname from pg_namespace
  
13:22:32.291618 [debug] [ThreadPool]: Opening a new connection, currently in state init
13:22:32.314790 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
13:22:32.317367 [debug] [ThreadPool]: On list_adludio: Close
13:22:32.319292 [debug] [ThreadPool]: Acquiring new postgres connection "list_adludio_public"
13:22:32.328645 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
13:22:32.329038 [debug] [ThreadPool]: On list_adludio_public: BEGIN
13:22:32.329324 [debug] [ThreadPool]: Opening a new connection, currently in state closed
13:22:32.340033 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
13:22:32.340456 [debug] [ThreadPool]: Using postgres connection "list_adludio_public"
13:22:32.340802 [debug] [ThreadPool]: On list_adludio_public: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "list_adludio_public"} */
select
      'adludio' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'adludio' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
  
13:22:32.344104 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.0 seconds
13:22:32.346460 [debug] [ThreadPool]: On list_adludio_public: ROLLBACK
13:22:32.346898 [debug] [ThreadPool]: On list_adludio_public: Close
13:22:32.353966 [debug] [MainThread]: Using postgres connection "master"
13:22:32.354402 [debug] [MainThread]: On master: BEGIN
13:22:32.354700 [debug] [MainThread]: Opening a new connection, currently in state init
13:22:32.368449 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
13:22:32.368932 [debug] [MainThread]: Using postgres connection "master"
13:22:32.369239 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
13:22:32.441493 [debug] [MainThread]: SQL status: SELECT 5 in 0.07 seconds
13:22:32.444096 [debug] [MainThread]: On master: ROLLBACK
13:22:32.444716 [debug] [MainThread]: Using postgres connection "master"
13:22:32.445037 [debug] [MainThread]: On master: BEGIN
13:22:32.445603 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
13:22:32.445909 [debug] [MainThread]: On master: COMMIT
13:22:32.446182 [debug] [MainThread]: Using postgres connection "master"
13:22:32.446439 [debug] [MainThread]: On master: COMMIT
13:22:32.446801 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
13:22:32.447082 [debug] [MainThread]: On master: Close
13:22:32.447708 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
13:22:32.448418 [info ] [MainThread]: 
13:22:32.454958 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.transformed_sales_number_data
13:22:32.455509 [info ] [Thread-1  ]: 1 of 3 START table model public.transformed_sales_number_data................... [RUN]
13:22:32.462436 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.462773 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.transformed_sales_number_data
13:22:32.463120 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.transformed_sales_number_data
13:22:32.468810 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.469574 [debug] [Thread-1  ]: finished collecting timing info
13:22:32.469908 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.transformed_sales_number_data
13:22:32.539614 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.540550 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.540893 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: BEGIN
13:22:32.541165 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:22:32.563251 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
13:22:32.566964 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.567362 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */


  create  table "adludio"."public"."transformed_sales_number_data__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select *, DATE_PART('week', to_date("Deal_created_at", 'DD/MM/YYYY')) as week from sales_table
)

select "Deal_id" as id, "Deal_created_at" as Deal_created_at,
"Deal_Value" as deal_value, "Deal_Stage" as deal_Stage,
"Deal _Status" as deal_status, "Deal_Email_messages_count" as deal_email_messages_count,
"Deal_Total_activities" as deal_total_activities, "Deal_Currency" as deal_currency,
"Deal_Region" as deal_region, week
from source_data
ORDER BY week
 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
13:22:32.581922 [debug] [Thread-1  ]: SQL status: SELECT 2037 in 0.01 seconds
13:22:32.599749 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.605684 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data" rename to "transformed_sales_number_data__dbt_backup"
13:22:32.609551 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
13:22:32.613314 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.613629 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
alter table "adludio"."public"."transformed_sales_number_data__dbt_tmp" rename to "transformed_sales_number_data"
13:22:32.614745 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
13:22:32.661299 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
13:22:32.661852 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.662219 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: COMMIT
13:22:32.672020 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
13:22:32.680430 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.transformed_sales_number_data"
13:22:32.680756 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.transformed_sales_number_data"} */
drop table if exists "adludio"."public"."transformed_sales_number_data__dbt_backup" cascade
13:22:32.684751 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
13:22:32.686137 [debug] [Thread-1  ]: finished collecting timing info
13:22:32.686401 [debug] [Thread-1  ]: On model.Analytics_dbt.transformed_sales_number_data: Close
13:22:32.693045 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fbe89c67-7760-4a1a-927e-3e66c01ed02a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32d9a37fa0>]}
13:22:32.693584 [info ] [Thread-1  ]: 1 of 3 OK created table model public.transformed_sales_number_data.............. [[32mSELECT 2037[0m in 0.22s]
13:22:32.693994 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.transformed_sales_number_data
13:22:32.697587 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.deal_value_per_week
13:22:32.697979 [info ] [Thread-1  ]: 2 of 3 START table model public.deal_value_per_week............................. [RUN]
13:22:32.698645 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.699109 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.deal_value_per_week
13:22:32.699337 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.deal_value_per_week
13:22:32.813154 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.deal_value_per_week"
13:22:32.814054 [debug] [Thread-1  ]: finished collecting timing info
13:22:32.814423 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.deal_value_per_week
13:22:32.818677 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.deal_value_per_week"
13:22:32.819442 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.819762 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: BEGIN
13:22:32.820039 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:22:32.837709 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
13:22:32.838289 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.838619 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */


  create  table "adludio"."public"."deal_value_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_value) as avg_deal_value, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
13:22:32.841050 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.0 seconds
13:22:32.855711 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.856260 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week" rename to "deal_value_per_week__dbt_backup"
13:22:32.856905 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
13:22:32.864224 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.864777 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
alter table "adludio"."public"."deal_value_per_week__dbt_tmp" rename to "deal_value_per_week"
13:22:32.865421 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
13:22:32.868486 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
13:22:32.868940 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.869252 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: COMMIT
13:22:32.870550 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
13:22:32.873184 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.deal_value_per_week"
13:22:32.873547 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.deal_value_per_week"} */
drop table if exists "adludio"."public"."deal_value_per_week__dbt_backup" cascade
13:22:32.875767 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
13:22:32.877682 [debug] [Thread-1  ]: finished collecting timing info
13:22:32.881278 [debug] [Thread-1  ]: On model.Analytics_dbt.deal_value_per_week: Close
13:22:32.887571 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fbe89c67-7760-4a1a-927e-3e66c01ed02a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32d85fe760>]}
13:22:32.888343 [info ] [Thread-1  ]: 2 of 3 OK created table model public.deal_value_per_week........................ [[32mSELECT 53[0m in 0.19s]
13:22:32.889499 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.deal_value_per_week
13:22:32.889884 [debug] [Thread-1  ]: Began running node model.Analytics_dbt.email_per_week
13:22:32.890421 [info ] [Thread-1  ]: 3 of 3 START table model public.email_per_week.................................. [RUN]
13:22:32.892006 [debug] [Thread-1  ]: Acquiring new postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.892310 [debug] [Thread-1  ]: Began compiling node model.Analytics_dbt.email_per_week
13:22:32.892600 [debug] [Thread-1  ]: Compiling model.Analytics_dbt.email_per_week
13:22:32.897669 [debug] [Thread-1  ]: Writing injected SQL for node "model.Analytics_dbt.email_per_week"
13:22:32.898370 [debug] [Thread-1  ]: finished collecting timing info
13:22:32.898707 [debug] [Thread-1  ]: Began executing node model.Analytics_dbt.email_per_week
13:22:32.906401 [debug] [Thread-1  ]: Writing runtime SQL for node "model.Analytics_dbt.email_per_week"
13:22:32.907336 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.908714 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: BEGIN
13:22:32.909061 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:22:32.925130 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
13:22:32.925647 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.929189 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */


  create  table "adludio"."public"."email_per_week__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (
    select * from "adludio"."public"."transformed_sales_number_data"
)

select AVG(deal_email_messages_count) as avg_email_count, week
from source_data
group by week
ORDER BY week 

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
13:22:32.939482 [debug] [Thread-1  ]: SQL status: SELECT 53 in 0.01 seconds
13:22:32.944179 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.944605 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week" rename to "email_per_week__dbt_backup"
13:22:32.947874 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
13:22:32.951795 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.952227 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
alter table "adludio"."public"."email_per_week__dbt_tmp" rename to "email_per_week"
13:22:32.957100 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
13:22:32.962707 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
13:22:32.963031 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.963307 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: COMMIT
13:22:32.974400 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
13:22:32.980696 [debug] [Thread-1  ]: Using postgres connection "model.Analytics_dbt.email_per_week"
13:22:32.981029 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "Analytics_dbt", "target_name": "dev", "node_id": "model.Analytics_dbt.email_per_week"} */
drop table if exists "adludio"."public"."email_per_week__dbt_backup" cascade
13:22:32.983484 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
13:22:32.985614 [debug] [Thread-1  ]: finished collecting timing info
13:22:32.985945 [debug] [Thread-1  ]: On model.Analytics_dbt.email_per_week: Close
13:22:32.988662 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fbe89c67-7760-4a1a-927e-3e66c01ed02a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32d9a27790>]}
13:22:32.989259 [info ] [Thread-1  ]: 3 of 3 OK created table model public.email_per_week............................. [[32mSELECT 53[0m in 0.10s]
13:22:32.989810 [debug] [Thread-1  ]: Finished running node model.Analytics_dbt.email_per_week
13:22:32.991271 [debug] [MainThread]: Acquiring new postgres connection "master"
13:22:32.991594 [debug] [MainThread]: Using postgres connection "master"
13:22:32.991816 [debug] [MainThread]: On master: BEGIN
13:22:32.992030 [debug] [MainThread]: Opening a new connection, currently in state closed
13:22:33.002615 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
13:22:33.003068 [debug] [MainThread]: On master: COMMIT
13:22:33.003364 [debug] [MainThread]: Using postgres connection "master"
13:22:33.003627 [debug] [MainThread]: On master: COMMIT
13:22:33.003996 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
13:22:33.004293 [debug] [MainThread]: On master: Close
13:22:33.004967 [info ] [MainThread]: 
13:22:33.006004 [info ] [MainThread]: Finished running 3 table models in 0.73s.
13:22:33.006682 [debug] [MainThread]: Connection 'master' was properly closed.
13:22:33.006951 [debug] [MainThread]: Connection 'model.Analytics_dbt.email_per_week' was properly closed.
13:22:33.015336 [info ] [MainThread]: 
13:22:33.015934 [info ] [MainThread]: [32mCompleted successfully[0m
13:22:33.019846 [info ] [MainThread]: 
13:22:33.020430 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
13:22:33.024180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32d85dddf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32d9a142b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32d9a02f70>]}
